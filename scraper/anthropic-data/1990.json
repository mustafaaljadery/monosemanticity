{"index": 1990, "act_buckets": [115, 104, 102, 88, 65, 43, 63, 52, 42, 41, 36, 35, 23, 20, 32, 24, 12, 18, 17, 9, 7, 10, 9, 7, 6, 2, 3, 1, 3, 2, 3, 1, 2, 0, 1, 0, 0, 0, 1, 1], "act_bucket_edges": [0, 0.1045, 0.209, 0.3135, 0.418, 0.5225, 0.627, 0.7315, 0.836, 0.9405, 1.045, 1.149, 1.254, 1.358, 1.463, 1.567, 1.672, 1.776, 1.881, 1.985, 2.09, 2.194, 2.299, 2.403, 2.508, 2.612, 2.717, 2.821, 2.926, 3.03, 3.135, 3.239, 3.344, 3.448, 3.553, 3.657, 3.762, 3.866, 3.971, 4.075, 4.18], "logit_buckets": [1, 0, 2, 6, 9, 24, 38, 90, 254, 457, 771, 1324, 2127, 3093, 4276, 5554, 7042, 7868, 10740, 6857, 3996, 2950, 2172, 1676, 1232, 891, 654, 472, 311, 213, 160, 102, 66, 43, 21, 20, 12, 5, 1, 5], "logit_bucket_edges": [-0.3616, -0.3415, -0.3214, -0.3013, -0.2812, -0.2611, -0.2409, -0.2208, -0.2007, -0.1806, -0.1605, -0.1404, -0.1203, -0.1001, -0.08003, -0.05992, -0.0398, -0.01969, 0.0004241, 0.02054, 0.04065, 0.06076, 0.08088, 0.101, 0.1211, 0.1412, 0.1613, 0.1814, 0.2016, 0.2217, 0.2418, 0.2619, 0.282, 0.3021, 0.3222, 0.3424, 0.3625, 0.3826, 0.4027, 0.4228, 0.4429], "examples_quantiles": [{"quantile_name": "Top Activations", "quantile_max_act": 5.114, "examples": [{"tokens_str_list": [" about", " causal", " powers", ":", " invoking", " them", " in", " a", " characterization"], "tokens_acts_list": [4.434, 1.22, 1.338, 3.666, 5.114, 2.671, 4.188, 3.624, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 8852199, "ablations": {"loss_ablation": [0.2242, 3.303, 0.03943, 0.248, 1.636, 0.05069, -0.1894, 0.4098, 0], "top_logit_ablation_tokens_str": [[" County", " County", "oted", "peed", "shirt", " Services", " Makeup", " County", "<META>"], [" however", " Rs", " Services", "chedule", "<EOT>", "chedule", "lop", "ette", "<EOT>"], [" Circuit", "hire", "iaries", "burg", "shirts", "shirt", "robe", "verter", "<META_START>"], ["hire", " 1000", "osit", "lovakia", "hire", " County", "reek", " lieu", "<SOS>"], [" sir", " 7", "pool", "hell", "res", "resistant", "hip", "osit", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["eced", "ibility", " Aristotle", " philosophical", " philosophical", " philosophical", " philosophers", "eced", "<META>"], [".", " discourse", "hetic", " empirical", " ourselves", " conceive", " metaph", " philosophers", "<EOT>"], ["\"", " philosophical", " Descartes", " metaph", " metaph", "aments", " philosophical", "ism", "<META_START>"], [" causal", " empirical", "icism", " inferences", " truths", "actic", " ontology", "ifies", "<SOS>"], [" oneself", " notions", "ical", " moral", " ontology", " philosophers", " causal", "itatively", "<META_END>"]], "top_logit_ablation_values": [[-0.2112, -1.862, -0.2584, 0.07694, 1.052, -1.002, 0.4756, 0.7654, 0], [-0.2192, -1.881, -0.2797, 0.03837, 1.013, -1.13, 0.3886, 0.5811, 0], [-0.2249, -1.983, -0.3336, 0.02549, 0.9247, -1.135, 0.2882, 0.486, 0], [-0.2792, -2.011, -0.3484, 0.01788, 0.8036, -1.349, 0.2819, 0.4606, 0], [-0.2932, -2.057, -0.3776, 0.003907, 0.7901, -1.398, 0.2345, 0.4396, 0]], "bottom_logit_ablation_values": [[-1.885, -8.43, -1.883, -1.861, -3.589, -7.445, -3.007, -5.382, 0], [-1.862, -8.357, -1.867, -1.837, -3.539, -7.378, -2.998, -5.28, 0], [-1.835, -8.261, -1.847, -1.78, -3.524, -7.377, -2.991, -5.276, 0], [-1.817, -8.175, -1.825, -1.76, -3.502, -7.371, -2.991, -5.193, 0], [-1.817, -8.062, -1.812, -1.758, -3.49, -7.357, -2.973, -5.121, 0]], "residual_loss_ablation": [-0.4458, 0.9599, 0.1987, 0.6823, -0.5563, 0.8275, -0.3045, -0.8227, 0]}}, {"tokens_str_list": [" is", " or", " does", ",", " invoking", " it", " fails", " to", " contribute"], "tokens_acts_list": [1.527, 0, 1.898, 0.6457, 5.066, 2.139, 1.444, 2.311, 3.297], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 83032547, "ablations": {"loss_ablation": [0.03953, 0.1665, 0, 0.233, 0.2065, -0.09951, 0.005351, -0.08964, 0], "top_logit_ablation_tokens_str": [["amide", " booked", "<META>", "day", " sir", "chedule", "inafter", "days", "<META>"], ["id", " parked", "<EOT>", "chedule", "======", " ambulance", " Makeup", "chedule", "<EOT>"], ["ants", " scheduled", "<META_START>", "week", " huh", " Services", "burg", "cheon", "<META_START>"], [" booked", " eaten", "<SOS>", "otherapy", " TX", " Kit", "!\u201d", " Monday", "<SOS>"], ["leneck", " filed", "<META_END>", " hike", "000", "catch", " illegally", " Saturday", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", " philosophers", "<META>", "utable", "gence", " ontology", " ontology", " philosophers", "<META>"], [" linguistic", "ness", "<EOT>", "ulates", "itor", "matical", "\u03c0", " meanings", "<EOT>"], [" logical", "atural", "<META_START>", " meanings", "licity", "licity", " philosophical", " thinkers", "<META_START>"], [" metaph", "mitives", "<SOS>", " philosophical", "volent", "istem", " meanings", " ontology", "<SOS>"], [" Descartes", " notions", "<META_END>", "uality", "ness", " ideological", " contexts", "utable", "<META_END>"]], "top_logit_ablation_values": [[0.1017, 0.8434, 0, 0.8049, 0.1784, -0.2094, 0.5256, 0.475, 0], [0.08491, 0.7301, 0, 0.5631, 0.1317, -0.4857, 0.4867, 0.4539, 0], [0.08368, 0.7037, 0, 0.4737, 0.1313, -0.5128, 0.4582, 0.4354, 0], [0.08358, 0.6413, 0, 0.4303, 0.1204, -0.5538, 0.3465, 0.4319, 0], [0.08219, 0.6251, 0, 0.3991, 0.1134, -0.5729, 0.3371, 0.4021, 0]], "bottom_logit_ablation_values": [[-0.09932, -1.477, 0, -2.607, -0.6579, -6.016, -2.873, -1.656, 0], [-0.09488, -1.459, 0, -2.255, -0.6429, -5.954, -2.794, -1.647, 0], [-0.09439, -1.436, 0, -2.249, -0.6409, -5.947, -2.794, -1.566, 0], [-0.09433, -1.434, 0, -2.191, -0.6274, -5.813, -2.736, -1.521, 0], [-0.09366, -1.429, 0, -2.19, -0.6243, -5.808, -2.711, -1.517, 0]], "residual_loss_ablation": [2.365, 2.033, 2.626, 1.232, -0.1313, 0.7988, 2.639, -0.2573, 0]}}, {"tokens_str_list": [" point", " of", " view", ")", " for", " taking", " something", " to", " be"], "tokens_acts_list": [0, 3.074, 1.619, 2.972, 4.832, 3.224, 1.789, 2.93, 2.502], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 89924343, "ablations": {"loss_ablation": [-0.1147, 0, -0.6363, -0.3013, -0.6749, -0.4304, 1.068, -0.006512, 0], "top_logit_ablation_tokens_str": [["chedule", "<META>", "iler", "peed", " County", "gery", "chedule", "ctomy", "<META>"], ["shirt", "<EOT>", "oot", "shirt", "\n\f", "jury", "away", "mary", "<EOT>"], ["})$", "<META_START>", "iper", "ports", "unded", "shirt", "shirt", "out", "<META_START>"], ["0", "<SOS>", "uming", "sers", " county", "oes", "out", " ashore", "<SOS>"], ["})$.", "<META_END>", " jail", "hire", "County", " County", " County", "pm", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "<META>", "\u03b3", " philosophical", " ontology", "\u03c1", " Aristotle", " contexts", "<META>"], [" oneself", "<EOT>", " Marx", " empirical", " contexts", "\u03b3", " metaph", " cognition", "<EOT>"], [" Descartes", "<META_START>", " notions", " moral", " philosophers", " Freud", "inciple", " ontology", "<META_START>"], [" ourselves", "<SOS>", " constructs", " ethical", " Descartes", "\u03c9", " Descartes", "istem", "<SOS>"], [" philosophers", "<META_END>", " Aristotle", "\u03c1", " Freud", "\u03c0", "uality", " philosophers", "<META_END>"]], "top_logit_ablation_values": [[-0.2835, 0, -0.3648, -0.03641, 0.3582, 1.079, 0.09196, 0.03837, 0], [-0.303, 0, -0.5609, -0.04392, 0.1248, 0.9724, 0.06905, -0.05912, 0], [-0.3084, 0, -0.5954, -0.04902, 0.06696, 0.8101, -0.08956, -0.06173, 0], [-0.3148, 0, -0.6662, -0.07643, 0.06446, 0.735, -0.1918, -0.1138, 0], [-0.3244, 0, -0.6966, -0.09187, -0.02582, 0.7109, -0.2727, -0.1607, 0]], "bottom_logit_ablation_values": [[-1.548, 0, -5.179, -2.202, -3.911, -6.652, -4.633, -2.557, 0], [-1.545, 0, -4.973, -2.187, -3.907, -6.63, -4.558, -2.445, 0], [-1.491, 0, -4.962, -2.178, -3.889, -6.334, -4.557, -2.428, 0], [-1.487, 0, -4.952, -2.159, -3.826, -6.268, -4.553, -2.416, 0], [-1.471, 0, -4.933, -2.126, -3.747, -6.266, -4.535, -2.4, 0]], "residual_loss_ablation": [1.235, 0.6474, 6.497, 7.58, -0.4259, 0.5483, -0.7508, 0.7057, 0]}}, {"tokens_str_list": [".", " Thus", ",", " many", " contemporary", " philosophers", " think", " that", " predicates"], "tokens_acts_list": [0.7064, 3.89, 2.693, 4.96, 4.815, 2.306, 2.772, 4.148, 2.504], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 14529774, "ablations": {"loss_ablation": [-0.1627, 0.05683, -0.2945, 0.02934, 1.993, 0.6991, 0.3579, 0.01757, 0], "top_logit_ablation_tokens_str": [["hire", "shirt", "shirt", "shirt", " County", "ved", "hire", "hire", "<META>"], ["led", " County", "shirts", " Circuit", "ToMany", "fits", " booked", " tank", "<EOT>"], [" booked", "hire", "hire", " sir", "hire", "ported", " County", "stown", "<META_START>"], ["raiser", "sville", " TX", " County", "ported", "anted", "days", " Makeup", "<SOS>"], ["chedule", "oa", "sville", "jar", "fits", "ases", "borough", "out", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", " philosophical", " existence", ".", "actic", " whether", "ential", "utable", "<META>"], [" Descartes", " metaph", " philosophical", "itude", " conception", " anything", " negation", " interpretation", "<EOT>"], [" metaph", " oneself", "ulsion", ".:", " morality", " \\xce", " truths", "\u03c1", "<META_START>"], [" truths", " truths", " empirical", "arrative", " hypothesis", " what", " metaph", " negation", "<SOS>"], [" empirical", " empirical", " meanings", " existence", " ethics", " why", " ontology", " reasoning", "<META_END>"]], "top_logit_ablation_values": [[0.3999, -0.007852, -0.7591, -0.5219, -0.788, 1.703, 0.05041, 0.931, 0], [0.3975, -0.04045, -0.9445, -0.5234, -0.8543, 1.462, -0.1311, 0.8945, 0], [0.3873, -0.05253, -0.9986, -0.54, -1.092, 1.426, -0.1574, 0.8494, 0], [0.3804, -0.07792, -1.015, -0.5588, -1.214, 1.386, -0.2087, 0.83, 0], [0.3735, -0.0884, -1.023, -0.5653, -1.246, 1.324, -0.2229, 0.7377, 0]], "bottom_logit_ablation_values": [[-1.592, -1.17, -6.27, -4.098, -7.861, -5.114, -3.383, -2.989, 0], [-1.577, -1.15, -6.244, -3.98, -7.779, -4.995, -3.206, -2.833, 0], [-1.574, -1.133, -6.185, -3.874, -7.776, -4.989, -3.189, -2.832, 0], [-1.566, -1.128, -6.104, -3.804, -7.598, -4.878, -3.177, -2.815, 0], [-1.56, -1.119, -6.082, -3.789, -7.575, -4.857, -3.17, -2.809, 0]], "residual_loss_ablation": [0.4785, 0.5136, -0.4912, -0.5599, -1.869, 1.855, 1.405, 0.005167, 0]}}, {"tokens_str_list": [" whatsoever", ",", " except", " perhaps", " for", " cases", " that", " would", " lead"], "tokens_acts_list": [0.6172, 1.582, 1.986, 3.15, 4.789, 0.8449, 3.205, 1.628, 3.295], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 65747794, "ablations": {"loss_ablation": [0.03931, 0.05933, -0.4111, 0.4618, -0.6736, -0.2646, 0.1238, -0.2338, 0], "top_logit_ablation_tokens_str": [["raiser", "ctomy", " booked", "ed", " TX", "warded", " booked", "shirt", "<META>"], ["arrow", " pursuant", "raiser", " TX", " booked", "gery", "ctomy", " TX", "<EOT>"], ["shirt", " aboard", " respectively", "shirt", "peed", " TX", "burg", " booked", "<META_START>"], ["roller", " ashore", " TX", "erry", "shirt", "ouchers", " TX", "shirts", "<SOS>"], ["sville", " Makeup", " pending", "ing", "raiser", "ced", "unded", "raiser", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["\u03c1", " philosophers", "ize", "\u03c1", " existence", "\u03c1", " philosophical", "ential", "<META>"], ["\u03c0", " philosophical", " oneself", "\u03c9", " discourse", "\\xe1", " ontology", " meanings", "<EOT>"], ["phrine", " oneself", "itude", " philosophical", " ontology", " relates", " metaph", " philosophical", "<META_START>"], [" \\xce", " metaph", " Marx", "\u03b1", " contexts", "\u03b3", " Marx", "\u03c0", "<SOS>"], ["\u03b9", " existence", " ontology", " metaph", " meanings", "\u03c9", " notions", " metaph", "<META_END>"]], "top_logit_ablation_values": [[0.3144, -0.03692, 0.2468, -0.07785, -0.5478, 1.066, 0.03125, -0.4387, 0], [0.3056, -0.05509, 0.2082, -0.3308, -0.5746, 0.7786, 0.02921, -0.7081, 0], [0.2737, -0.06496, 0.1997, -0.3368, -0.72, 0.7393, 0.0174, -0.8918, 0], [0.2393, -0.06807, 0.1763, -0.3684, -0.783, 0.4652, 0.01693, -0.9402, 0], [0.2309, -0.06983, 0.1761, -0.4074, -0.8252, 0.4586, 0.008946, -1.063, 0]], "bottom_logit_ablation_values": [[-0.9257, -0.9269, -1.674, -3.287, -5.066, -6.59, -1.131, -6.231, 0], [-0.9181, -0.8969, -1.656, -3.066, -4.983, -5.907, -1.103, -6.117, 0], [-0.9174, -0.8938, -1.652, -2.991, -4.963, -5.726, -1.102, -6.063, 0], [-0.8539, -0.8786, -1.569, -2.987, -4.938, -5.7, -1.09, -5.861, 0], [-0.8523, -0.874, -1.54, -2.978, -4.918, -5.68, -1.076, -5.719, 0]], "residual_loss_ablation": [4.215, -1.509, 1.588, 1.132, 0.7735, 1.882, -0.06988, 1.002, 0]}}, {"tokens_str_list": [" chapter", " I", " argued", " that", " neither", " intuition", " nor", " perception", " are"], "tokens_acts_list": [0.1056, 1.743, 1.947, 3.139, 4.737, 0.6546, 2.671, 2.222, 3.364], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 53686744, "ablations": {"loss_ablation": [-0.09844, -0.01511, 0.4796, -0.005105, 0.153, 2.677, -0.008138, 0.4493, 0], "top_logit_ablation_tokens_str": [[" Year", "hire", "shirt", " County", "shirt", "chedule", "led", "chedule", "<META>"], [" Week", " County", "AP", "shirt", " County", "shirt", "shirt", "STRICT", "<EOT>"], [" Circuit", " TX", " County", " booked", " TX", "iscopal", " (@", "aggle", "<META_START>"], [" inbox", "shirt", "burg", "ulf", "amycin", " Week", "iest", "\"}\\", "<SOS>"], [" month", " opener", "hire", "out", " Loan", " Tomatoes", " Reserved", "CUSSION", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" oneself", " philosophical", " philosophical", " ontology", " philosophical", " existence", " philosophical", "minded", "<META>"], [" necessarily", " meanings", " metaph", " philosophical", " meanings", " phenomena", " ontology", "utable", "<EOT>"], [" ourselves", " moral", " ontology", " metaph", " existence", " inferences", "icism", ".:", "<META_START>"], [" itself", " empirical", " truths", " philosophers", " causal", " meanings", "quences", "ibility", "<SOS>"], [" why", " existence", " Aristotle", " interpretation", "eced", "ential", " moral", "\u03c0", "<META_END>"]], "top_logit_ablation_values": [[-0.0919, -0.02176, -0.06314, -0.7395, -1.143, -0.7947, -0.009574, -0.04461, 0], [-0.1186, -0.02997, -0.1644, -0.7565, -1.4, -1.296, -0.09022, -0.1771, 0], [-0.1285, -0.0304, -0.1832, -0.8524, -1.474, -1.599, -0.1178, -0.263, 0], [-0.1345, -0.03213, -0.2088, -0.8715, -1.579, -1.827, -0.1198, -0.2687, 0], [-0.138, -0.03306, -0.2154, -0.8762, -1.629, -1.878, -0.1203, -0.2804, 0]], "bottom_logit_ablation_values": [[-1.118, -0.163, -2.849, -3.345, -5.645, -9.167, -1.057, -3.513, 0], [-1.089, -0.1572, -2.832, -3.303, -5.623, -9.053, -1.054, -3.406, 0], [-1.079, -0.1571, -2.8, -3.217, -5.616, -9.051, -1.04, -3.372, 0], [-1.055, -0.1565, -2.797, -3.139, -5.57, -8.98, -1.033, -3.345, 0], [-1.048, -0.1557, -2.778, -3.133, -5.549, -8.913, -1.01, -3.331, 0]], "residual_loss_ablation": [6.319, 3.232, 0.1984, -0.2134, 0.3332, 1.39, 4.961, -0.6932, 0]}}, {"tokens_str_list": [" leaving", " the", " former", " concept", " with", " no", " clear", " intuitive", " content"], "tokens_acts_list": [3.311, 1.238, 1.385, 0, 4.731, 2.547, 1.773, 1.963, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 80119186, "ablations": {"loss_ablation": [-0.1713, 0.2066, -0.2401, 0.4452, 0, -0.2987, -0.05463, 0.7618, 0], "top_logit_ablation_tokens_str": [[" respectively", "chedule", "chedule", " Atty", "<META>", "shirt", "zzles", "lee", "<META>"], [" booked", "shirt", " freezer", " boyfriend", "<EOT>", "held", "chedule", "shirt", "<EOT>"], [" sir", " County", " receptionist", " bartender", "<META_START>", "holding", "sed", "led", "<META_START>"], ["======", " TX", " weekend", " office", "<SOS>", " TX", "zzle", "chedule", "<SOS>"], [" TX", " Makeup", " fridge", " roommate", "<META_END>", "ered", "rist", "lights", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" oneself", "eced", " Aristotle", " meanings", "<META>", " phenomena", " its", " Aristotle", "<META>"], ["itude", " ontology", " its", "\\xe1", "<EOT>", " inferences", "licity", " Descartes", "<EOT>"], [" ontology", "\u03c1", " Marx", "\u03c6", "<META_START>", "\u03b3", " the", "\u03c0", "<META_START>"], ["utable", " relativity", " how", "\u03b1", "<SOS>", "trical", " ourselves", "\u03b3", "<SOS>"], ["mathcal", " philosophical", " anything", "eness", "<META_END>", " representations", " these", "ifolds", "<META_END>"]], "top_logit_ablation_values": [[0.5764, -0.2343, 0.05015, -0.2939, 0, 0.02744, 0.7101, 0.5712, 0], [0.3931, -0.4205, -0.01974, -0.3179, 0, -0.05597, 0.3374, 0.4301, 0], [0.3879, -0.5429, -0.05774, -0.3369, 0, -0.1507, 0.3337, 0.4169, 0], [0.3749, -0.6241, -0.06236, -0.3486, 0, -0.2108, 0.3143, 0.3907, 0], [0.3636, -0.6279, -0.06484, -0.3489, 0, -0.3488, 0.3138, 0.3596, 0]], "bottom_logit_ablation_values": [[-1.377, -5, -1.515, -1.913, 0, -6.593, -3.436, -2.078, 0], [-1.359, -4.87, -1.506, -1.912, 0, -6.588, -3.315, -1.963, 0], [-1.311, -4.855, -1.493, -1.898, 0, -6.531, -3.286, -1.959, 0], [-1.303, -4.833, -1.485, -1.896, 0, -6.459, -3.277, -1.957, 0], [-1.303, -4.811, -1.485, -1.894, 0, -6.458, -3.275, -1.937, 0]], "residual_loss_ablation": [0.3388, 0.078, -0.463, -0.2674, -0.952, 0.6372, 1.061, 0.8512, 0]}}, {"tokens_str_list": [" beliefs", " regarding", " the", " relation", " between", " God", "'s", " will", " and"], "tokens_acts_list": [0.6427, 4.696, 1.604, 0, 4.722, 1.125, 2.043, 1.405, 0.5596], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 53976446, "ablations": {"loss_ablation": [0.2652, -0.02851, -0.1439, 0.03791, 0, -0.01447, -0.03714, -0.05602, 0], "top_logit_ablation_tokens_str": [[" Makeup", "chedule", "cheon", " Netherlands", "<META>", "sville", "dess", " Republic", "<META>"], [" Atty", "hire", "kins", "ctomy", "<EOT>", "shirt", "hire", "esville", "<EOT>"], [" Office", "ulf", " Pradesh", " Pradesh", "<META_START>", "days", "sville", " Day", "<META_START>"], ["shirt", "days", "chedule", "esville", "<SOS>", "noon", "wear", "ctomy", "<SOS>"], [" Observatory", "peed", " Laboratory", " weekend", "<META_END>", "Ct", "noon", "cology", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" oneself", " philosophical", "ensible", " something", "<META>", "ational", "utable", "ational", "<META>"], [" itself", " empirical", " discourse", " anything", "<EOT>", " causal", " empirical", "utable", "<EOT>"], [" anything", " moral", " causal", " oneself", "<META_START>", " inferences", " philosophical", " oneself", "<META_START>"], [" something", "ernate", "ational", " itself", "<SOS>", " contradictory", " causal", " anything", "<SOS>"], ["eced", " causal", " inferences", " whether", "<META_END>", " necessarily", " inferences", "urally", "<META_END>"]], "top_logit_ablation_values": [[0.533, 0.1271, -0.4572, -0.0554, 0, 0.6556, 0.6175, 0.2981, 0], [0.4264, 0.1234, -0.462, -0.08162, 0, 0.4487, 0.5962, 0.2939, 0], [0.4207, 0.1048, -0.5294, -0.1421, 0, 0.1119, 0.5002, 0.2927, 0], [0.3963, 0.1046, -0.5385, -0.1427, 0, 0.1055, 0.4789, 0.2712, 0], [0.3712, 0.07351, -0.6103, -0.1732, 0, -0.03606, 0.4742, 0.2379, 0]], "bottom_logit_ablation_values": [[-1.431, -0.6812, -5.891, -2.08, 0, -6.08, -1.049, -2.574, 0], [-1.379, -0.6464, -5.888, -2.058, 0, -5.834, -1.049, -2.446, 0], [-1.353, -0.6444, -5.881, -2.043, 0, -5.824, -1.046, -2.43, 0], [-1.33, -0.6441, -5.838, -2, 0, -5.782, -1.045, -2.353, 0], [-1.326, -0.6365, -5.795, -1.964, 0, -5.757, -1.042, -2.35, 0]], "residual_loss_ablation": [0.15, 1.565, 1.923, -0.05732, 3.97, 0.2069, 0.7359, -0.9217, 0]}}, {"tokens_str_list": ["1", "),", " nor", " equivalent", " with", " (", "T", ").", " Accordingly"], "tokens_acts_list": [0.5375, 3.203, 0.9332, 3.716, 4.71, 1.852, 0, 2.302, 1.081], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 1433234, "ablations": {"loss_ablation": [0, -0.02331, -0.7158, 0.1205, -0.3155, 0.137, -0.0545, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "month", " Ltd", "ouchers", "ette", "holding", "shirt", "<META>", "<META>"], ["<EOT>", "shirt", "erry", "anty", "ouchers", "shirt", "out", "<EOT>", "<EOT>"], ["<META_START>", " TX", "shirt", "amide", "roup", "erry", " County", "<META_START>", "<META_START>"], ["<SOS>", "AP", "oa", " booked", "ved", "hold", "AP", "<SOS>", "<SOS>"], ["<META_END>", " County", " sweater", "arnish", "ht", "mary", "hire", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " philosophical", " phenomena", " biology", " infinite", " phenomena", " philosophical", "<META>", "<META>"], ["<EOT>", " philosophers", " hypotheses", " phenomena", "actic", " relativity", " philosophers", "<EOT>", "<EOT>"], ["<META_START>", " empirical", " consequence", " theories", "\u03c0", " discoveries", " Aristotle", "<META_START>", "<META_START>"], ["<SOS>", " metaph", " mathematical", " disciplines", " Hawking", " theories", " metaph", "<SOS>", "<SOS>"], ["<META_END>", " truths", " worlds", " mathematical", " Aristotle", " universe", " Descartes", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, -0.1532, 0.3992, -0.01012, 1.521, 1.211, 0.0532, 0, 0], [0, -0.1726, 0.2949, -0.03929, 1.5, 1.054, -0.07808, 0, 0], [0, -0.1985, 0.2908, -0.06133, 1.444, 0.9057, -0.1375, 0, 0], [0, -0.1997, 0.09272, -0.07177, 1.409, 0.8051, -0.1498, 0, 0], [0, -0.2002, 0.07201, -0.07911, 1.362, 0.6873, -0.1565, 0, 0]], "bottom_logit_ablation_values": [[0, -1.134, -3.793, -1.284, -3.49, -5.873, -2.364, 0, 0], [0, -1.128, -3.76, -1.253, -3.488, -5.23, -2.344, 0, 0], [0, -1.104, -3.758, -1.211, -3.475, -5.18, -2.343, 0, 0], [0, -1.097, -3.581, -1.198, -3.452, -5.127, -2.317, 0, 0], [0, -1.096, -3.566, -1.195, -3.445, -5.064, -2.311, 0, 0]], "residual_loss_ablation": [-0.3334, 2.572, 3.948, 0.772, 0.3183, 1.929, 0.6241, -0.7923, 0]}}, {"tokens_str_list": [" any", " possibility", " of", " interaction", " with", " empirical", " traits", ".", " Reconstruction"], "tokens_acts_list": [2.514, 0.6718, 3.19, 0.7016, 4.686, 2.256, 1.269, 0, 0.6808], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 84249996, "ablations": {"loss_ablation": [-0.4151, 0.1619, -0.1565, 0.3457, -0.1273, 3.152, 0.3175, -0.1566, 0], "top_logit_ablation_tokens_str": [[" fees", " refund", "days", " laund", "led", "shirt", "oted", "peed", "<META>"], [" Rs", "ctomy", "chedule", " refund", "heed", " TX", "zzles", "led", "<EOT>"], [" refund", "ved", "ctomy", " booking", "ulf", "shirts", " fees", "ulf", "<META_START>"], [" tickets", "ette", " thereof", " deposit", "ed", "illings", "runch", " purchased", "<SOS>"], [" purchased", "oprop", "\n\f", "ette", "ht", "ahoe", "sed", " booked", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["icism", "){#", " philosophical", " Aristotle", " Descartes", " philosophical", "acular", " philosophical", "<META>"], ["scious", " ourselves", " ontology", " Descartes", " philosophical", "ism", " why", " metaph", "<EOT>"], [" philosophers", " Descartes", " Freud", " philosophers", "beit", " truths", " oneself", " Philosophy", "<META_START>"], [" theorists", " Aristotle", " meanings", "icism", "\u03c1", " metaph", " whether", "inciple", "<SOS>"], [" relativity", " oneself", "\u03c1", "lected", " empirical", " hypothesis", " Aristotle", " ontology", "<META_END>"]], "top_logit_ablation_values": [[0.2013, 0.3177, 0.2212, 0.1353, 0.3054, 0.02909, -0.5754, -0.05399, 0], [0.007068, 0.2801, 0.1883, 0.07729, 0.3016, -0.5669, -0.606, -0.05677, 0], [-0.01254, 0.2649, 0.1518, -0.05976, 0.2732, -0.8744, -0.6402, -0.0643, 0], [-0.2056, 0.2174, 0.1267, -0.09193, 0.2592, -1.08, -0.6439, -0.08831, 0], [-0.2154, 0.2077, 0.1201, -0.1152, 0.2394, -1.176, -0.6887, -0.1036, 0]], "bottom_logit_ablation_values": [[-4.332, -3.64, -0.9605, -4.956, -0.7399, -8.274, -3.661, -1.819, 0], [-3.982, -3.482, -0.9529, -4.737, -0.7275, -8.168, -3.584, -1.709, 0], [-3.956, -3.408, -0.9506, -4.71, -0.7234, -8.121, -3.566, -1.704, 0], [-3.917, -3.406, -0.9486, -4.709, -0.7164, -8.097, -3.515, -1.691, 0], [-3.906, -3.354, -0.9415, -4.656, -0.7058, -8.022, -3.46, -1.681, 0]], "residual_loss_ablation": [2.051, 1.853, 0.0412, 3.613, -0.7245, -0.02391, -1.006, 1.484, 0]}}, {"tokens_str_list": [" (", "b", ")", " whether", " given", " that", " content", ",", " there"], "tokens_acts_list": [2.309, 0, 3.147, 3.845, 4.665, 3.59, 1.013, 0.8409, 0.402], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 78230168, "ablations": {"loss_ablation": [0, -0.1492, 0, 0.2156, 0.4023, 0.2317, 0.1896, 0.1713, 0], "top_logit_ablation_tokens_str": [["<META>", "shirt", "<META>", " respectively", " ago", " oven", " sweater", "led", "<META>"], ["<EOT>", "out", "<EOT>", "hire", "out", "chedule", "shirt", "ed", "<EOT>"], ["<META_START>", "AP", "<META_START>", " County", " sweater", "day", " Makeup", "shirt", "<META_START>"], ["<SOS>", "hire", "<SOS>", " JJ", " Ltd", "heed", " TX", "shots", "<SOS>"], ["<META_END>", "month", "<META_END>", "weekly", " TX", " ashore", " Rs", "ous", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " philosophical", "<META>", "minded", "trical", " relativity", "ational", " philosophical", "<META>"], ["<EOT>", " philosophers", "<EOT>", "ity", " relativity", " distinguishing", "itus", " mathematical", "<EOT>"], ["<META_START>", " metaph", "<META_START>", ",", "notations", "sofar", "eced", " relativity", "<META_START>"], ["<SOS>", " Aristotle", "<SOS>", " Aristotle", "onom", " defining", "ifiable", " metaph", "<SOS>"], ["<META_END>", " Descartes", "<META_END>", " ontology", " existence", "\u03c0", "\u03b2", " Descartes", "<META_END>"]], "top_logit_ablation_values": [[0, 0.2468, 0, 1.466, 0.3708, 1.145, -0.7867, 0.166, 0], [0, 0.104, 0, 1.379, 0.1532, 0.6668, -1.006, 0.1534, 0], [0, -0.01566, 0, 1.266, -0.0008192, 0.6106, -1.031, 0.1475, 0], [0, -0.04833, 0, 1.203, -0.05408, 0.4874, -1.077, 0.09188, 0], [0, -0.06523, 0, 1.202, -0.09132, 0.3577, -1.146, 0.04331, 0]], "bottom_logit_ablation_values": [[0, -2.936, 0, -2.443, -4.686, -5.665, -6.4, -1.376, 0], [0, -2.906, 0, -2.387, -4.472, -5.545, -5.928, -1.316, 0], [0, -2.899, 0, -2.366, -4.446, -5.518, -5.913, -1.314, 0], [0, -2.893, 0, -2.275, -4.392, -5.467, -5.873, -1.285, 0], [0, -2.85, 0, -2.274, -4.347, -5.382, -5.842, -1.275, 0]], "residual_loss_ablation": [0.7426, 3.905, 0.0453, 1.793, 1.032, -0.1004, 1.968, -1.463, 0]}}, {"tokens_str_list": [" than", " being", " empir", "ically", " given", ".", " Contin", "uity", " is"], "tokens_acts_list": [3.712, 3.734, 0.3131, 1.243, 4.662, 0, 0, 2.063, 1.973], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 66187310, "ablations": {"loss_ablation": [-0.3148, 0.3912, 1.17, -0.003371, -0.05447, -0.5929, 0, 0, 0], "top_logit_ablation_tokens_str": [["hire", "chedule", " booked", "res", " booked", "day", "<META>", "<META>", "<META>"], ["======", "lace", " purchased", "oxide", " rinsed", " manufacturer", "<EOT>", "<EOT>", "<EOT>"], ["ina", "ixel", " shipped", "cin", " purchased", "lasses", "<META_START>", "<META_START>", "<META_START>"], ["wear", "isters", " evacuated", "aged", " evacuated", "chedule", "<SOS>", "<SOS>", "<SOS>"], ["ixel", "imize", " installed", "days", " incubated", "\u00f1os", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" ourselves", " sake", " hypothesis", " philosophical", " vagu", "esian", "<META>", "<META>", "<META>"], ["acy", " reasons", "ness", " philosophers", " semantics", " argues", "<EOT>", "<EOT>", "<EOT>"], ["\u03b3", " meanings", "eness", " linguistic", " truths", "onomy", "<META_START>", "<META_START>", "<META_START>"], ["UTR", "comings", "arity", " oneself", " ontology", "consistency", "<SOS>", "<SOS>", "<SOS>"], [" Descartes", " hypothesis", " implication", " metaph", " ambiguity", "licity", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[1.192, 1.105, 0.7766, 0.1588, -0.1062, 0.9046, 0, 0, 0], [1.05, 1.071, 0.5264, 0.1557, -0.2449, 0.6955, 0, 0, 0], [1.031, 1.02, 0.5042, 0.1472, -0.2488, 0.6668, 0, 0, 0], [1.025, 1.006, 0.4899, 0.1433, -0.2544, 0.6477, 0, 0, 0], [1.012, 0.9796, 0.3212, 0.1429, -0.2596, 0.5573, 0, 0, 0]], "bottom_logit_ablation_values": [[-2.614, -3.801, -4.798, -0.2859, -1.884, -5.462, 0, 0, 0], [-2.592, -3.683, -4.694, -0.278, -1.865, -4.993, 0, 0, 0], [-2.352, -3.414, -4.441, -0.2776, -1.85, -4.905, 0, 0, 0], [-2.339, -3.359, -4.388, -0.2745, -1.847, -4.855, 0, 0, 0], [-2.337, -3.331, -4.266, -0.2737, -1.814, -4.851, 0, 0, 0]], "residual_loss_ablation": [-0.1206, 0.2683, 1.762, 7.025, 0.9685, 1.4, 1.429, -1.535, 0]}}, {"tokens_str_list": [" are", " primarily", " symbolic", " as", " opposed", " to", " sub", "-", "symbolic"], "tokens_acts_list": [2.348, 1.274, 1.815, 3.144, 4.662, 3.898, 1.946, 1.359, 1.882], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 46057433, "ablations": {"loss_ablation": [-0.05921, -0.1985, 0.3503, 0.1959, -1.095, -0.04551, 0.2706, -0.1734, 0], "top_logit_ablation_tokens_str": [["hire", " purchased", " booked", "led", "isters", "isters", "ixel", "shirt", "<META>"], ["days", " booked", "banded", "ted", "pons", "============", "shirt", "ixel", "<EOT>"], ["peed", " installed", " purchased", "oted", "sets", " to", "boards", "oxide", "<META_START>"], [" TX", " located", " located", "ed", "bike", " ago", "shirts", "sville", "<SOS>"], [" County", " configured", "eper", "pons", "wear", "boards", "aged", "hire", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", "itation", " hypothesis", " Aristotle", "utable", " rational", "esis", " reasoning", "<META>"], [" truths", " existence", " morality", " MERCHANTABILITY", "gment", "...](", "ity", " philosophical", "<EOT>"], [" moral", " hypothesis", " existence", " Descartes", "irection", " morality", "\u03b1", " hypothesis", "<META_START>"], [" morality", "tion", " Aristotle", " oneself", " contradiction", " Philosophy", "leness", " interpretation", "<SOS>"], [" negation", "ness", " contradiction", "utable", " hypothesis", " sake", "\\x80", " metaph", "<META_END>"]], "top_logit_ablation_values": [[-0.02375, 0.5769, 0.8972, 1.469, 2.03, 1.618, 1.947, 0.5751, 0], [-0.05788, 0.552, 0.8027, 1.292, 1.979, 0.8943, 0.9995, 0.5092, 0], [-0.0583, 0.5424, 0.7922, 1.23, 1.928, 0.8413, 0.9729, 0.4083, 0], [-0.06528, 0.5255, 0.7899, 1.223, 1.779, 0.8398, 0.9215, 0.3901, 0], [-0.06586, 0.516, 0.7865, 1.162, 1.747, 0.7886, 0.8062, 0.3697, 0]], "bottom_logit_ablation_values": [[-0.5352, -3.088, -1.412, -1.198, -3.351, -5.168, -4.851, -2.117, 0], [-0.5272, -3.081, -1.281, -1.177, -2.793, -5.051, -4.824, -2.098, 0], [-0.5158, -2.968, -1.268, -1.014, -2.744, -5.015, -4.606, -2.082, 0], [-0.5089, -2.944, -1.265, -0.9758, -2.582, -4.948, -4.515, -2.074, 0], [-0.504, -2.815, -1.235, -0.9436, -2.549, -4.925, -4.487, -2.07, 0]], "residual_loss_ablation": [-0.2377, -0.07058, 3.543, 0.285, 1.736, 0.9024, 1.284, 0.6169, 0]}}, {"tokens_str_list": [" not", " a", " concept", ",", " whereas", ",", " e", ".", "g"], "tokens_acts_list": [1.636, 3.039, 0, 1.108, 4.66, 1.619, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 66443586, "ablations": {"loss_ablation": [-0.3789, 0.01295, 0.5218, 0, -0.06067, 0.09759, 0.2371, 0, 0], "top_logit_ablation_tokens_str": [[" booked", " booked", "chedule", "<META>", " sir", "shirt", "shirt", "<META>", "<META>"], [" parked", " County", "shirt", "<EOT>", "shirt", " sweater", "cin", "<EOT>", "<EOT>"], [" County", "ched", " sweater", "<META_START>", "cin", " ago", " sir", "<META_START>", "<META_START>"], [" shipped", " parked", " County", "<SOS>", " huh", " TX", " uh", "<SOS>", "<SOS>"], [" remanded", " evacuated", " mall", "<META_END>", " though", " County", " respectively", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" inferences", " hypothesis", " truths", "<META>", " hypotheses", "tic", " disciplines", "<META>", "<META>"], [" hypotheses", " hypotheses", " hypotheses", "<EOT>", " truths", " hypotheses", " philosophical", "<EOT>", "<EOT>"], [" hypothesis", " inferences", " inferences", "<META_START>", " ontology", " disciplines", " empirical", "<META_START>", "<META_START>"], [" theories", "ologies", " ourselves", "<SOS>", " metaph", " phenomena", "itude", "<SOS>", "<SOS>"], [" assumptions", " insights", " assumptions", "<META_END>", " phenomena", "actic", " hypotheses", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.219, 0.7125, 0.4568, 0, 0.3048, -0.862, -0.174, 0, 0], [0.1987, 0.6077, 0.161, 0, 0.2253, -1.03, -0.2636, 0, 0], [0.1293, 0.5357, 0.112, 0, 0.1708, -1.173, -0.2667, 0, 0], [0.08718, 0.5088, 0.06007, 0, 0.1663, -1.426, -0.302, 0, 0], [0.06213, 0.4937, 0.05011, 0, 0.1387, -1.499, -0.3332, 0, 0]], "bottom_logit_ablation_values": [[-3.255, -2.089, -4.015, 0, -1.157, -7.581, -2.268, 0, 0], [-3.234, -1.959, -3.952, 0, -1.116, -7.524, -2.265, 0, 0], [-3.174, -1.956, -3.918, 0, -1.108, -7.46, -2.265, 0, 0], [-3.115, -1.953, -3.906, 0, -1.084, -7.422, -2.233, 0, 0], [-3.097, -1.919, -3.843, 0, -1.079, -7.357, -2.227, 0, 0]], "residual_loss_ablation": [-0.06212, 0.7647, 1.057, -0.1626, -0.2045, 0.7314, -1.174, 0.9021, 0]}}, {"tokens_str_list": [",", " and", " abstract", "s", " from", " all", " contents", " of", " understanding"], "tokens_acts_list": [2.08, 0.9769, 3.468, 3.429, 4.655, 3.656, 1.571, 3.21, 3.046], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 69106271, "ablations": {"loss_ablation": [0.3814, -0.2722, 0.3426, -1.472, -0.4595, -0.4832, 0.2833, -0.2802, 0], "top_logit_ablation_tokens_str": [[" ago", " County", " thereafter", "ed", "led", "duled", "ergic", " booked", "<META>"], ["============", " respectively", "liest", "ing", "pons", "shirt", "ales", " respectively", "<EOT>"], [" outdoors", " however", "\"}.", "s", "isters", " TX", "esville", "hire", "<META_START>"], [" afternoon", " according", " booked", "ols", "outheast", "boards", "ials", " shipped", "<SOS>"], [" after", " sir", "]{}]{}", "shirt", "hire", " County", "ickets", " County", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["\u00dfer", "utation", "ident", "forth", " truth", "ism", "gment", " philosophical", "<META>"], ["ition", " contexts", "utation", " Aristotle", " Reality", "\u03c1", " morality", " morality", "<EOT>"], ["icism", "ness", " empirical", " contradictory", "\u03c1", "leness", "minded", " reasoning", "<META_START>"], [" inferences", " empirical", "\u03c1", " \\xce", " reality", "\u03b3", " hypothesis", " empirical", "<SOS>"], ["tru", "ition", "ius", " plausible", " morality", "\\'", " existence", "inciple", "<META_END>"]], "top_logit_ablation_values": [[1.091, -0.2517, -0.3148, 1.923, 2.155, 0.2116, 2.92, -0.06701, 0], [0.9669, -0.2626, -0.3276, 1.224, 2.102, 0.08774, 2.671, -0.08116, 0], [0.3096, -0.2761, -0.3277, 0.7793, 2.008, 0.07818, 2.627, -0.08962, 0], [0.2967, -0.2994, -0.339, 0.7724, 1.972, -0.09979, 2.549, -0.09162, 0], [0.2945, -0.3025, -0.3492, 0.6765, 1.922, -0.1333, 2.457, -0.09291, 0]], "bottom_logit_ablation_values": [[-4.723, -2.405, -1.33, -4.311, -2.832, -6.634, -3.486, -1.992, 0], [-4.719, -2.399, -1.286, -3.656, -2.8, -6.418, -3.104, -1.953, 0], [-4.698, -2.396, -1.269, -3.612, -2.655, -6.177, -3.071, -1.921, 0], [-4.674, -2.386, -1.253, -3.6, -2.637, -6.159, -3.069, -1.919, 0], [-4.672, -2.378, -1.25, -3.527, -2.637, -6.017, -2.905, -1.919, 0]], "residual_loss_ablation": [1.476, 0.1804, -0.604, 7.462, 0.9826, 0.5507, 2.144, 1.382, 0]}}, {"tokens_str_list": [" particular", " case", " is", " resolved", " by", " being", " sub", "su", "med"], "tokens_acts_list": [2.345, 0.4707, 2.459, 2.472, 4.649, 3.29, 0.626, 0, 2.556], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 5918103, "ablations": {"loss_ablation": [-0.116, -0.5058, -0.01475, -0.4464, -0.1366, 0.3753, 0.2872, -0.01613, 0], "top_logit_ablation_tokens_str": [["orta", " weekend", "shirt", " remanded", " pursuant", "catch", " booked", "shirt", "<META>"], ["ched", " restaurant", "robe", " booked", " ashore", " ambulance", " evacuated", "oxide", "<EOT>"], ["oxide", " hotel", "inafter", " filed", "out", "passed", " notified", "led", "<META_START>"], ["irt", " notch", "out", " parked", "ette", "\"}.", " shipped", "sville", "<SOS>"], ["chedule", " Circuit", "worker", " shipped", " Wednesday", " spokeswoman", " repaired", "ixel", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" oneself", " oneself", " truths", " thinkers", " philosophers", "isms", "ness", " philosophical", "<META>"], [" ourselves", " Aristotle", " philosophers", " philosophers", "\u03c1", "\u03b3", "eness", " Aristotle", "<EOT>"], [" what", " ourselves", " philosophical", " interpretations", " Philosophy", "ism", " existence", " philosophers", "<META_START>"], [" itself", " what", " beings", "ness", " Aristotle", "ibility", " distinction", " empirical", "<SOS>"], [" how", " philosophers", " inferences", " inferences", " morality", " sake", "ology", " Descartes", "<META_END>"]], "top_logit_ablation_values": [[-0.3248, -0.9999, 0.001123, -0.3054, 0.303, -1.531, -0.4131, 0.09641, 0], [-0.4689, -1.011, -0.005138, -0.3632, 0.2347, -1.85, -0.6448, 0.01641, 0], [-0.5431, -1.047, -0.01377, -0.4289, 0.2269, -1.942, -0.7219, 0.009162, 0], [-0.5461, -1.058, -0.02129, -0.4752, 0.167, -2.038, -0.9165, 0.006664, 0], [-0.7196, -1.098, -0.03363, -0.595, 0.1445, -2.051, -0.9714, 0.00241, 0]], "bottom_logit_ablation_values": [[-7.011, -4.709, -0.6948, -4.11, -2.894, -8.814, -5.544, -0.7601, 0], [-6.95, -4.674, -0.689, -4.079, -2.836, -8.6, -5.445, -0.7566, 0], [-6.821, -4.615, -0.6863, -3.877, -2.799, -8.436, -5.314, -0.7458, 0], [-6.57, -4.56, -0.6748, -3.838, -2.757, -8.237, -5.265, -0.7442, 0], [-6.542, -4.538, -0.6531, -3.766, -2.748, -8.03, -5.228, -0.742, 0]], "residual_loss_ablation": [0.6165, 2.337, 3.173, 1.232, 1.001, 0.666, 1.008, 0.4726, 0]}}, {"tokens_str_list": [" rational", " plays", ",", " as", " opposed", " to", " more", " mund", "ane"], "tokens_acts_list": [1.604, 0.7656, 0.1458, 2.808, 4.638, 3.447, 2.488, 0, 1.5], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 38516050, "ablations": {"loss_ablation": [0.6193, 0.6789, 0.08668, -0.01505, -0.7805, -0.04085, 0.1754, 0.1486, 0], "top_logit_ablation_tokens_str": [[" booked", "ded", "chedule", " huh", "oxide", "chedule", "shirt", "sville", "<META>"], ["amide", "oted", "heed", " sir", "per", "============", "chedule", "stown", "<EOT>"], ["eper", "ized", "out", " including", "pool", "isters", " Trump", "boards", "<META_START>"], [" parked", "istically", "ulf", " buddy", "chedule", " ashore", "sville", "chedule", "<SOS>"], [" filed", "ised", "heet", " especially", "ht", " ago", " ranch", "heed", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["\u043b\u044f", " Aristotle", " ontology", "asures", "utable", " sake", "ity", "ifies", "<META>"], [" consciousness", "ingu", " reasoning", " oneself", "ENCE", "asing", "ateral", " Descartes", "<EOT>"], [" existence", " Descartes", " negation", "mathbf", "ation", "istem", "ential", "arity", "<META_START>"], [" notions", " linguistic", " assumptions", "utation", " meanings", " ontology", "ibility", "ity", "<SOS>"], [" negation", "\\xe1", " meanings", "itions", "arrative", "notations", " follows", " negation", "<META_END>"]], "top_logit_ablation_values": [[0.1414, 0.5465, 0.32, 0.02591, 0.5296, -0.1069, -0.1627, 0.66, 0], [0.0671, 0.5287, 0.308, 0.02525, 0.5278, -0.1608, -0.3767, 0.6312, 0], [0.06506, 0.4471, 0.25, 0.01834, 0.4428, -0.2205, -0.4934, 0.5999, 0], [0.03196, 0.4373, 0.226, 0.01741, 0.4305, -0.4556, -0.5097, 0.5837, 0], [0.01731, 0.4277, 0.2245, 0.01715, 0.4134, -0.4559, -0.5111, 0.4967, 0]], "bottom_logit_ablation_values": [[-1.639, -1.807, -0.8559, -0.1393, -3.462, -6.322, -5.3, -3.358, 0], [-1.616, -1.763, -0.8496, -0.1388, -3.339, -6.119, -5.171, -3.294, 0], [-1.609, -1.74, -0.8316, -0.1386, -3.203, -5.954, -5.096, -3.108, 0], [-1.608, -1.726, -0.8297, -0.1358, -3.153, -5.94, -5.069, -3.049, 0], [-1.595, -1.721, -0.8202, -0.1347, -3.105, -5.872, -5.029, -3.046, 0]], "residual_loss_ablation": [2.793, -1.115, 1.12, 0.06285, 1.528, 0.2552, -0.1374, 0.7126, 0]}}, {"tokens_str_list": [" negation", " acts", " in", " combination", " with", " other", " arrows", ",", " which"], "tokens_acts_list": [0.8392, 0.5299, 3.717, 0.9065, 4.618, 4.076, 0.112, 1.82, 2.916], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 54966951, "ablations": {"loss_ablation": [0.4268, 0.2318, -0.001956, -0.8406, -0.2206, -0.5374, -0.6217, -0.001186, 0], "top_logit_ablation_tokens_str": [[" Ltd", "oa", "chedule", "shirt", "led", "shirt", "shirt", " (@", "<META>"], ["shirts", "shirt", "peed", "osit", "ved", "shirts", "={", "shirt", "<EOT>"], [" County", "hire", "pace", " inbox", "hire", " Sabha", " Charg", "hire", "<META_START>"], [" TX", "led", "heed", " County", "amycin", "bags", "shots", "arrow", "<SOS>"], ["shirt", "rangle", "heet", "amycin", "============", "ipts", "ials", "letters", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["itude", " philosophical", " philosophical", " forth", " philosophical", " philosophical", " ourselves", " philosophical", "<META>"], [" existence", " empirical", "ernate", "ifies", " empirical", " scientific", " Descartes", " moral", "<EOT>"], [".", " moral", " existence", "\u03c1", " certainty", " philosophers", " Aristotle", " Aristotle", "<META_START>"], ["ism", " metaph", " empirical", "\u03c3", " truths", " morality", " morality", " morality", "<SOS>"], [" philosophical", " Aristotle", " truths", " empirical", " Aristotle", " empirical", " Freud", " philosophy", "<META_END>"]], "top_logit_ablation_values": [[-0.2076, -0.02077, 0.06922, 0.2188, 0.2424, 0.7918, -0.8051, 0.003507, 0], [-0.2204, -0.04966, 0.02261, 0.2135, 0.2051, 0.3618, -0.8884, 0.00255, 0], [-0.2469, -0.06866, 0.01787, 0.08837, 0.1887, -0.3663, -1, 0.002312, 0], [-0.2591, -0.06898, 0.01559, -0.01323, 0.08004, -0.4525, -1.008, -0.004527, 0], [-0.2616, -0.08606, 0.01012, -0.0187, 0.07918, -0.5056, -1.036, -0.005425, 0]], "bottom_logit_ablation_values": [[-1.369, -1.155, -0.6339, -5.313, -1.127, -6.753, -6.845, -0.1442, 0], [-1.316, -1.089, -0.6241, -5.051, -1.083, -6.358, -6.788, -0.1355, 0], [-1.268, -1.072, -0.6172, -5.025, -1.074, -6.337, -6.773, -0.1351, 0], [-1.262, -1.071, -0.6166, -5.021, -1.071, -6.319, -6.751, -0.1347, 0], [-1.256, -1.058, -0.6072, -4.929, -1.055, -6.313, -6.747, -0.1336, 0]], "residual_loss_ablation": [2.261, 3.298, -0.4608, 1.082, 2.534, -0.125, 3.207, 0.7974, 0]}}, {"tokens_str_list": [" no", " small", " order", ".", " Neither", " Qu", "ine", " nor", " any"], "tokens_acts_list": [1.986, 1.611, 0.5993, 1.208, 4.61, 0, 2.946, 1.924, 2.865], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 91350903, "ablations": {"loss_ablation": [0.2048, 0.05685, 0.1095, -0.05963, 0.08702, 0.1495, 0, -0.2039, 0], "top_logit_ablation_tokens_str": [[" rinsed", "zzles", "gie", "shirt", " County", "ved", "<META>", "days", "<META>"], [" booked", "sed", " cooker", "ixel", "shirt", "oa", "<EOT>", "cks", "<EOT>"], ["oprop", " longer", "}$.", " cooker", "hire", "grant", "<META_START>", "shirt", "<META_START>"], [" served", "isfactory", " bowl", " Makeup", "AP", "u\u00e7\u00e3o", "<SOS>", "burg", "<SOS>"], [" mounted", "chedule", " skillet", "lace", " Services", "unded", "<META_END>", "oa", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["ity", " ourselves", "osing", " Descartes", " inferences", " meanings", "<META>", " meanings", "<META>"], [" ourselves", "){#", "\u03c0", "icism", " phenomena", " consciousness", "<EOT>", " causal", "<EOT>"], [" hypothesis", "\\xe1", "icism", " ontology", " meanings", " inferences", "<META_START>", " ontology", "<META_START>"], [" inferences", "ness", " Descartes", "\\xe1", " oneself", " existence", "<SOS>", " philosophical", "<SOS>"], ["mitives", "\u03c0", " these", "\u03c1", " truths", "ential", "<META_END>", " mathematical", "<META_END>"]], "top_logit_ablation_values": [[0.7295, 0.6438, 0.4164, 0.05868, -0.3972, -0.5541, 0, 0.3081, 0], [0.7178, 0.6243, 0.254, 0.03642, -0.3984, -0.683, 0, 0.1816, 0], [0.7077, 0.6052, 0.2208, 0.03516, -0.4375, -0.7381, 0, 0.1439, 0], [0.6769, 0.597, 0.217, 0.02696, -0.4453, -0.7391, 0, 0.1426, 0], [0.6669, 0.5742, 0.2112, 0.01261, -0.4726, -0.7562, 0, 0.1325, 0]], "bottom_logit_ablation_values": [[-3.343, -2.092, -1.96, -0.8294, -2.21, -7.367, 0, -3.534, 0], [-3.246, -2.004, -1.889, -0.8041, -2.206, -7.071, 0, -3.528, 0], [-3.207, -1.993, -1.857, -0.7905, -2.205, -6.981, 0, -3.524, 0], [-3.179, -1.991, -1.843, -0.7867, -2.192, -6.931, 0, -3.508, 0], [-3.062, -1.975, -1.803, -0.7699, -2.19, -6.928, 0, -3.46, 0]], "residual_loss_ablation": [0.9359, 0.1756, -0.7495, 0.3458, -0.7534, 0.7004, 1.755, 5.821, 0]}}, {"tokens_str_list": [" be", " real", " definitions", ",", " for", " example", ",", " that", " Pe"], "tokens_acts_list": [3.013, 2.737, 1.175, 2.147, 4.605, 1.953, 1.716, 3.571, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 14136797, "ablations": {"loss_ablation": [-0.3024, 0.4231, 1.133, 0.0837, -0.2904, -0.6549, -0.0646, 0.13, 0], "top_logit_ablation_tokens_str": [["hire", " booked", "locate", "heet", " including", "shirt", "shirt", " TX", "<META>"], ["shirt", " rinsed", "ded", "burg", " however", "shirts", "ixel", "shirt", "<EOT>"], ["pillar", " cleaned", "located", "hire", " preferably", "hire", "shirts", " cans", "<META_START>"], ["ette", " purchased", "istically", "park", " huh", " TX", "hire", " County", "<SOS>"], ["chedule", " shipped", "fits", "days", "shirts", " sweater", " TX", "shirts", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["itics", " hypothesis", " Aristotle", " philosophical", " oneself", "hea", " philosophical", "mathbf", "<META>"], [" reasoning", "ortality", " Descartes", " consciousness", " himself", "lected", " empirical", " causal", "<EOT>"], [" relativity", "ness", " hypothesis", " existence", "licity", "\u03b3", " causal", "itude", "<META_START>"], [" philosophical", " existence", " empirical", " empirical", " causal", "lections", " metaph", " philosophical", "<SOS>"], [" Philosophy", "eness", " Philosophy", " intuition", "utable", " relativity", " moral", " empirical", "<META_END>"]], "top_logit_ablation_values": [[0.5063, 0.8493, 2.49, 0.3156, 0.86, 1.96, 0.3944, -0.07871, 0], [0.4816, 0.7762, 2.427, 0.2832, 0.8462, 1.859, 0.3938, -0.08351, 0], [0.3548, 0.7604, 2.304, 0.2362, 0.8008, 1.749, 0.2939, -0.09544, 0], [0.3367, 0.7243, 2.227, 0.2253, 0.7539, 1.17, 0.2342, -0.1019, 0], [0.3366, 0.6677, 2.087, 0.22, 0.7459, 1.167, 0.2104, -0.1296, 0]], "bottom_logit_ablation_values": [[-2.944, -4.692, -2.066, -1.239, -1.742, -5.289, -2.323, -1.954, 0], [-2.906, -4.425, -1.861, -1.219, -1.674, -5.214, -2.281, -1.946, 0], [-2.895, -4.34, -1.742, -1.211, -1.651, -4.843, -2.242, -1.886, 0], [-2.883, -4.2, -1.621, -1.206, -1.624, -4.828, -2.219, -1.869, 0], [-2.786, -4.137, -1.609, -1.2, -1.6, -4.774, -2.213, -1.867, 0]], "residual_loss_ablation": [0.2695, 0.3667, 2.537, -0.1642, 0.1547, 4.128, -0.1306, 0.1327, 0]}}]}, {"quantile_name": "Subsample Interval 0", "quantile_max_act": 4.18, "examples": [{"tokens_str_list": [" reasoning", " about", " explanation", ",", " namely", ",", " the", " Pr", "inciple"], "tokens_acts_list": [0, 4.63, 0, 1.782, 4.18, 1.477, 1.831, 0, 0.9743], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 20073477, "ablations": {"loss_ablation": [0.9891, 0, 2.696, 0, -0.1865, -0.3741, 0.1833, -0.1137, 0], "top_logit_ablation_tokens_str": [["eral", "<META>", "sville", "<META>", " huh", " daddy", " daddy", "chedule", "<META>"], ["oa", "<EOT>", " County", "<EOT>", " sir", "iper", " Holder", " District", "<EOT>"], ["ities", "<META_START>", "iper", "<META_START>", " County", " TX", " sir", " rate", "<META_START>"], ["ellee", "<SOS>", "shirt", "<SOS>", "AFF", " jail", "shirt", " Diocese", "<SOS>"], ["oid", "<META_END>", " parked", "<META_END>", " while", "shirt", " huh", " youngest", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["utable", "<META>", "ibility", "<META>", "ditions", "eced", " contexts", " its", "<META>"], [" analogous", "<EOT>", " meanings", "<EOT>", " oneself", "\u03c3", " disciplines", " oneself", "<EOT>"], [" explicitly", "<META_START>", " inferences", "<META_START>", " contexts", "\u03c1", " phenomena", "actic", "<META_START>"], [" contradictory", "<SOS>", "matical", "<SOS>", "ness", " meanings", " philosophical", " any", "<SOS>"], [" clearer", "<META_END>", " interpretations", "<META_END>", " ontology", " conceptual", " metaph", "ceive", "<META_END>"]], "top_logit_ablation_values": [[1.014, 0, -0.8287, 0, 0.3653, -1.405, -0.205, -0.2251, 0], [0.921, 0, -1.006, 0, 0.2941, -1.413, -0.2229, -0.2281, 0], [0.8645, 0, -1.32, 0, 0.2055, -1.437, -0.231, -0.3193, 0], [0.782, 0, -1.322, 0, 0.1661, -1.478, -0.2412, -0.3246, 0], [0.7751, 0, -1.372, 0, 0.1608, -1.518, -0.2685, -0.3304, 0]], "bottom_logit_ablation_values": [[-2.612, 0, -7.905, 0, -1.811, -6.256, -1.84, -2.288, 0], [-2.562, 0, -7.884, 0, -1.743, -6.21, -1.769, -2.252, 0], [-2.537, 0, -7.843, 0, -1.734, -6.053, -1.732, -2.226, 0], [-2.492, 0, -7.716, 0, -1.716, -6.04, -1.72, -2.212, 0], [-2.465, 0, -7.609, 0, -1.711, -6.023, -1.714, -2.199, 0]], "residual_loss_ablation": [1.73, 0.2261, -0.9955, -1.105, 0.6055, 2.542, -0.02434, -0.2044, 0]}}, {"tokens_str_list": [",", " as", " a", " thought", " about", " or", " belief", " in", " that"], "tokens_acts_list": [0.7277, 3.313, 2.896, 0.4703, 4.01, 0.8939, 0, 3.674, 3.294], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 24254147, "ablations": {"loss_ablation": [0.01399, -0.03652, 0.2712, 0.8067, 0.03237, -0.4507, 0.2307, 0, 0], "top_logit_ablation_tokens_str": [["ports", " booked", " TX", " midfielder", "out", "ahoe", "deal", "<META>", "<META>"], ["hire", " airport", " booked", " carrier", "week", "shirt", "phans", "<EOT>", "<EOT>"], [" picker", " TX", "shirt", " photographer", "abeth", "============", "gie", "<META_START>", "<META_START>"], ["stations", " installed", "per", "chedule", "rile", "ixel", "naments", "<SOS>", "<SOS>"], ["ed", "area", " Reserved", " teenager", "boat", " Springs", " outdoors", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", "\u03c0", " interpretations", " ourselves", " inferences", " inferences", "positions", "<META>", "<META>"], [" moral", "mathbf", "\u03c1", " what", " interpretations", " conclusions", " philosophers", "<EOT>", "<EOT>"], ["\u03c1", "###", " interpretation", " Aristotle", " philosophical", " interpretations", " Aristotle", "<META_START>", "<META_START>"], ["\u03c0", "\u03b3", " intuit", " truths", " philosophers", " meanings", " Descartes", "<SOS>", "<SOS>"], ["\u03b1", "####", " meanings", " how", "utable", "udes", " Philosophy", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.009695, 0.1057, 0.6696, -0.3889, 0.03235, 0.8418, 0.3881, 0, 0], [0.006867, 0.1053, 0.4154, -0.464, 0.02052, 0.7271, 0.3832, 0, 0], [0.004849, 0.1019, 0.4035, -0.4804, 0.01717, 0.5446, 0.3758, 0, 0], [0.004744, 0.1018, 0.3786, -0.5029, 0.01391, 0.4198, 0.3652, 0, 0], [-0.001758, 0.1011, 0.3628, -0.5158, 0.01376, 0.3389, 0.3311, 0, 0]], "bottom_logit_ablation_values": [[-0.9212, -0.7032, -4.203, -4.316, -0.6575, -5.754, -0.9423, 0, 0], [-0.916, -0.7002, -4.167, -4.172, -0.6364, -5.458, -0.9048, 0, 0], [-0.9059, -0.6643, -4.066, -4.11, -0.6293, -5.414, -0.9018, 0, 0], [-0.8966, -0.6326, -4.065, -4.089, -0.6284, -5.366, -0.8997, 0, 0], [-0.8735, -0.6314, -4.064, -4.067, -0.6279, -5.259, -0.8988, 0, 0]], "residual_loss_ablation": [0.4757, -0.4495, -0.6634, 0.6809, -2.799, 1.477, 1.687, 2.167, 0]}}, {"tokens_str_list": [" the", " pragmatic", " theory", " include", " that", " it", " conf", "uses", " truth"], "tokens_acts_list": [0.699, 0.7147, 0.05399, 3.743, 3.56, 1.915, 0, 1.297, 1.159], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 6371147, "ablations": {"loss_ablation": [0.3752, 0.1573, 0.04303, -0.002297, 0.04117, 0.1866, 0.3958, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "chedule", "oted", " Makeup", "shirt", "shirt", "inerary", "<META>", "<META>"], [" County", " inbox", "runch", " of", "shirts", "poon", "burg", "<EOT>", "<EOT>"], ["chedule", " County", "reated", "led", "noon", " Makeup", " wore", "<META_START>", "<META_START>"], ["kins", " Oval", " midfielder", "rooms", "pons", "ctomy", " shipped", "<SOS>", "<SOS>"], [" inbox", "esville", " District", " (@", "ries", "ahawks", "als", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["ential", " its", " MERCHANTABILITY", "antically", "omorphic", "utable", " consciousness", "<META>", "<META>"], ["ical", " oneself", " any", "\u03c1", "ensible", "\u03c0", " ontology", "<EOT>", "<EOT>"], ["ateral", " anything", " its", "\u03c6", "utable", "asm", " philosophical", "<META_START>", "<META_START>"], ["ence", ":", "\u03c3", "\u03ba", " oneself", " oneself", " Philosophy", "<SOS>", "<SOS>"], ["asing", " something", "\u03bb", "\u03c3", "%\"", "itatively", " Aristotle", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[-0.2048, -0.01705, 0.1796, 0.01554, 0.8116, 1.286, 0.3097, 0, 0], [-0.2775, -0.02786, 0.1351, 0.01449, 0.6571, 0.9409, 0.2562, 0, 0], [-0.375, -0.04043, 0.1317, 0.01329, 0.6078, 0.7511, 0.2398, 0, 0], [-0.4239, -0.05715, 0.1191, 0.01327, 0.4589, 0.7467, 0.2216, 0, 0], [-0.5371, -0.05842, 0.1126, 0.01134, 0.3687, 0.7173, 0.217, 0, 0]], "bottom_logit_ablation_values": [[-4.427, -0.9279, -0.8522, -0.05493, -3.916, -4.113, -2.48, 0, 0], [-4.085, -0.9104, -0.752, -0.04949, -3.797, -4.093, -2.451, 0, 0], [-4.067, -0.8981, -0.7408, -0.04869, -3.787, -4.074, -2.421, 0, 0], [-4.04, -0.8934, -0.734, -0.04846, -3.754, -4.045, -2.383, 0, 0], [-4.008, -0.8861, -0.7328, -0.04823, -3.71, -4.036, -2.381, 0, 0]], "residual_loss_ablation": [-0.1225, 0.1095, 1.691, 4.593, 0.5319, -0.5531, 1.03, 1.638, 0]}}, {"tokens_str_list": [" in", " any", " terms", " other", " than", " themselves", ".", " (", "Other"], "tokens_acts_list": [3.567, 2.869, 0.6568, 3.646, 3.399, 2.696, 0.4351, 1.715, 2.717], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 40825363, "ablations": {"loss_ablation": [-0.1013, -0.1497, 0.7157, 0.2108, -0.4991, 0.395, -0.3807, 0.1123, 0], "top_logit_ablation_tokens_str": [[" ashore", "shirt", " County", "leneck", " than", " TX", " booked", "shirt", "<META>"], [" Wednesday", "osit", " weekend", "borough", " (@", "chedule", "peed", " County", "<EOT>"], [" Monday", "munk", " mall", " TX", "lights", "ahoe", " parked", "hire", "<META_START>"], [" pursuant", " County", "shirt", "boro", "ials", " booked", " shipped", " TX", "<SOS>"], [" Thursday", " TX", " TX", "peed", "spots", "days", " (@", "chedule", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "ism", " Descartes", " philosophical", " Marx", " Philosophy", " ontology", " philosophical", "<META>"], [" philosophers", "isms", " oneself", " Philosophy", " Descartes", " philosophical", " philosophical", " metaph", "<EOT>"], [" Philosophy", "ifies", "ism", " metaph", "gel", "ness", " metaph", " ontology", "<META_START>"], ["icism", " philosophical", " Aristotle", " philosophers", " Philosophy", "\u03c1", " Philosophy", " empirical", "<SOS>"], [" ontology", " philosophers", "licity", " Descartes", " oneself", " interpretation", " mathematics", " truths", "<META_END>"]], "top_logit_ablation_values": [[-0.08118, -0.6803, -0.0773, 0.2407, -0.4366, 0.585, 0.01737, -0.07149, 0], [-0.1394, -0.8635, -0.2183, 0.2387, -0.6437, -0.1329, -0.1347, -0.09073, 0], [-0.1407, -0.9923, -0.2837, 0.2362, -0.803, -0.3595, -0.156, -0.104, 0], [-0.1825, -1.009, -0.2978, 0.2317, -0.8068, -0.4066, -0.1967, -0.1147, 0], [-0.1867, -1.024, -0.3007, 0.2314, -0.8579, -0.4283, -0.2119, -0.1213, 0]], "bottom_logit_ablation_values": [[-3.098, -6.819, -4.35, -0.7243, -6.217, -5.183, -4.311, -0.7322, 0], [-3.028, -6.462, -4.277, -0.7169, -6.18, -5.126, -4.184, -0.7171, 0], [-3.022, -6.445, -4.258, -0.6877, -6.089, -4.846, -4.113, -0.7036, 0], [-3.004, -6.357, -4.234, -0.6874, -6.075, -4.814, -4.093, -0.6945, 0], [-2.989, -6.325, -4.201, -0.6843, -6.074, -4.814, -3.949, -0.6921, 0]], "residual_loss_ablation": [-0.1606, 1.117, -0.661, 0.1159, 3.822, 2.716, 1.754, -0.01619, 0]}}, {"tokens_str_list": [",", " as", " opposed", " to", " being", " merely", " an", " arte", "fact"], "tokens_acts_list": [1.112, 3.226, 4.982, 4.079, 3.386, 3.61, 2.762, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 54052290, "ablations": {"loss_ablation": [0.1739, -0.09512, -1.019, -0.04132, -0.07591, 0.9046, -0.04063, -0.03998, 0], "top_logit_ablation_tokens_str": [["sed", " pursuant", "per", "isters", "ixel", " booked", "amide", "shirt", "<META>"], ["chedule", " sir", " Reserved", "============", "shirt", " fired", "ixel", "ixel", "<EOT>"], ["iator", " booked", "led", " thereto", "iper", " notified", " booked", " ambulance", "<META_START>"], ["ixel", " preferably", " soon", " to", "ameth", " knocked", " evacuated", " SUV", "<SOS>"], ["led", " respectively", " booked", " ago", " Rs", " assaulted", "ali", " actress", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "onomy", "utable", "istem", "\u03b1", " Aristotle", "selves", "ifies", "<META>"], [" philosophers", "ition", " metaph", "otic", "esis", " philosophical", " philosophers", " certain", "<EOT>"], [" Marx", " ontology", "ificance", " morality", " philosophical", "ness", " Aristotle", " us", "<META_START>"], [" contradictory", " theories", "onom", " rational", "\u03c0", " metaph", "eness", " Aristotle", "<SOS>"], [" philosophical", ":", " truths", " Philosophy", "ism", " philosophers", " Descartes", " philosophers", "<META_END>"]], "top_logit_ablation_values": [[0.2861, -0.1497, 0.6857, 0.3425, 0.1143, -0.17, 0.1683, -0.1177, 0], [0.1634, -0.1693, 0.3772, 0.3348, -0.06042, -0.3724, 0.03883, -0.285, 0], [0.0989, -0.1734, 0.3562, 0.1814, -0.1264, -0.409, -0.0803, -0.4839, 0], [0.08803, -0.1899, 0.3484, 0.04497, -0.5583, -0.4593, -0.275, -0.5407, 0], [0.05215, -0.1903, 0.3428, 0.01771, -0.565, -0.4601, -0.291, -0.5841, 0]], "bottom_logit_ablation_values": [[-2.543, -1.406, -4.075, -6.674, -6.574, -5.157, -5.685, -4.424, 0], [-2.316, -1.402, -3.885, -6.452, -6.526, -5.104, -5.494, -4.345, 0], [-2.247, -1.396, -3.852, -6.367, -6.387, -5.038, -5.41, -4.312, 0], [-2.24, -1.365, -3.843, -6.31, -6.343, -4.99, -5.343, -4.297, 0], [-2.16, -1.358, -3.795, -6.281, -6.263, -4.987, -5.337, -4.237, 0]], "residual_loss_ablation": [-0.791, -0.05398, 1.056, 0.7485, 2.526, 1.206, 0.6959, 0.04469, 0]}}]}, {"quantile_name": "Subsample Interval 1", "quantile_max_act": 4.18, "examples": [{"tokens_str_list": [" reasoning", " about", " explanation", ",", " namely", ",", " the", " Pr", "inciple"], "tokens_acts_list": [0, 4.63, 0, 1.782, 4.18, 1.477, 1.831, 0, 0.9743], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 20073477, "ablations": {"loss_ablation": [0.9891, 0, 2.696, 0, -0.1865, -0.3741, 0.1833, -0.1137, 0], "top_logit_ablation_tokens_str": [["eral", "<META>", "sville", "<META>", " huh", " daddy", " daddy", "chedule", "<META>"], ["oa", "<EOT>", " County", "<EOT>", " sir", "iper", " Holder", " District", "<EOT>"], ["ities", "<META_START>", "iper", "<META_START>", " County", " TX", " sir", " rate", "<META_START>"], ["ellee", "<SOS>", "shirt", "<SOS>", "AFF", " jail", "shirt", " Diocese", "<SOS>"], ["oid", "<META_END>", " parked", "<META_END>", " while", "shirt", " huh", " youngest", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["utable", "<META>", "ibility", "<META>", "ditions", "eced", " contexts", " its", "<META>"], [" analogous", "<EOT>", " meanings", "<EOT>", " oneself", "\u03c3", " disciplines", " oneself", "<EOT>"], [" explicitly", "<META_START>", " inferences", "<META_START>", " contexts", "\u03c1", " phenomena", "actic", "<META_START>"], [" contradictory", "<SOS>", "matical", "<SOS>", "ness", " meanings", " philosophical", " any", "<SOS>"], [" clearer", "<META_END>", " interpretations", "<META_END>", " ontology", " conceptual", " metaph", "ceive", "<META_END>"]], "top_logit_ablation_values": [[1.014, 0, -0.8287, 0, 0.3653, -1.405, -0.205, -0.2251, 0], [0.921, 0, -1.006, 0, 0.2941, -1.413, -0.2229, -0.2281, 0], [0.8645, 0, -1.32, 0, 0.2055, -1.437, -0.231, -0.3193, 0], [0.782, 0, -1.322, 0, 0.1661, -1.478, -0.2412, -0.3246, 0], [0.7751, 0, -1.372, 0, 0.1608, -1.518, -0.2685, -0.3304, 0]], "bottom_logit_ablation_values": [[-2.612, 0, -7.905, 0, -1.811, -6.256, -1.84, -2.288, 0], [-2.562, 0, -7.884, 0, -1.743, -6.21, -1.769, -2.252, 0], [-2.537, 0, -7.843, 0, -1.734, -6.053, -1.732, -2.226, 0], [-2.492, 0, -7.716, 0, -1.716, -6.04, -1.72, -2.212, 0], [-2.465, 0, -7.609, 0, -1.711, -6.023, -1.714, -2.199, 0]], "residual_loss_ablation": [1.73, 0.2261, -0.9955, -1.105, 0.6055, 2.542, -0.02434, -0.2044, 0]}}, {"tokens_str_list": [",", " as", " a", " thought", " about", " or", " belief", " in", " that"], "tokens_acts_list": [0.7277, 3.313, 2.896, 0.4703, 4.01, 0.8939, 0, 3.674, 3.294], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 24254147, "ablations": {"loss_ablation": [0.01399, -0.03652, 0.2712, 0.8067, 0.03237, -0.4507, 0.2307, 0, 0], "top_logit_ablation_tokens_str": [["ports", " booked", " TX", " midfielder", "out", "ahoe", "deal", "<META>", "<META>"], ["hire", " airport", " booked", " carrier", "week", "shirt", "phans", "<EOT>", "<EOT>"], [" picker", " TX", "shirt", " photographer", "abeth", "============", "gie", "<META_START>", "<META_START>"], ["stations", " installed", "per", "chedule", "rile", "ixel", "naments", "<SOS>", "<SOS>"], ["ed", "area", " Reserved", " teenager", "boat", " Springs", " outdoors", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", "\u03c0", " interpretations", " ourselves", " inferences", " inferences", "positions", "<META>", "<META>"], [" moral", "mathbf", "\u03c1", " what", " interpretations", " conclusions", " philosophers", "<EOT>", "<EOT>"], ["\u03c1", "###", " interpretation", " Aristotle", " philosophical", " interpretations", " Aristotle", "<META_START>", "<META_START>"], ["\u03c0", "\u03b3", " intuit", " truths", " philosophers", " meanings", " Descartes", "<SOS>", "<SOS>"], ["\u03b1", "####", " meanings", " how", "utable", "udes", " Philosophy", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.009695, 0.1057, 0.6696, -0.3889, 0.03235, 0.8418, 0.3881, 0, 0], [0.006867, 0.1053, 0.4154, -0.464, 0.02052, 0.7271, 0.3832, 0, 0], [0.004849, 0.1019, 0.4035, -0.4804, 0.01717, 0.5446, 0.3758, 0, 0], [0.004744, 0.1018, 0.3786, -0.5029, 0.01391, 0.4198, 0.3652, 0, 0], [-0.001758, 0.1011, 0.3628, -0.5158, 0.01376, 0.3389, 0.3311, 0, 0]], "bottom_logit_ablation_values": [[-0.9212, -0.7032, -4.203, -4.316, -0.6575, -5.754, -0.9423, 0, 0], [-0.916, -0.7002, -4.167, -4.172, -0.6364, -5.458, -0.9048, 0, 0], [-0.9059, -0.6643, -4.066, -4.11, -0.6293, -5.414, -0.9018, 0, 0], [-0.8966, -0.6326, -4.065, -4.089, -0.6284, -5.366, -0.8997, 0, 0], [-0.8735, -0.6314, -4.064, -4.067, -0.6279, -5.259, -0.8988, 0, 0]], "residual_loss_ablation": [0.4757, -0.4495, -0.6634, 0.6809, -2.799, 1.477, 1.687, 2.167, 0]}}, {"tokens_str_list": [" the", " pragmatic", " theory", " include", " that", " it", " conf", "uses", " truth"], "tokens_acts_list": [0.699, 0.7147, 0.05399, 3.743, 3.56, 1.915, 0, 1.297, 1.159], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 6371147, "ablations": {"loss_ablation": [0.3752, 0.1573, 0.04303, -0.002297, 0.04117, 0.1866, 0.3958, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "chedule", "oted", " Makeup", "shirt", "shirt", "inerary", "<META>", "<META>"], [" County", " inbox", "runch", " of", "shirts", "poon", "burg", "<EOT>", "<EOT>"], ["chedule", " County", "reated", "led", "noon", " Makeup", " wore", "<META_START>", "<META_START>"], ["kins", " Oval", " midfielder", "rooms", "pons", "ctomy", " shipped", "<SOS>", "<SOS>"], [" inbox", "esville", " District", " (@", "ries", "ahawks", "als", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["ential", " its", " MERCHANTABILITY", "antically", "omorphic", "utable", " consciousness", "<META>", "<META>"], ["ical", " oneself", " any", "\u03c1", "ensible", "\u03c0", " ontology", "<EOT>", "<EOT>"], ["ateral", " anything", " its", "\u03c6", "utable", "asm", " philosophical", "<META_START>", "<META_START>"], ["ence", ":", "\u03c3", "\u03ba", " oneself", " oneself", " Philosophy", "<SOS>", "<SOS>"], ["asing", " something", "\u03bb", "\u03c3", "%\"", "itatively", " Aristotle", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[-0.2048, -0.01705, 0.1796, 0.01554, 0.8116, 1.286, 0.3097, 0, 0], [-0.2775, -0.02786, 0.1351, 0.01449, 0.6571, 0.9409, 0.2562, 0, 0], [-0.375, -0.04043, 0.1317, 0.01329, 0.6078, 0.7511, 0.2398, 0, 0], [-0.4239, -0.05715, 0.1191, 0.01327, 0.4589, 0.7467, 0.2216, 0, 0], [-0.5371, -0.05842, 0.1126, 0.01134, 0.3687, 0.7173, 0.217, 0, 0]], "bottom_logit_ablation_values": [[-4.427, -0.9279, -0.8522, -0.05493, -3.916, -4.113, -2.48, 0, 0], [-4.085, -0.9104, -0.752, -0.04949, -3.797, -4.093, -2.451, 0, 0], [-4.067, -0.8981, -0.7408, -0.04869, -3.787, -4.074, -2.421, 0, 0], [-4.04, -0.8934, -0.734, -0.04846, -3.754, -4.045, -2.383, 0, 0], [-4.008, -0.8861, -0.7328, -0.04823, -3.71, -4.036, -2.381, 0, 0]], "residual_loss_ablation": [-0.1225, 0.1095, 1.691, 4.593, 0.5319, -0.5531, 1.03, 1.638, 0]}}, {"tokens_str_list": [" in", " any", " terms", " other", " than", " themselves", ".", " (", "Other"], "tokens_acts_list": [3.567, 2.869, 0.6568, 3.646, 3.399, 2.696, 0.4351, 1.715, 2.717], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 40825363, "ablations": {"loss_ablation": [-0.1013, -0.1497, 0.7157, 0.2108, -0.4991, 0.395, -0.3807, 0.1123, 0], "top_logit_ablation_tokens_str": [[" ashore", "shirt", " County", "leneck", " than", " TX", " booked", "shirt", "<META>"], [" Wednesday", "osit", " weekend", "borough", " (@", "chedule", "peed", " County", "<EOT>"], [" Monday", "munk", " mall", " TX", "lights", "ahoe", " parked", "hire", "<META_START>"], [" pursuant", " County", "shirt", "boro", "ials", " booked", " shipped", " TX", "<SOS>"], [" Thursday", " TX", " TX", "peed", "spots", "days", " (@", "chedule", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "ism", " Descartes", " philosophical", " Marx", " Philosophy", " ontology", " philosophical", "<META>"], [" philosophers", "isms", " oneself", " Philosophy", " Descartes", " philosophical", " philosophical", " metaph", "<EOT>"], [" Philosophy", "ifies", "ism", " metaph", "gel", "ness", " metaph", " ontology", "<META_START>"], ["icism", " philosophical", " Aristotle", " philosophers", " Philosophy", "\u03c1", " Philosophy", " empirical", "<SOS>"], [" ontology", " philosophers", "licity", " Descartes", " oneself", " interpretation", " mathematics", " truths", "<META_END>"]], "top_logit_ablation_values": [[-0.08118, -0.6803, -0.0773, 0.2407, -0.4366, 0.585, 0.01737, -0.07149, 0], [-0.1394, -0.8635, -0.2183, 0.2387, -0.6437, -0.1329, -0.1347, -0.09073, 0], [-0.1407, -0.9923, -0.2837, 0.2362, -0.803, -0.3595, -0.156, -0.104, 0], [-0.1825, -1.009, -0.2978, 0.2317, -0.8068, -0.4066, -0.1967, -0.1147, 0], [-0.1867, -1.024, -0.3007, 0.2314, -0.8579, -0.4283, -0.2119, -0.1213, 0]], "bottom_logit_ablation_values": [[-3.098, -6.819, -4.35, -0.7243, -6.217, -5.183, -4.311, -0.7322, 0], [-3.028, -6.462, -4.277, -0.7169, -6.18, -5.126, -4.184, -0.7171, 0], [-3.022, -6.445, -4.258, -0.6877, -6.089, -4.846, -4.113, -0.7036, 0], [-3.004, -6.357, -4.234, -0.6874, -6.075, -4.814, -4.093, -0.6945, 0], [-2.989, -6.325, -4.201, -0.6843, -6.074, -4.814, -3.949, -0.6921, 0]], "residual_loss_ablation": [-0.1606, 1.117, -0.661, 0.1159, 3.822, 2.716, 1.754, -0.01619, 0]}}, {"tokens_str_list": [",", " as", " opposed", " to", " being", " merely", " an", " arte", "fact"], "tokens_acts_list": [1.112, 3.226, 4.982, 4.079, 3.386, 3.61, 2.762, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 54052290, "ablations": {"loss_ablation": [0.1739, -0.09512, -1.019, -0.04132, -0.07591, 0.9046, -0.04063, -0.03998, 0], "top_logit_ablation_tokens_str": [["sed", " pursuant", "per", "isters", "ixel", " booked", "amide", "shirt", "<META>"], ["chedule", " sir", " Reserved", "============", "shirt", " fired", "ixel", "ixel", "<EOT>"], ["iator", " booked", "led", " thereto", "iper", " notified", " booked", " ambulance", "<META_START>"], ["ixel", " preferably", " soon", " to", "ameth", " knocked", " evacuated", " SUV", "<SOS>"], ["led", " respectively", " booked", " ago", " Rs", " assaulted", "ali", " actress", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "onomy", "utable", "istem", "\u03b1", " Aristotle", "selves", "ifies", "<META>"], [" philosophers", "ition", " metaph", "otic", "esis", " philosophical", " philosophers", " certain", "<EOT>"], [" Marx", " ontology", "ificance", " morality", " philosophical", "ness", " Aristotle", " us", "<META_START>"], [" contradictory", " theories", "onom", " rational", "\u03c0", " metaph", "eness", " Aristotle", "<SOS>"], [" philosophical", ":", " truths", " Philosophy", "ism", " philosophers", " Descartes", " philosophers", "<META_END>"]], "top_logit_ablation_values": [[0.2861, -0.1497, 0.6857, 0.3425, 0.1143, -0.17, 0.1683, -0.1177, 0], [0.1634, -0.1693, 0.3772, 0.3348, -0.06042, -0.3724, 0.03883, -0.285, 0], [0.0989, -0.1734, 0.3562, 0.1814, -0.1264, -0.409, -0.0803, -0.4839, 0], [0.08803, -0.1899, 0.3484, 0.04497, -0.5583, -0.4593, -0.275, -0.5407, 0], [0.05215, -0.1903, 0.3428, 0.01771, -0.565, -0.4601, -0.291, -0.5841, 0]], "bottom_logit_ablation_values": [[-2.543, -1.406, -4.075, -6.674, -6.574, -5.157, -5.685, -4.424, 0], [-2.316, -1.402, -3.885, -6.452, -6.526, -5.104, -5.494, -4.345, 0], [-2.247, -1.396, -3.852, -6.367, -6.387, -5.038, -5.41, -4.312, 0], [-2.24, -1.365, -3.843, -6.31, -6.343, -4.99, -5.343, -4.297, 0], [-2.16, -1.358, -3.795, -6.281, -6.263, -4.987, -5.337, -4.237, 0]], "residual_loss_ablation": [-0.791, -0.05398, 1.056, 0.7485, 2.526, 1.206, 0.6959, 0.04469, 0]}}]}, {"quantile_name": "Subsample Interval 2", "quantile_max_act": 3.399, "examples": [{"tokens_str_list": [" in", " any", " terms", " other", " than", " themselves", ".", " (", "Other"], "tokens_acts_list": [3.567, 2.869, 0.6568, 3.646, 3.399, 2.696, 0.4351, 1.715, 2.717], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 40825363, "ablations": {"loss_ablation": [-0.1013, -0.1497, 0.7157, 0.2108, -0.4991, 0.395, -0.3807, 0.1123, 0], "top_logit_ablation_tokens_str": [[" ashore", "shirt", " County", "leneck", " than", " TX", " booked", "shirt", "<META>"], [" Wednesday", "osit", " weekend", "borough", " (@", "chedule", "peed", " County", "<EOT>"], [" Monday", "munk", " mall", " TX", "lights", "ahoe", " parked", "hire", "<META_START>"], [" pursuant", " County", "shirt", "boro", "ials", " booked", " shipped", " TX", "<SOS>"], [" Thursday", " TX", " TX", "peed", "spots", "days", " (@", "chedule", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "ism", " Descartes", " philosophical", " Marx", " Philosophy", " ontology", " philosophical", "<META>"], [" philosophers", "isms", " oneself", " Philosophy", " Descartes", " philosophical", " philosophical", " metaph", "<EOT>"], [" Philosophy", "ifies", "ism", " metaph", "gel", "ness", " metaph", " ontology", "<META_START>"], ["icism", " philosophical", " Aristotle", " philosophers", " Philosophy", "\u03c1", " Philosophy", " empirical", "<SOS>"], [" ontology", " philosophers", "licity", " Descartes", " oneself", " interpretation", " mathematics", " truths", "<META_END>"]], "top_logit_ablation_values": [[-0.08118, -0.6803, -0.0773, 0.2407, -0.4366, 0.585, 0.01737, -0.07149, 0], [-0.1394, -0.8635, -0.2183, 0.2387, -0.6437, -0.1329, -0.1347, -0.09073, 0], [-0.1407, -0.9923, -0.2837, 0.2362, -0.803, -0.3595, -0.156, -0.104, 0], [-0.1825, -1.009, -0.2978, 0.2317, -0.8068, -0.4066, -0.1967, -0.1147, 0], [-0.1867, -1.024, -0.3007, 0.2314, -0.8579, -0.4283, -0.2119, -0.1213, 0]], "bottom_logit_ablation_values": [[-3.098, -6.819, -4.35, -0.7243, -6.217, -5.183, -4.311, -0.7322, 0], [-3.028, -6.462, -4.277, -0.7169, -6.18, -5.126, -4.184, -0.7171, 0], [-3.022, -6.445, -4.258, -0.6877, -6.089, -4.846, -4.113, -0.7036, 0], [-3.004, -6.357, -4.234, -0.6874, -6.075, -4.814, -4.093, -0.6945, 0], [-2.989, -6.325, -4.201, -0.6843, -6.074, -4.814, -3.949, -0.6921, 0]], "residual_loss_ablation": [-0.1606, 1.117, -0.661, 0.1159, 3.822, 2.716, 1.754, -0.01619, 0]}}, {"tokens_str_list": [",", " as", " opposed", " to", " being", " merely", " an", " arte", "fact"], "tokens_acts_list": [1.112, 3.226, 4.982, 4.079, 3.386, 3.61, 2.762, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 1, "datapoint_ind": 54052290, "ablations": {"loss_ablation": [0.1739, -0.09512, -1.019, -0.04132, -0.07591, 0.9046, -0.04063, -0.03998, 0], "top_logit_ablation_tokens_str": [["sed", " pursuant", "per", "isters", "ixel", " booked", "amide", "shirt", "<META>"], ["chedule", " sir", " Reserved", "============", "shirt", " fired", "ixel", "ixel", "<EOT>"], ["iator", " booked", "led", " thereto", "iper", " notified", " booked", " ambulance", "<META_START>"], ["ixel", " preferably", " soon", " to", "ameth", " knocked", " evacuated", " SUV", "<SOS>"], ["led", " respectively", " booked", " ago", " Rs", " assaulted", "ali", " actress", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "onomy", "utable", "istem", "\u03b1", " Aristotle", "selves", "ifies", "<META>"], [" philosophers", "ition", " metaph", "otic", "esis", " philosophical", " philosophers", " certain", "<EOT>"], [" Marx", " ontology", "ificance", " morality", " philosophical", "ness", " Aristotle", " us", "<META_START>"], [" contradictory", " theories", "onom", " rational", "\u03c0", " metaph", "eness", " Aristotle", "<SOS>"], [" philosophical", ":", " truths", " Philosophy", "ism", " philosophers", " Descartes", " philosophers", "<META_END>"]], "top_logit_ablation_values": [[0.2861, -0.1497, 0.6857, 0.3425, 0.1143, -0.17, 0.1683, -0.1177, 0], [0.1634, -0.1693, 0.3772, 0.3348, -0.06042, -0.3724, 0.03883, -0.285, 0], [0.0989, -0.1734, 0.3562, 0.1814, -0.1264, -0.409, -0.0803, -0.4839, 0], [0.08803, -0.1899, 0.3484, 0.04497, -0.5583, -0.4593, -0.275, -0.5407, 0], [0.05215, -0.1903, 0.3428, 0.01771, -0.565, -0.4601, -0.291, -0.5841, 0]], "bottom_logit_ablation_values": [[-2.543, -1.406, -4.075, -6.674, -6.574, -5.157, -5.685, -4.424, 0], [-2.316, -1.402, -3.885, -6.452, -6.526, -5.104, -5.494, -4.345, 0], [-2.247, -1.396, -3.852, -6.367, -6.387, -5.038, -5.41, -4.312, 0], [-2.24, -1.365, -3.843, -6.31, -6.343, -4.99, -5.343, -4.297, 0], [-2.16, -1.358, -3.795, -6.281, -6.263, -4.987, -5.337, -4.237, 0]], "residual_loss_ablation": [-0.791, -0.05398, 1.056, 0.7485, 2.526, 1.206, 0.6959, 0.04469, 0]}}, {"tokens_str_list": [" of", " knowledge", " and", " belief", " for", " multi", "-", "agent", " systems"], "tokens_acts_list": [2.303, 0, 1.201, 0, 3.333, 0, 0.6651, 0, 0.1089], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 44339932, "ablations": {"loss_ablation": [0, 0.4037, 0, 0.2478, 0, -0.3539, 0, -0.3131, 0], "top_logit_ablation_tokens_str": [["<META>", "shirt", "<META>", "]{}]{}", "<META>", "shirt", "<META>", "shirt", "<META>"], ["<EOT>", "sville", "<EOT>", "nol", "<EOT>", "ette", "<EOT>", "hire", "<EOT>"], ["<META_START>", " Broncos", "<META_START>", "\"}.", "<META_START>", " TX", "<META_START>", "week", "<META_START>"], ["<SOS>", "ette", "<SOS>", "rangle", "<SOS>", "nb", "<SOS>", "month", "<SOS>"], ["<META_END>", "ftime", "<META_END>", " facility", "<META_END>", " County", "<META_END>", "oxide", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " metaph", "<META>", " Marx", "<META>", "\u03c1", "<META>", " metaph", "<META>"], ["<EOT>", " philosophers", "<EOT>", "sided", "<EOT>", " Freud", "<EOT>", " Aristotle", "<EOT>"], ["<META_START>", " Marx", "<META_START>", "\u03c0", "<META_START>", " metaph", "<META_START>", " philosophical", "<META_START>"], ["<SOS>", " philosophical", "<SOS>", "aments", "<SOS>", " Marx", "<SOS>", " truths", "<SOS>"], ["<META_END>", " Aristotle", "<META_END>", "utable", "<META_END>", " philosophical", "<META_END>", " Descartes", "<META_END>"]], "top_logit_ablation_values": [[0, -0.4843, 0, -0.0814, 0, -0.2612, 0, 0.1399, 0], [0, -0.4934, 0, -0.1622, 0, -0.3943, 0, 0.1057, 0], [0, -0.5252, 0, -0.1724, 0, -0.4574, 0, 0.09083, 0], [0, -0.5656, 0, -0.1866, 0, -0.6359, 0, 0.08736, 0], [0, -0.5949, 0, -0.1909, 0, -0.6596, 0, 0.06072, 0]], "bottom_logit_ablation_values": [[0, -3.747, 0, -1.666, 0, -5.531, 0, -1.065, 0], [0, -3.673, 0, -1.648, 0, -5.454, 0, -1.054, 0], [0, -3.649, 0, -1.634, 0, -5.428, 0, -1.042, 0], [0, -3.598, 0, -1.608, 0, -5.412, 0, -1.017, 0], [0, -3.596, 0, -1.589, 0, -5.363, 0, -1.015, 0]], "residual_loss_ablation": [-0.4922, 1.759, -0.3266, 0.3714, -1.744, 1.161, 0.6926, 1.738, 0]}}, {"tokens_str_list": [" difference", " between", " true", " and", " false", " is", " central", " to", " the"], "tokens_acts_list": [0, 3.866, 3.673, 1.212, 3.218, 1.778, 2.065, 3.13, 0.9772], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 37754134, "ablations": {"loss_ablation": [-0.03858, 0, 1.34, -0.6554, -0.03387, 0.4263, 0.1825, -0.1815, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "shirt", "shirt", " lowest", "out", "shirt", "izer", "<META>"], [" Diocese", "<EOT>", "shirts", "shirts", "naments", "shirt", " (@", "ised", "<EOT>"], ["chedule", "<META_START>", "sville", "ety", " Falcons", "outs", "oprop", "led", "<META_START>"], [" District", "<SOS>", " FC", "roup", "avirus", "roup", " booked", "chedule", "<SOS>"], [" Falcons", "<META_END>", "lace", "esville", "shirt", "ette", "lower", "bie", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Aristotle", "<META>", " philosophical", " Aristotle", "eced", " what", " philosophers", " Aristotle", "<META>"], [" mankind", "<EOT>", " scientific", " metaph", " Freud", " how", " thinkers", " contradictory", "<EOT>"], [" truths", "<META_START>", " Aristotle", " Freud", " philosophers", " metaph", " truths", " philosophical", "<META_START>"], [" metaph", "<SOS>", " contradictory", " consciously", "ison", " why", " theories", " metaph", "<SOS>"], [" anything", "<META_END>", " scientifically", " philosophical", ".\"", " Aristotle", " philosophical", "icism", "<META_END>"]], "top_logit_ablation_values": [[-0.3296, 0, -0.1894, -0.4583, -0.9413, -0.2749, 0.04333, -0.7579, 0], [-0.441, 0, -0.957, -0.8445, -0.9488, -0.2804, 0.02689, -0.8186, 0], [-0.4781, 0, -1.114, -1.054, -0.9525, -0.4284, 0.02595, -0.839, 0], [-0.4797, 0, -1.247, -1.134, -1.014, -0.7089, -0.07268, -0.8408, 0], [-0.4822, 0, -1.416, -1.164, -1.016, -0.7312, -0.1111, -0.8814, 0]], "bottom_logit_ablation_values": [[-1.766, 0, -6.683, -6.34, -2.742, -5.761, -2.884, -3.551, 0], [-1.76, 0, -6.665, -6.331, -2.74, -5.61, -2.64, -3.387, 0], [-1.737, 0, -6.655, -6.302, -2.714, -5.406, -2.618, -3.253, 0], [-1.711, 0, -6.581, -6.285, -2.701, -5.39, -2.607, -3.253, 0], [-1.706, 0, -6.568, -6.269, -2.697, -5.389, -2.593, -3.232, 0]], "residual_loss_ablation": [0.4527, 1.688, -0.3378, 1.014, 3.304, 0.8618, -0.07545, 2.546, 0]}}, {"tokens_str_list": ["Ar", "ist", "otle", " talks", " in", " a", " curs", "ory", " manner"], "tokens_acts_list": [0, 0, 0, 0, 3.21, 2.422, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 60425799, "ablations": {"loss_ablation": [0.06048, 0, 0, 0, 0, 0.3136, 0.02326, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "<META>", "<META>", "<META>", "shirt", "irt", "<META>", "<META>"], ["out", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " County", "shirt", "<EOT>", "<EOT>"], ["AP", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " TX", " County", "<META_START>", "<META_START>"], ["month", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " Orange", " mall", "<SOS>", "<SOS>"], ["arrow", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "shots", " cafe", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", "<META>", "<META>", "<META>", "<META>", " hypotheses", " ourselves", "<META>", "<META>"], [" philosophers", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " assumptions", " inferences", "<EOT>", "<EOT>"], [" metaph", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " empirical", " hypotheses", "<META_START>", "<META_START>"], [" ontology", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " inferences", " philosophers", "<SOS>", "<SOS>"], [" truths", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "asures", " assumptions", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.2228, 0, 0, 0, 0, -0.1251, 0.2412, 0, 0], [0.06888, 0, 0, 0, 0, -0.2064, 0.1505, 0, 0], [0.001603, 0, 0, 0, 0, -0.3512, 0.05102, 0, 0], [-0.0303, 0, 0, 0, 0, -0.4022, 0.024, 0, 0], [-0.05384, 0, 0, 0, 0, -0.4473, -0.03341, 0, 0]], "bottom_logit_ablation_values": [[-2.217, 0, 0, 0, 0, -4.418, -3.387, 0, 0], [-2.209, 0, 0, 0, 0, -4.408, -3.308, 0, 0], [-2.197, 0, 0, 0, 0, -4.323, -3.252, 0, 0], [-2.177, 0, 0, 0, 0, -4.25, -3.229, 0, 0], [-2.153, 0, 0, 0, 0, -4.243, -3.214, 0, 0]], "residual_loss_ablation": [-0.2648, 2.897, 4.254, 1.27, -0.1281, -0.3573, 0.5339, -0.04499, 0]}}]}, {"quantile_name": "Subsample Interval 3", "quantile_max_act": 3.002, "examples": [{"tokens_str_list": [" impressions", " by", " as", "sent", "ing", " to", " their", " corresponding", " pro"], "tokens_acts_list": [1.203, 3.823, 3.512, 1.378, 3.002, 2.788, 1.301, 0.7249, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 24332817, "ablations": {"loss_ablation": [0.306, 0.1734, 0.3718, -1.262, -0.5798, -0.1998, -0.04705, -0.1313, 0], "top_logit_ablation_tokens_str": [["oted", "chedule", "shirt", "oxide", "ials", "chedule", "chedule", " inbox", "<META>"], ["chedule", "peed", "hire", "per", "rals", "shirt", "shirt", "letters", "<EOT>"], ["isters", "weet", "\"}.", "chedule", "ed", "peed", " inbox", "shirt", "<META_START>"], ["ize", "ulf", " County", "par", "robe", " Services", " County", "hire", "<SOS>"], ["ier", "burg", " photographer", "pons", "shots", "hire", "letters", "osit", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" trillion", "ernate", "\u03b3", "utable", " hypothesis", " philosophical", " philosophical", " Marx", "<META>"], [" Aristotle", " philosophical", " philosophical", "inciple", " Marx", " empirical", "ential", " Aristotle", "<EOT>"], [" contradictory", " reasoning", "isms", " ontology", " ontology", " ontology", " empirical", " why", "<META_START>"], [" empirical", " ontology", "ibility", " interpretation", " relativity", " evolutionary", " reasoning", "ational", "<SOS>"], ["utable", " empirical", "\u03b2", "arrative", " evolutionary", " conceptual", "ateral", "utable", "<META_END>"]], "top_logit_ablation_values": [[1.049, 0.144, -0.3812, 1.218, 0.853, 1.026, 0.4197, 0.3766, 0], [1.009, 0.1435, -0.6601, 1.075, 0.7717, 0.8585, 0.4184, 0.2885, 0], [0.9943, 0.1216, -0.6668, 1.064, 0.7166, 0.7035, -0.01467, 0.2774, 0], [0.9018, 0.1178, -0.7551, 0.9771, 0.7139, 0.5704, -0.04207, 0.2509, 0], [0.8779, 0.08147, -0.7882, 0.9454, 0.6742, 0.5682, -0.04501, 0.2482, 0]], "bottom_logit_ablation_values": [[-2.165, -1.237, -6.084, -4.04, -1.035, -2.693, -3.582, -1.392, 0], [-2.131, -1.22, -5.588, -3.63, -1.026, -2.635, -3.567, -1.286, 0], [-2.049, -1.19, -5.506, -3.565, -1.022, -2.598, -3.428, -1.27, 0], [-2.041, -1.187, -5.474, -3.54, -1.02, -2.59, -3.41, -1.266, 0], [-2.015, -1.171, -5.405, -3.506, -1.009, -2.557, -3.407, -1.228, 0]], "residual_loss_ablation": [3.278, -0.3785, 0.8752, 2.147, 5.329, 1.144, -0.1431, 0.9179, 0]}}, {"tokens_str_list": [" concepts", " are", " today", " sustained", " by", " theology", " alone", ",", " as"], "tokens_acts_list": [1.232, 2.669, 0, 1.072, 2.983, 0, 1.578, 0.4177, 2.234], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 26406983, "ablations": {"loss_ablation": [0.2569, -0.2024, 0.1464, 0, -0.0953, 0.3273, 0, 0.09279, 0], "top_logit_ablation_tokens_str": [["shops", "days", " booked", "<META>", " Makeup", " Hurricane", "<META>", " Makeup", "<META>"], ["days", "hire", " shipped", "<EOT>", "outs", " firefighters", "<EOT>", " weekend", "<EOT>"], ["shots", " shipped", " purchased", "<META_START>", " weekend", " helicopter", "<META_START>", " ago", "<META_START>"], ["__\":", "burg", " underway", "<SOS>", " Friday", "shirt", "<SOS>", "days", "<SOS>"], [" Services", "park", " lined", "<META_END>", " Monday", "days", "<META_END>", "!!!", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" oneself", " philosophical", " hypothesis", "<META>", "mathcal", "ibility", "<META>", "\u03b3", "<META>"], [" whether", " Descartes", "ity", "<EOT>", "isp", "\u03b3", "<EOT>", "\u03ba", "<EOT>"], [" why", " ontology", "itation", "<META_START>", " ontology", "ity", "<META_START>", "\u03c0", "<META_START>"], [" itself", " Philosophy", "ness", "<SOS>", "istem", " predicate", "<SOS>", " ontology", "<SOS>"], ["utable", " negation", " existence", "<META_END>", " intuition", "istem", "<META_END>", " causal", "<META_END>"]], "top_logit_ablation_values": [[0.2138, 0.1253, 0.9596, 0, 0.487, 0.05525, 0, 0.4941, 0], [0.19, 0.08724, 0.9418, 0, 0.4186, -0.02099, 0, 0.4905, 0], [0.1584, 0.04325, 0.917, 0, 0.4167, -0.03887, 0, 0.4715, 0], [0.1372, 0.01911, 0.874, 0, 0.4114, -0.06687, 0, 0.4684, 0], [0.1022, 0.01013, 0.8144, 0, 0.4063, -0.124, 0, 0.4615, 0]], "bottom_logit_ablation_values": [[-4.367, -1.57, -3.439, 0, -0.9553, -3.853, 0, -1.427, 0], [-3.85, -1.504, -3.233, 0, -0.8779, -3.667, 0, -1.406, 0], [-3.845, -1.5, -3.106, 0, -0.8759, -3.587, 0, -1.402, 0], [-3.733, -1.494, -3.045, 0, -0.8661, -3.576, 0, -1.3, 0], [-3.729, -1.49, -3.012, 0, -0.8529, -3.561, 0, -1.299, 0]], "residual_loss_ablation": [2.714, 1.338, -0.6707, 1.765, 0.8311, 1.969, 1.126, -0.5322, 0]}}, {"tokens_str_list": [" they", " are", " not", " plausible", " for", " determining", " whether", " or", " not"], "tokens_acts_list": [0.8247, 2.217, 0.9769, 1.9, 2.926, 2.403, 2.644, 0, 1.928], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 15461723, "ablations": {"loss_ablation": [-0.07781, -0.08266, -0.1717, 0.4021, -0.3736, -0.1326, -0.3516, -0.4064, 0], "top_logit_ablation_tokens_str": [[" sir", "hire", " booked", " booked", "<EOT>", "alls", "((-", " County", "<META>"], ["shirts", " booked", " purchased", " notified", "chedule", "shirts", "shirt", "abet", "<EOT>"], [" fees", "burg", " rinsed", " evacuated", " (@", "ouchers", " fees", "out", "<META_START>"], [" pursuant", "shirt", " shipped", " eligible", "ed", "grant", "chedule", "MEM", "<SOS>"], ["reatment", "United", " scheduled", "xac", "cheon", "gery", "$.", "abouts", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" hypothesis", " ontology", " hypothesis", " hypothesis", " MERCHANTABILITY", " relates", " ontology", "trical", "<META>"], ["utation", " philosophical", "ularity", " Descartes", " Descartes", "\\xe1", " philosophical", "notations", "<EOT>"], ["ness", " Descartes", " ontology", "mitives", "\u03c1", " consciousness", " Aristotle", " conjecture", "<META_START>"], ["mathcal", " Aristotle", " consciousness", " ontology", "mathcal", " follows", "\u03c1", "istem", "<SOS>"], ["gence", " Freud", "ness", " Aristotle", "\\xe1", "ularity", " metaph", " hypothesis", "<META_END>"]], "top_logit_ablation_values": [[0.1977, -0.1384, 0.7048, 0.5183, 0.746, 1.297, 0.4419, 0.4977, 0], [0.1618, -0.1398, 0.4084, 0.3764, 0.7256, 1.295, 0.3267, 0.4787, 0], [0.1561, -0.1545, 0.407, 0.374, 0.7152, 1.274, 0.2966, 0.4435, 0], [0.1502, -0.1675, 0.3575, 0.3374, 0.6771, 1.241, 0.1851, 0.3908, 0], [0.1437, -0.1768, 0.3387, 0.3359, 0.6416, 1.205, 0.1252, 0.3525, 0]], "bottom_logit_ablation_values": [[-0.4201, -1.344, -3.079, -1.186, -2.081, -2.758, -3.354, -3.36, 0], [-0.4119, -1.304, -2.859, -1.146, -2.027, -2.692, -3.167, -3.164, 0], [-0.4097, -1.298, -2.783, -1.133, -2.003, -2.612, -3.109, -3.107, 0], [-0.4089, -1.292, -2.771, -1.107, -1.981, -2.587, -3.099, -3.053, 0], [-0.4057, -1.288, -2.763, -1.095, -1.978, -2.53, -3.073, -3.042, 0]], "residual_loss_ablation": [1.608, -0.605, -0.4846, 2.431, -0.3806, 0.4073, 2.636, 1.148, 0]}}, {"tokens_str_list": ["the", " first", " is", " grounded", " upon", " a", " definition", " of", " _"], "tokens_acts_list": [0, 0.3679, 1.223, 1.59, 2.817, 1.35, 0, 2.526, 1.239], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 59242587, "ablations": {"loss_ablation": [0.1984, 0, 0.04555, 0.193, -0.03328, 0.02053, 0.3871, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "shirt", " booked", "out", "noon", "irt", "<META>", "<META>"], ["out", "<EOT>", "week", "out", " ago", "sgiving", "chedule", "<EOT>", "<EOT>"], ["AP", "<META_START>", "weekly", "weekly", "hire", " festival", "ctomy", "<META_START>", "<META_START>"], ["month", "<SOS>", " afternoon", " (@", "chedule", "reated", "AP", "<SOS>", "<SOS>"], ["burg", "<META_END>", " month", "shirt", "erry", " trout", "shirt", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", "<META>", " inferences", " inferences", " assumptions", " ontology", " ourselves", "<META>", "<META>"], [" philosophers", "<EOT>", "utable", " hypotheses", " inferences", " hypotheses", " oneself", "<EOT>", "<EOT>"], [" metaph", "<META_START>", "tru", " theories", " relativity", " causal", " beings", "<META_START>", "<META_START>"], [" ontology", "<SOS>", " intuit", " assumptions", " hypotheses", " contexts", "ions", "<SOS>", "<SOS>"], [" Aristotle", "<META_END>", " ourselves", " hypothesis", " empirical", "mmas", " truths", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.1414, 0, 0.09377, 0.6199, 0.9268, 0.7385, 0.4011, 0, 0], [0.06951, 0, 0.09023, 0.5587, 0.9141, 0.5878, 0.3038, 0, 0], [0.01072, 0, 0.06916, 0.5472, 0.8634, 0.5811, 0.2744, 0, 0], [-0.006589, 0, 0.06874, 0.5437, 0.8209, 0.5337, 0.2657, 0, 0], [-0.01621, 0, 0.0661, 0.5189, 0.7939, 0.4671, 0.2545, 0, 0]], "bottom_logit_ablation_values": [[-1.299, 0, -0.4006, -1.223, -1.147, -2.876, -1.565, 0, 0], [-1.297, 0, -0.3971, -1.129, -1.067, -2.788, -1.435, 0, 0], [-1.28, 0, -0.3896, -1.11, -1.038, -2.763, -1.366, 0, 0], [-1.27, 0, -0.3731, -1.092, -1.012, -2.725, -1.351, 0, 0], [-1.27, 0, -0.363, -1.078, -0.9955, -2.719, -1.348, 0, 0]], "residual_loss_ablation": [-0.3549, -1.252, 0.1678, 0.08282, 0.4878, 0.2563, 1.533, -0.07042, 0]}}, {"tokens_str_list": ["knowledge", " arise", " and", " count", " as", " knowledge", "?", " M", "uker"], "tokens_acts_list": [1.772, 2.299, 1.155, 0, 2.733, 0.6338, 1.517, 0, 1.323], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 1814034, "ablations": {"loss_ablation": [0, 0.1997, 0.1268, -0.04038, 0, 1.084, 0.1307, 0.129, 0], "top_logit_ablation_tokens_str": [["<META>", "shops", "burg", " unload", "<META>", " soon", "teen", "shirt", "<META>"], ["<EOT>", "stown", "shots", " repair", "<EOT>", "week", "letters", "<EOT>", "<EOT>"], ["<META_START>", "teen", "shops", " ride", "<META_START>", "oxide", "boards", " County", "<META_START>"], ["<SOS>", "led", " ashore", " rebuild", "<SOS>", "reek", "ously", " Services", "<SOS>"], ["<META_END>", "park", "hire", " install", "<META_END>", "par", "shops", "out", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "utable", " empirical", "mentioned", "<META>", "utable", " existence", " metaph", "<META>"], ["<EOT>", " philosophical", " negation", "\u03c6", "<EOT>", "atory", " philosophical", " philosophical", "<EOT>"], ["<META_START>", " negation", " justification", "mpty", "<META_START>", "atable", " moral", " rational", "<META_START>"], ["<SOS>", " existence", " philosophical", ".:", "<SOS>", "ativity", "ularity", " empirical", "<SOS>"], ["<META_END>", " empirical", " morality", " words", "<META_END>", "romic", " empirical", " mathematical", "<META_END>"]], "top_logit_ablation_values": [[0, 0.7738, 0.7277, 0.2821, 0, 0.3685, 0.0287, -0.1628, 0], [0, 0.6757, 0.6983, 0.1972, 0, 0.1908, 0.01599, -0.2634, 0], [0, 0.671, 0.5905, 0.1963, 0, 0.1706, 0.01229, -0.319, 0], [0, 0.6674, 0.5361, 0.1908, 0, 0.07195, 0.01022, -0.3288, 0], [0, 0.6612, 0.5321, 0.1848, 0, 0.05527, -0.003849, -0.3448, 0]], "bottom_logit_ablation_values": [[0, -1.468, -2.522, -1.054, 0, -3.343, -0.8161, -2.092, 0], [0, -1.455, -2.509, -1.052, 0, -3.243, -0.7875, -2.083, 0], [0, -1.452, -2.489, -1.043, 0, -3.22, -0.7853, -2.047, 0], [0, -1.447, -2.458, -1.025, 0, -3.202, -0.7794, -2.045, 0], [0, -1.445, -2.444, -1.021, 0, -3.167, -0.7711, -2.036, 0]], "residual_loss_ablation": [1.512, -0.2209, 0.487, 1.031, 0.5679, -0.1388, 0.9284, -0.4031, 0]}}]}, {"quantile_name": "Subsample Interval 4", "quantile_max_act": 2.547, "examples": [{"tokens_str_list": [" (", "1", ")", " as", " a", " premise", " can", " imp", "inge"], "tokens_acts_list": [0.7316, 0.4616, 2.346, 2.552, 2.547, 0.05634, 1.877, 0, 0.8548], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 90760153, "ablations": {"loss_ablation": [0.01879, -0.1907, -0.0827, 0.03116, 0.08027, 0.5617, 0.001505, 0.6033, 0], "top_logit_ablation_tokens_str": [["shirt", "shirt", " TX", "shirt", "shirt", " bartender", "shirt", "shirt", "<META>"], ["away", "out", "chedule", "oprop", " TX", "chedule", "amide", "ette", "<EOT>"], ["chedule", "AP", "week", "month", "oxide", "shirt", "raiser", "burg", "<META_START>"], [" fees", " County", "month", "chedule", " booked", " weekend", "chedule", "hire", "<SOS>"], ["out", "month", "shirt", "ette", "amide", "AP", " hike", "isters", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["uality", " philosophical", " Descartes", " ontology", "utable", " oneself", " meanings", " philosophical", "<META>"], ["itudes", " philosophers", " philosophers", " contexts", " ontology", " ourselves", " truths", " ontology", "<EOT>"], ["inciple", " metaph", " notions", " phenomena", "uality", "ions", " notions", " beliefs", "<META_START>"], ["itude", " Aristotle", " linguistic", " philosophers", " existence", " themselves", " contexts", " meanings", "<SOS>"], [" ontology", " ontology", " philosophical", " notions", "asures", " beings", " beings", " Descartes", "<META_END>"]], "top_logit_ablation_values": [[-0.3758, 0.03265, -0.1546, 0.2867, -0.1667, -0.4231, 0.007661, -0.1541, 0], [-0.4436, -0.03543, -0.1678, 0.2289, -0.2484, -0.4586, 0.007424, -0.1815, 0], [-0.4526, -0.06928, -0.1684, 0.176, -0.2598, -0.5832, 0.006388, -0.185, 0], [-0.6032, -0.07149, -0.1715, 0.1586, -0.3762, -0.5923, 0.000648, -0.1931, 0], [-0.6529, -0.07267, -0.1757, 0.1381, -0.3853, -0.6054, -0.001408, -0.2379, 0]], "bottom_logit_ablation_values": [[-3.572, -1.03, -1.005, -3.003, -3.85, -4.057, -0.08957, -3.204, 0], [-3.493, -1.026, -0.997, -2.986, -3.805, -4.05, -0.08792, -3.175, 0], [-3.434, -1.014, -0.9916, -2.945, -3.715, -3.908, -0.08592, -3.077, 0], [-3.39, -1.011, -0.9846, -2.904, -3.705, -3.891, -0.08417, -3.067, 0], [-3.38, -1.002, -0.9814, -2.844, -3.704, -3.846, -0.08389, -3.063, 0]], "residual_loss_ablation": [2.961, 2.093, -0.3699, 0.4586, -0.2848, 2.642, 0.7087, 0.2844, 0]}}, {"tokens_str_list": [",", " Stark", " and", " Fin", "ke", " (", "2000", ")", " compared"], "tokens_acts_list": [0, 0.5006, 0, 0, 2.531, 0, 0.09321, 1.763, 0.8067], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 79914194, "ablations": {"loss_ablation": [0, 0, 0.02377, 0, 0, 0.2973, 0, -0.0106, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "burg", "<META>", "<META>", "stown", "<META>", "AP", "<META>"], ["<EOT>", "<EOT>", "ville", "<EOT>", "<EOT>", "burg", "<EOT>", " TX", "<EOT>"], ["<META_START>", "<META_START>", " County", "<META_START>", "<META_START>", " County", "<META_START>", "burg", "<META_START>"], ["<SOS>", "<SOS>", "stown", "<SOS>", "<SOS>", "cks", "<SOS>", "km", "<SOS>"], ["<META_END>", "<META_END>", "sville", "<META_END>", "<META_END>", "ctomy", "<META_END>", " County", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", " Aristotle", "<META>", "<META>", " logical", "<META>", " truths", "<META>"], ["<EOT>", "<EOT>", " logical", "<EOT>", "<EOT>", " metaph", "<EOT>", " ontology", "<EOT>"], ["<META_START>", "<META_START>", " ontology", "<META_START>", "<META_START>", " causal", "<META_START>", " intuition", "<META_START>"], ["<SOS>", "<SOS>", " philosophical", "<SOS>", "<SOS>", " philosophical", "<SOS>", " metaph", "<SOS>"], ["<META_END>", "<META_END>", " metaph", "<META_END>", "<META_END>", " existence", "<META_END>", " oneself", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0.07184, 0, 0, 0.4733, 0, -0.01204, 0], [0, 0, 0.07006, 0, 0, 0.3935, 0, -0.01588, 0], [0, 0, 0.06254, 0, 0, 0.3688, 0, -0.02115, 0], [0, 0, 0.05658, 0, 0, 0.2358, 0, -0.02273, 0], [0, 0, 0.04953, 0, 0, 0.214, 0, -0.02587, 0]], "bottom_logit_ablation_values": [[0, 0, -0.7213, 0, 0, -3.044, 0, -0.1976, 0], [0, 0, -0.7165, 0, 0, -3.005, 0, -0.1962, 0], [0, 0, -0.712, 0, 0, -2.999, 0, -0.1939, 0], [0, 0, -0.7067, 0, 0, -2.993, 0, -0.1918, 0], [0, 0, -0.7002, 0, 0, -2.978, 0, -0.1907, 0]], "residual_loss_ablation": [0.2861, -0.0147, -2.165, -0.05693, 0.2218, -0.7459, 0.7162, 0.05827, 0]}}, {"tokens_str_list": [" simple", " things", ",", " so", " a", " typical", " observer", " could", " be"], "tokens_acts_list": [2.495, 0, 0.5975, 0.7773, 2.486, 0, 0.704, 2.003, 1.821], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 93285977, "ablations": {"loss_ablation": [0.1694, 1.43, 0, -0.04671, 0.1519, -0.1832, 0, -0.08073, 0], "top_logit_ablation_tokens_str": [[" TX", "sed", "<META>", " Makeup", " TX", "chedule", "<META>", "ixel", "<META>"], [" booked", "z", "<EOT>", " huh", "shirt", "ctomy", "<EOT>", "hire", "<EOT>"], ["hire", "cm", "<META_START>", " sir", " County", " TX", "<META_START>", " booked", "<META_START>"], ["semble", "bie", "<SOS>", "}}$", " Makeup", " buddy", "<SOS>", "}<", "<SOS>"], [" Hurricane", "day", "<META_END>", "resistant", "sburg", "2", "<META_END>", "led", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["utable", "icism", "<META>", " ontology", "utation", " oneself", "<META>", "\u03b3", "<META>"], [" meanings", " oneself", "<EOT>", "atural", " meanings", " ourselves", "<EOT>", " Marx", "<EOT>"], ["gment", "arrative", "<META_START>", " oneself", " discourse", " meanings", "<META_START>", " metaph", "<META_START>"], [" interpretation", "bids", "<SOS>", "utation", " interpretations", " beliefs", "<SOS>", "phrine", "<SOS>"], ["ential", " sake", "<META_END>", " hypothesis", " metaph", " themselves", "<META_END>", " truths", "<META_END>"]], "top_logit_ablation_values": [[-0.2636, 0.679, 0, 0.123, 0.0235, -0.4442, 0, 0.04245, 0], [-0.4999, 0.4384, 0, 0.1204, -0.004376, -0.4948, 0, 0.001225, 0], [-0.5246, 0.2357, 0, 0.1063, -0.02168, -0.5112, 0, -0.01791, 0], [-0.5289, 0.2169, 0, 0.1003, -0.0279, -0.5634, 0, -0.03095, 0], [-0.5529, 0.2108, 0, 0.09055, -0.02927, -0.6267, 0, -0.03102, 0]], "bottom_logit_ablation_values": [[-3.552, -2.83, 0, -0.4999, -1.151, -4.489, 0, -0.8717, 0], [-3.546, -2.822, 0, -0.4993, -1.14, -4.365, 0, -0.8634, 0], [-3.531, -2.804, 0, -0.491, -1.129, -4.287, 0, -0.8513, 0], [-3.521, -2.802, 0, -0.482, -1.124, -4.2, 0, -0.8482, 0], [-3.496, -2.798, 0, -0.4754, -1.122, -4.199, 0, -0.8443, 0]], "residual_loss_ablation": [1.206, -0.01651, -1.63, 0.8687, 2.048, 0.1844, -0.3015, 3.404, 0]}}, {"tokens_str_list": ["ively", " present", ",", " as", " a", " fulfilled", " possessing", " of", " the"], "tokens_acts_list": [1.457, 0.4322, 0.7383, 2.551, 2.459, 1.457, 2.994, 1.862, 0.2093], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 96371059, "ablations": {"loss_ablation": [0, 0.1602, 0.04575, -0.0364, 0.1723, 0.3176, 0.2628, -0.2344, 0], "top_logit_ablation_tokens_str": [["<META>", "shire", "ed", " County", "per", " teenager", "day", "shirt", "<META>"], ["<EOT>", " County", "ently", "000", "week", " photographer", "month", "ixel", "<EOT>"], ["<META_START>", " parked", "============", "<EOT>", " TX", "week", "======", "fits", "<META_START>"], ["<SOS>", " evacuated", "roller", " TX", "shirt", "month", "week", "amide", "<SOS>"], ["<META_END>", " booked", " Wednesday", " booked", " soon", " midfielder", "year", "lace", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " philosophers", " interpretation", "utation", "utable", " ourselves", " inferences", " inferences", "<META>"], ["<EOT>", " philosophy", " philosophers", " hypothesis", " interpretation", "ions", " philosophers", "ification", "<EOT>"], ["<META_START>", " theories", " reasoning", "ness", " interpretations", "ations", " contexts", " interpretation", "<META_START>"], ["<SOS>", " Aristotle", " ontology", "ition", " hypothesis", "aturally", " interpretations", " inference", "<SOS>"], ["<META_END>", " Philosophy", "mmas", " oneself", "usive", " inferences", " estop", " philosophers", "<META_END>"]], "top_logit_ablation_values": [[0, 0.03914, 0.2438, 0.06391, 0.6036, 0.2439, 0.6254, 0.4939, 0], [0, -0.02108, 0.2412, 0.05661, 0.4422, 0.2024, 0.5357, 0.3456, 0], [0, -0.02418, 0.2412, 0.05303, 0.392, 0.1011, 0.494, 0.3102, 0], [0, -0.02957, 0.2217, 0.04983, 0.3785, 0.07763, 0.4503, 0.1104, 0], [0, -0.05042, 0.208, 0.04587, 0.3627, 0.06186, 0.4219, 0.1092, 0]], "bottom_logit_ablation_values": [[0, -1.994, -0.3974, -0.7978, -3.058, -3.138, -1.373, -3.792, 0], [0, -1.917, -0.3894, -0.7879, -2.963, -3.043, -1.345, -3.763, 0], [0, -1.917, -0.3821, -0.7871, -2.865, -2.923, -1.343, -3.698, 0], [0, -1.904, -0.3794, -0.7694, -2.806, -2.917, -1.282, -3.671, 0], [0, -1.894, -0.3768, -0.7498, -2.783, -2.901, -1.263, -3.616, 0]], "residual_loss_ablation": [1.69, 0.6941, 0.2103, -0.06154, -0.6057, 0.9133, 0.3249, -0.5745, 0]}}, {"tokens_str_list": [" that", " we", " do", " not", " fully", " understand", ",", " and", " have"], "tokens_acts_list": [3.344, 0.5531, 0, 1.561, 2.459, 2.609, 1.558, 0.9554, 2.282], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 61865480, "ablations": {"loss_ablation": [0.2157, -0.1756, -0.0178, 0, 0.1919, -0.05882, 0.1006, -0.1946, 0], "top_logit_ablation_tokens_str": [["sville", " Makeup", "shirt", "<META>", "chedule", "hire", "ably", " sir", "<META>"], ["shirt", "shirt", "burg", "<EOT>", " grill", "heat", "cin", " County", "<EOT>"], ["ixel", " TX", " booked", "<META_START>", " hike", " booked", "tered", " please", "<META_START>"], ["led", "amycin", "hire", "<SOS>", " attend", "chedule", "ied", "onc", "<SOS>"], ["chedule", " afternoon", "eds", "<META_END>", " booked", " inbox", "led", " buddy", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" ontology", " existence", "ential", "<META>", " meanings", " reasons", "ilinear", "ential", "<META>"], [" empirical", "ential", " ontology", "<EOT>", "\u03c0", " meanings", " explanation", "itions", "<EOT>"], [" philosophical", " causal", " oneself", "<META_START>", " oneself", " sake", " assumption", "ition", "<META_START>"], [" metaph", "itive", " Aristotle", "<SOS>", " interpretations", " oneself", " inference", "itive", "<SOS>"], [" assumptions", "licity", " metaph", "<META_END>", " interpretation", " theories", " reason", "volent", "<META_END>"]], "top_logit_ablation_values": [[0.1668, -0.5175, -0.005444, 0, 0.4839, 0.5377, 1.12, 0.2974, 0], [0.1521, -0.5312, -0.0125, 0, 0.3795, 0.4921, 1.039, -0.002085, 0], [0.1518, -0.65, -0.02323, 0, 0.3749, 0.4784, 0.9956, -0.02344, 0], [0.1512, -0.7493, -0.02514, 0, 0.3713, 0.4037, 0.9762, -0.03545, 0], [0.1335, -0.8014, -0.04197, 0, 0.358, 0.363, 0.929, -0.03596, 0]], "bottom_logit_ablation_values": [[-1.036, -5.256, -0.8759, 0, -2.025, -3.421, -2.973, -2.123, 0], [-1.024, -5.249, -0.8594, 0, -1.983, -3.29, -2.902, -1.954, 0], [-1.02, -5.211, -0.8539, 0, -1.957, -3.288, -2.826, -1.916, 0], [-1.004, -5.132, -0.8441, 0, -1.905, -3.115, -2.819, -1.896, 0], [-0.9962, -5.123, -0.8434, 0, -1.901, -3.09, -2.816, -1.856, 0]], "residual_loss_ablation": [0.1918, 0.07317, 0.3013, 1.022, 0.0752, 3.733, 0.368, 0.48, 0]}}]}, {"quantile_name": "Subsample Interval 5", "quantile_max_act": 2.127, "examples": [{"tokens_str_list": ["ation", " to", " say", " that", " for", " Aristotle", " the", " entire", " functioning"], "tokens_acts_list": [0, 0.8224, 0.2146, 1.396, 2.127, 0.255, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 49210060, "ablations": {"loss_ablation": [0, 0, -0.1613, -0.01226, -0.1427, 0.8273, 0.09915, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "days", " goodbye", " Makeup", "hire", " County", "<META>", "<META>"], ["<EOT>", "<EOT>", "day", "day", "shirt", "aging", "sville", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "hire", "out", " Diocese", " Makeup", "stown", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "shirt", " Wednesday", " TX", "shots", "burg", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", " announce", "week", " Cemetery", "day", "shirt", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "ential", " inferences", " causal", " inferences", " causal", "<META>", "<META>"], ["<EOT>", "<EOT>", " ontology", " notions", " negation", " assumptions", " inferences", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "onest", " reasoning", " inferences", " notions", " philosophical", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", " philosophical", " causal", " semantics", " reasoning", " empirical", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "asing", " assumptions", " predicates", " predicate", " logical", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0.07824, 0.1174, -0.03056, 1.145, 0.06039, 0, 0], [0, 0, 0.0495, 0.08857, -0.06879, 0.9668, 0.05388, 0, 0], [0, 0, 0.04251, 0.08507, -0.1893, 0.9386, 0.04664, 0, 0], [0, 0, 0.01605, 0.08142, -0.2182, 0.9283, 0.03844, 0, 0], [0, 0, 0.009149, 0.0797, -0.2183, 0.9176, 0.03169, 0, 0]], "bottom_logit_ablation_values": [[0, 0, -1.209, -0.2183, -2.13, -2.187, -0.2778, 0, 0], [0, 0, -1.129, -0.2145, -2.028, -2.134, -0.2685, 0, 0], [0, 0, -1.11, -0.2123, -2.02, -2.06, -0.2667, 0, 0], [0, 0, -1.095, -0.2059, -2.018, -2.052, -0.2663, 0, 0], [0, 0, -1.085, -0.2039, -2.018, -2.042, -0.2659, 0, 0]], "residual_loss_ablation": [4.859, 1.323, 2.287, 0.03458, -0.4056, 2.393, 0.09294, -1.032, 0]}}, {"tokens_str_list": [" in", " dialect", "ics", ".", " Although", " there", " are", " a", " variety"], "tokens_acts_list": [0.846, 0, 0, 0, 2.118, 0, 1.098, 1.141, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 30000565, "ablations": {"loss_ablation": [0.01014, 0.369, 0, 0, 0, -0.3749, 0, 0.1913, 0], "top_logit_ablation_tokens_str": [[" Diocese", " County", "<META>", "<META>", "<META>", " Diocese", "<META>", " Makeup", "<META>"], ["sgiving", "osit", "<EOT>", "<EOT>", "<EOT>", "shirt", "<EOT>", "sgiving", "<EOT>"], ["fits", "sgiving", "<META_START>", "<META_START>", "<META_START>", " Makeup", "<META_START>", " TX", "<META_START>"], ["noon", "cheon", "<SOS>", "<SOS>", "<SOS>", " County", "<SOS>", " County", "<SOS>"], ["clave", "shots", "<META_END>", "<META_END>", "<META_END>", " TX", "<META_END>", "alls", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" ourselves", " causal", "<META>", "<META>", "<META>", " causal", "<META>", " ontology", "<META>"], [" causal", " assumptions", "<EOT>", "<EOT>", "<EOT>", " existence", "<EOT>", " negation", "<EOT>"], [" what", " hypotheses", "<META_START>", "<META_START>", "<META_START>", " hypothesis", "<META_START>", " reasoning", "<META_START>"], [" intuition", " ontology", "<SOS>", "<SOS>", "<SOS>", " predicate", "<SOS>", " hypothesis", "<SOS>"], [" reasoning", " intuition", "<META_END>", "<META_END>", "<META_END>", " hypotheses", "<META_END>", "tru", "<META_END>"]], "top_logit_ablation_values": [[0.04517, 0.3841, 0, 0, 0, 0.2089, 0, -0.08847, 0], [0.03312, 0.3614, 0, 0, 0, -0.005371, 0, -0.1007, 0], [0.02944, 0.3592, 0, 0, 0, -0.1449, 0, -0.1456, 0], [0.02686, 0.3586, 0, 0, 0, -0.1513, 0, -0.1724, 0], [0.02664, 0.3524, 0, 0, 0, -0.1524, 0, -0.1814, 0]], "bottom_logit_ablation_values": [[-0.0922, -0.882, 0, 0, 0, -3.097, 0, -1.694, 0], [-0.08971, -0.8516, 0, 0, 0, -3.061, 0, -1.606, 0], [-0.08867, -0.8511, 0, 0, 0, -2.953, 0, -1.589, 0], [-0.08697, -0.8498, 0, 0, 0, -2.937, 0, -1.582, 0], [-0.08646, -0.8264, 0, 0, 0, -2.91, 0, -1.55, 0]], "residual_loss_ablation": [0.9357, 1.424, 6.08, 0.6409, 0.2778, 0.2749, -0.561, -0.776, 0]}}, {"tokens_str_list": [" naturally", ".", " Its", " point", " of", " entry", " is", " this", "."], "tokens_acts_list": [1.161, 0, 0.7973, 0, 2.085, 0.2637, 1.367, 1.253, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 29581084, "ablations": {"loss_ablation": [0.123, 0.08441, 0, -0.05001, 0, -0.2029, 0.01331, 0.3629, 0], "top_logit_ablation_tokens_str": [["eper", " (@", "<META>", " biggest", "<META>", "iler", "ctomy", "ched", "<META>"], ["cheon", "shirt", "<EOT>", "reek", "<EOT>", "ahoo", "0", " booked", "<EOT>"], ["lop", " booked", "<META_START>", "shirt", "<META_START>", " outage", " Monday", " filed", "<META_START>"], ["ted", "out", "<SOS>", " Diocese", "<SOS>", "iper", " Tuesday", "oprop", "<SOS>"], ["hire", "============", "<META_END>", "banded", "<META_END>", " spokeswoman", "cheon", " canceled", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" morality", " Philosophy", "<META>", " conceive", "<META>", " philosophers", " philosophical", " philosophers", "<META>"], [" moral", " philosophers", "<EOT>", " oneself", "<EOT>", "\u03b3", " philosophers", " thinkers", "<EOT>"], [" philosophical", " metaph", "<META_START>", " ourselves", "<META_START>", " Marx", " moral", "ness", "<META_START>"], [" Aristotle", " morality", "<SOS>", "hemat", "<SOS>", " Aristotle", " Aristotle", " morality", "<SOS>"], [" philosophers", " philosophical", "<META_END>", " Aristotle", "<META_END>", "minded", " oneself", " theories", "<META_END>"]], "top_logit_ablation_values": [[0.05352, 0.1435, 0, -0.2618, 0, -1.309, -0.05026, -0.1494, 0], [0.04044, 0.1382, 0, -0.2718, 0, -1.358, -0.05671, -0.1632, 0], [0.001122, 0.1313, 0, -0.2789, 0, -1.379, -0.05741, -0.1652, 0], [-5.102e-05, 0.1242, 0, -0.3399, 0, -1.396, -0.05742, -0.1983, 0], [-0.01146, 0.1089, 0, -0.3465, 0, -1.4, -0.05754, -0.2241, 0]], "bottom_logit_ablation_values": [[-1.02, -1.669, 0, -1.301, 0, -4.362, -0.3776, -2.393, 0], [-0.982, -1.652, 0, -1.294, 0, -4.348, -0.3713, -2.248, 0], [-0.9628, -1.604, 0, -1.285, 0, -4.324, -0.3678, -2.199, 0], [-0.9586, -1.574, 0, -1.276, 0, -4.287, -0.3648, -2.191, 0], [-0.9544, -1.566, 0, -1.271, 0, -4.255, -0.3632, -2.14, 0]], "residual_loss_ablation": [0.12, 2.139, 1.513, 0.06792, -0.6253, 3.081, 1.946, -0.244, 0]}}, {"tokens_str_list": [" rec", "ited", " in", " proposition", "al", " form", ",", " either", " aloud"], "tokens_acts_list": [0, 1.191, 1.996, 0, 2.072, 0.4544, 0.5541, 2.975, 2.844], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 4516234, "ablations": {"loss_ablation": [0.2091, 0, -0.03198, 0.6627, 0, 0.1754, 0.01426, 0.06968, 0], "top_logit_ablation_tokens_str": [[" booked", "<META>", " ashore", " County", "<META>", "lop", "amide", " sir", "<META>"], [" purchased", "<EOT>", "hire", "burg", "<EOT>", "amped", "ants", " booked", "<EOT>"], [" cleaned", "<META_START>", "out", "ette", "<META_START>", "ded", "cin", " parked", "<META_START>"], [" parked", "<SOS>", " booked", "cheon", "<SOS>", "ved", "ulated", " rinsed", "<SOS>"], [" shipped", "<META_END>", " Monday", "MEM", "<META_END>", "led", "burg", " pursuant", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" ontology", "<META>", " ontology", " ontology", "<META>", " oneself", " philosophical", "mathcal", "<META>"], ["mitives", "<EOT>", " Philosophy", " philosophers", "<EOT>", "\\xe1", "\u03c1", " oneself", "<EOT>"], [" existence", "<META_START>", " Aristotle", "ism", "<META_START>", "\u03b2", " metaph", " ontology", "<META_START>"], [" consciousness", "<SOS>", " philosophical", "isms", "<SOS>", " causal", " ontology", "ness", "<SOS>"], [" beings", "<META_END>", " metaph", "eced", "<META_END>", " consciously", " truths", ":", "<META_END>"]], "top_logit_ablation_values": [[0.1456, 0, 0.2136, 0.1895, 0, 1.249, 0.1377, 0.08, 0], [-0.09505, 0, 0.1531, 0.181, 0, 1.135, 0.1062, 0.06252, 0], [-0.1415, 0, 0.1491, 0.1369, 0, 1.081, 0.09872, 0.04734, 0], [-0.1618, 0, 0.1229, 0.123, 0, 1.034, 0.09615, 0.04647, 0], [-0.1624, 0, 0.111, 0.04267, 0, 1.024, 0.09393, 0.03892, 0]], "bottom_logit_ablation_values": [[-3.248, 0, -1.281, -2.922, 0, -1.57, -0.449, -0.5513, 0], [-3.187, 0, -1.215, -2.791, 0, -1.515, -0.4275, -0.5436, 0], [-3.132, 0, -1.207, -2.788, 0, -1.474, -0.4239, -0.5429, 0], [-3.111, 0, -1.203, -2.772, 0, -1.442, -0.4215, -0.5343, 0], [-3.078, 0, -1.202, -2.766, 0, -1.429, -0.4211, -0.5341, 0]], "residual_loss_ablation": [0.03167, 1.298, 0.1087, 2.353, 11.06, -0.637, -0.02933, 0.2902, 0]}}, {"tokens_str_list": [" question", " of", " whether", " it", " is", " ethical", " or", " not", " to"], "tokens_acts_list": [0, 1.851, 2.045, 1.188, 2.062, 0.6834, 0, 1.798, 1.252], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 6958822, "ablations": {"loss_ablation": [0, 0, -0.2421, -0.02766, -0.02901, 0.731, -0.02094, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", " County", " County", "amide", " booked", "dehyde", "<META>", "<META>"], ["<EOT>", "<EOT>", " Sheriff", "out", "rapper", " parked", "icillin", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", " Grill", "borough", " expires", "heet", "resistant", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", " sheriff", "shirt", " booked", " purchased", "chedule", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", " bartender", "erry", " purchased", " filed", "week", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", " phenomena", "trical", " relativity", "mitives", "\\xe1", "<META>", "<META>"], ["<EOT>", "<EOT>", " ontology", " phenomena", " philosophical", " phenomena", " Descartes", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", " empirical", " hypotheses", " ontology", " hypothesis", " Aristotle", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", " hypotheses", " inferences", " Philosophy", " inferences", "mmas", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", " inferences", " relativity", " intuition", " hypotheses", " MERCHANTABILITY", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0.1503, 0.2615, 0.0501, 0.4258, 0.4628, 0, 0], [0, 0, -0.1211, -0.06941, -0.03736, 0.3427, 0.4308, 0, 0], [0, 0, -0.1425, -0.08088, -0.03894, 0.3133, 0.4225, 0, 0], [0, 0, -0.154, -0.1064, -0.0732, 0.2952, 0.4216, 0, 0], [0, 0, -0.1667, -0.1169, -0.085, 0.272, 0.3928, 0, 0]], "bottom_logit_ablation_values": [[0, 0, -2.478, -3.096, -2.16, -2.901, -0.4522, 0, 0], [0, 0, -2.469, -2.92, -2.104, -2.665, -0.448, 0, 0], [0, 0, -2.423, -2.909, -2.075, -2.639, -0.447, 0, 0], [0, 0, -2.421, -2.803, -2.057, -2.638, -0.4044, 0, 0], [0, 0, -2.41, -2.786, -2.045, -2.611, -0.3844, 0, 0]], "residual_loss_ablation": [1.578, -0.4884, 5.03, -0.2878, 0.4236, 2.077, 0.5908, -0.9917, 0]}}]}, {"quantile_name": "Subsample Interval 6", "quantile_max_act": 1.682, "examples": [{"tokens_str_list": [" and", " intended", " consequences", " of", " any", " social", " pattern", ".", " Lat"], "tokens_acts_list": [0, 0.336, 0, 1.22, 1.682, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 33636394, "ablations": {"loss_ablation": [-0.08615, 0, 0.07876, 0, 0.1107, 0.1631, 0, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "ette", "<META>", "shirt", "chedule", "<META>", "<META>", "<META>"], [" midfielder", "<EOT>", "keeper", "<EOT>", " FC", "ixel", "<EOT>", "<EOT>", "<EOT>"], [" suburb", "<META_START>", "figh", "<META_START>", " County", "duled", "<META_START>", "<META_START>", "<META_START>"], ["istro", "<SOS>", " Makeup", "<SOS>", " Services", " County", "<SOS>", "<SOS>", "<SOS>"], [" Services", "<META_END>", "ouchers", "<META_END>", " Hurricane", "ergic", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" inferences", "<META>", " Philosophy", "<META>", "ensible", " ourselves", "<META>", "<META>", "<META>"], [" anything", "<EOT>", "usion", "<EOT>", " truths", " Descartes", "<EOT>", "<EOT>", "<EOT>"], [" ourselves", "<META_START>", " Aristotle", "<META_START>", "matical", " Aristotle", "<META_START>", "<META_START>", "<META_START>"], ["polate", "<SOS>", "utable", "<SOS>", " metaph", " itself", "<SOS>", "<SOS>", "<SOS>"], ["bles", "<META_END>", " philosophical", "<META_END>", " philosophers", " philosophers", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[-0.2166, 0, -0.03865, 0, -0.04037, 0.03362, 0, 0, 0], [-0.2704, 0, -0.03895, 0, -0.0806, -0.04838, 0, 0, 0], [-0.2811, 0, -0.04101, 0, -0.0888, -0.06235, 0, 0, 0], [-0.2867, 0, -0.04226, 0, -0.09205, -0.08234, 0, 0, 0], [-0.287, 0, -0.04307, 0, -0.1041, -0.1199, 0, 0, 0]], "bottom_logit_ablation_values": [[-1.383, 0, -0.5113, 0, -1.904, -2.986, 0, 0, 0], [-1.36, 0, -0.5079, 0, -1.83, -2.877, 0, 0, 0], [-1.336, 0, -0.5053, 0, -1.827, -2.82, 0, 0, 0], [-1.325, 0, -0.4957, 0, -1.779, -2.776, 0, 0, 0], [-1.318, 0, -0.4913, 0, -1.777, -2.749, 0, 0, 0]], "residual_loss_ablation": [-1.156, 2.243, 3.475, 0.5927, 0.5111, 1.678, -1.755, 0.4919, 0]}}, {"tokens_str_list": [" argue", " that", " to", " even", " make", " sense", " of", " non", "-"], "tokens_acts_list": [0, 1.176, 0.2643, 1.044, 1.675, 0.6788, 1.908, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 49212205, "ablations": {"loss_ablation": [0, 0, 0.1575, 0.07445, -0.2334, -0.01564, -0.1864, 0.1628, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "shirt", "shirt", "hire", "out", "hire", "shirt", "<META>"], ["<EOT>", "<EOT>", " TX", "chedule", "chedule", "hire", "chedule", " (@", "<EOT>"], ["<META_START>", "<META_START>", "shirts", "ftime", " County", "ctomy", "out", " midfielder", "<META_START>"], ["<SOS>", "<SOS>", " FC", "kins", "sville", "alls", "payers", " parked", "<SOS>"], ["<META_END>", "<META_END>", " Makeup", "ppers", "shirt", "esville", "ier", "cheon", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", " philosophical", " philosophical", " philosophical", "uality", " Aristotle", " Aristotle", "<META>"], ["<EOT>", "<EOT>", " metaph", " truths", " metaph", "istem", " philosophical", " philosophers", "<EOT>"], ["<META_START>", "<META_START>", "istem", "ential", " philosophers", "hea", " Philosophy", "eced", "<META_START>"], ["<SOS>", "<SOS>", "eced", "ity", " Philosophy", "scious", " philosophy", " chapters", "<SOS>"], ["<META_END>", "<META_END>", " philosophers", " metaph", " truths", " Aristotle", " philosophers", "ifies", "<META_END>"]], "top_logit_ablation_values": [[0, 0, -0.3463, -0.0167, -0.3207, 0.1548, 0.3261, 0.5217, 0], [0, 0, -0.4217, -0.05819, -0.3252, -0.02381, 0.319, 0.4689, 0], [0, 0, -0.4735, -0.06263, -0.4181, -0.1052, 0.3015, 0.443, 0], [0, 0, -0.5011, -0.08903, -0.4461, -0.1227, 0.2901, 0.3154, 0], [0, 0, -0.5544, -0.08938, -0.4472, -0.1315, 0.2738, 0.2842, 0]], "bottom_logit_ablation_values": [[0, 0, -2.268, -0.5232, -2.251, -2.453, -0.948, -2.399, 0], [0, 0, -2.224, -0.5157, -2.212, -2.339, -0.9246, -2.347, 0], [0, 0, -2.206, -0.5133, -2.167, -2.335, -0.8904, -2.339, 0], [0, 0, -2.197, -0.5127, -2.128, -2.29, -0.8793, -2.297, 0], [0, 0, -2.175, -0.5044, -2.119, -2.286, -0.8691, -2.278, 0]], "residual_loss_ablation": [1.592, 2.713, 0.1641, 0.5015, 1.248, 1.654, 0.7925, 1.848, 0]}}, {"tokens_str_list": [" perfect", " than", " myself", ".", " (", "AT", " 7", ":", " 8"], "tokens_acts_list": [1.46, 2.83, 0.6312, 0.01667, 1.673, 0.735, 0, 0.8413, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 11892952, "ablations": {"loss_ablation": [0.1123, -0.1462, -0.09492, -0.06933, 0.002265, -0.2008, -0.01831, 0, 0], "top_logit_ablation_tokens_str": [["ixel", "ed", "oxide", "lee", "shirt", "shirt", "shirt", "<META>", "<META>"], [" resistant", "ned", " TX", "hire", " TX", "out", "AP", "<EOT>", "<EOT>"], [" expensive", "led", "sgiving", "irt", "1", "AP", "arrow", "<META_START>", "<META_START>"], [" booked", "hire", "ixel", " booked", "2", "hire", "ixel", "<SOS>", "<SOS>"], [" affordable", "ette", "shirt", "!\u201d", "hire", " County", " TX", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" hypothesis", " contradictory", " hypotheses", " ontology", " empirical", " philosophical", " philosophical", "<META>", "<META>"], ["arity", " Aristotle", " causal", " philosophical", " oneself", " philosophers", " philosophers", "<EOT>", "<EOT>"], ["ifies", " philosophers", " inferences", " theories", " ontology", " metaph", " existence", "<META_START>", "<META_START>"], ["ity", " empirical", " ontology", " causal", " conceptual", " ontology", " ontology", "<SOS>", "<SOS>"], ["ification", " philosophical", " contexts", " metaph", " rational", " truths", " metaph", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[1.11, 0.5797, -0.02709, 0.06667, -0.001095, 0.1035, 0.04875, 0, 0], [0.9923, 0.3444, -0.1301, 0.06437, -0.001188, -0.1062, -0.06516, 0, 0], [0.9452, 0.2241, -0.1397, 0.05446, -0.001233, -0.1303, -0.1087, 0, 0], [0.9305, 0.221, -0.166, 0.04912, -0.001293, -0.1491, -0.1126, 0, 0], [0.8944, 0.2001, -0.1675, 0.04893, -0.00177, -0.1903, -0.1142, 0, 0]], "bottom_logit_ablation_values": [[-2.247, -1.822, -4.004, -0.8425, -0.01763, -2.574, -1.232, 0, 0], [-2.197, -1.782, -3.996, -0.7915, -0.01754, -2.541, -1.202, 0, 0], [-2.196, -1.758, -3.951, -0.7822, -0.01732, -2.53, -1.199, 0, 0], [-1.968, -1.755, -3.885, -0.7785, -0.01687, -2.497, -1.194, 0, 0], [-1.942, -1.734, -3.838, -0.7752, -0.01686, -2.49, -1.193, 0, 0]], "residual_loss_ablation": [0.2766, 2.494, 0.8061, 1.093, 0.3258, -0.6625, 0.4098, -1.215, 0]}}, {"tokens_str_list": ["concept", "ual", " element", " will", " take", " center", " stage", ".", " It"], "tokens_acts_list": [0, 0.5287, 0.3658, 1.542, 1.665, 0.8623, 0, 0, 1.547], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 95987148, "ablations": {"loss_ablation": [0.6389, 0, -0.01311, 0.02885, -0.1021, -0.3995, -0.1162, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "ved", "hire", "hire", "away", "chedule", "<META>", "<META>"], ["hire", "<EOT>", "izer", "raiser", "chedule", "out", "shirt", "<EOT>", "<EOT>"], ["out", "<META_START>", "gie", "led", "inafter", "chedule", "days", "<META_START>", "<META_START>"], ["oxide", "<SOS>", "runch", "als", "shirt", "shirt", "arrow", "<SOS>", "<SOS>"], ["sville", "<META_END>", "chedule", "ette", "day", "month", "esville", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophical", "<META>", " Aristotle", " philosophical", " reasoning", " Aristotle", " relativity", "<META>", "<META>"], [" Aristotle", "<EOT>", " their", " philosophers", " philosophical", "inciple", " Aristotle", "<EOT>", "<EOT>"], [" metaph", "<META_START>", " these", " Philosophy", " Aristotle", " philosophers", " philosophers", "<META_START>", "<META_START>"], [" itself", "<SOS>", " its", "\u03c0", " morality", " Descartes", " philosophical", "<SOS>", "<SOS>"], [" reasoning", "<META_END>", " those", " Aristotle", " philosophers", " relativity", " existence", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.4376, 0, -0.08559, 0.07258, -0.009334, 0.02491, -0.0123, 0, 0], [0.3495, 0, -0.1011, 0.06165, -0.1348, -0.1435, -0.04019, 0, 0], [0.2963, 0, -0.1056, 0.05229, -0.1619, -0.1873, -0.05706, 0, 0], [0.2766, 0, -0.1063, 0.04337, -0.1921, -0.3591, -0.0813, 0, 0], [0.2258, 0, -0.1204, 0.03914, -0.2119, -0.3968, -0.09635, 0, 0]], "bottom_logit_ablation_values": [[-2.463, 0, -0.7454, -0.3867, -2.393, -2.88, -1.6, 0, 0], [-2.413, 0, -0.7181, -0.3846, -2.368, -2.864, -1.476, 0, 0], [-2.389, 0, -0.715, -0.375, -2.319, -2.847, -1.453, 0, 0], [-2.356, 0, -0.7109, -0.3728, -2.308, -2.819, -1.449, 0, 0], [-2.333, 0, -0.7022, -0.3722, -2.293, -2.796, -1.444, 0, 0]], "residual_loss_ablation": [1.7, 12.5, 0.3118, -1.001, -0.1316, 1.186, 4.955, 0.402, 0]}}, {"tokens_str_list": ["\n", "on", " the", " doctrine", " of", " Scripture", ";", " and", " it"], "tokens_acts_list": [0.321, 0, 0, 0, 1.664, 0.08219, 0.1199, 0, 0.8622], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 43314533, "ablations": {"loss_ablation": [0, -0.04573, 0, 0, 0, -0.1255, 0.0008252, -0.01505, 0], "top_logit_ablation_tokens_str": [["<META>", "shirt", "<META>", "<META>", "<META>", "shirt", "burg", " County", "<META>"], ["<EOT>", "out", "<EOT>", "<EOT>", "<EOT>", "cheon", " County", "<EOT>", "<EOT>"], ["<META_START>", "hire", "<META_START>", "<META_START>", "<META_START>", "\n\f", "<EOT>", "shirt", "<META_START>"], ["<SOS>", "days", "<SOS>", "<SOS>", "<SOS>", " County", "!!!", " Diocese", "<SOS>"], ["<META_END>", "burg", "<META_END>", "<META_END>", "<META_END>", "oxide", "iscopal", "\n\f", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " philosophical", "<META>", "<META>", "<META>", " ontology", " ontology", " ontology", "<META>"], ["<EOT>", " metaph", "<EOT>", "<EOT>", "<EOT>", " oneself", " empirical", " causal", "<EOT>"], ["<META_START>", " ontology", "<META_START>", "<META_START>", "<META_START>", " Aristotle", " philosophical", "ential", "<META_START>"], ["<SOS>", " reasoning", "<SOS>", "<SOS>", "<SOS>", "wikipedia", " causal", " empirical", "<SOS>"], ["<META_END>", " philosophers", "<META_END>", "<META_END>", "<META_END>", " assumptions", " mathematical", " oneself", "<META_END>"]], "top_logit_ablation_values": [[0, 0.1802, 0, 0, 0, 0.5873, 0.01047, -0.00611, 0], [0, 0.1446, 0, 0, 0, 0.4864, 0.00425, -0.01, 0], [0, 0.1325, 0, 0, 0, 0.476, 0.002479, -0.01232, 0], [0, 0.1214, 0, 0, 0, 0.4726, 0.001012, -0.01577, 0], [0, 0.114, 0, 0, 0, 0.4501, -0.0004913, -0.01584, 0]], "bottom_logit_ablation_values": [[0, -0.4244, 0, 0, 0, -1.786, -0.08623, -0.1726, 0], [0, -0.4179, 0, 0, 0, -1.774, -0.08556, -0.1587, 0], [0, -0.4076, 0, 0, 0, -1.694, -0.08468, -0.1567, 0], [0, -0.4067, 0, 0, 0, -1.693, -0.08396, -0.1562, 0], [0, -0.4024, 0, 0, 0, -1.687, -0.08274, -0.1556, 0]], "residual_loss_ablation": [3.453, 0.5114, 0.8299, 0.3656, -0.2443, 2.774, 1.107, 0.3689, 0]}}]}, {"quantile_name": "Subsample Interval 7", "quantile_max_act": 1.26, "examples": [{"tokens_str_list": [" be", " taken", " to", " refer", " to", " the", " pre", "given", " environment"], "tokens_acts_list": [0.05709, 0, 0, 0, 1.26, 0.11, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 55620112, "ablations": {"loss_ablation": [-0.00696, -0.01341, 0, 0, 0, 0.01481, 0.00298, 0, 0], "top_logit_ablation_tokens_str": [["hire", " booked", "<META>", "<META>", "<META>", "shirt", "ixel", "<META>", "<META>"], ["shirt", " purchased", "<EOT>", "<EOT>", "<EOT>", " County", "shirt", "<EOT>", "<EOT>"], ["burg", " evacuated", "<META_START>", "<META_START>", "<META_START>", "ixel", " mall", "<META_START>", "<META_START>"], ["ette", " shipped", "<SOS>", "<SOS>", "<SOS>", "inafter", " Philippines", "<SOS>", "<SOS>"], ["days", " cleaned", "<META_END>", "<META_END>", "<META_END>", "shire", " youngest", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" ontology", " existence", "<META>", "<META>", "<META>", "eness", "actic", "<META>", "<META>"], [" philosophical", " hypothesis", "<EOT>", "<EOT>", "<EOT>", " truths", " Aristotle", "<EOT>", "<EOT>"], [" reasoning", " theory", "<META_START>", "<META_START>", "<META_START>", "tru", " Marx", "<META_START>", "<META_START>"], [" Aristotle", " ontology", "<SOS>", "<SOS>", "<SOS>", " philosophical", "ifies", "<SOS>", "<SOS>"], [" Descartes", " truths", "<META_END>", "<META_END>", "<META_END>", " hypothesis", " its", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.01135, 0.01299, 0, 0, 0, 0.08665, 0.004978, 0, 0], [0.007156, 0.006839, 0, 0, 0, 0.05846, 0.002788, 0, 0], [0.002265, 0.00597, 0, 0, 0, 0.02261, -0.001755, 0, 0], [0.002085, 0.004596, 0, 0, 0, -0.0003037, -0.002802, 0, 0], [0.001462, 0.002423, 0, 0, 0, -0.006104, -0.003279, 0, 0]], "bottom_logit_ablation_values": [[-0.1496, -0.09765, 0, 0, 0, -1.953, -0.1415, 0, 0], [-0.1495, -0.09602, 0, 0, 0, -1.944, -0.1325, 0, 0], [-0.1462, -0.09438, 0, 0, 0, -1.883, -0.1315, 0, 0], [-0.1451, -0.09428, 0, 0, 0, -1.869, -0.1305, 0, 0], [-0.1432, -0.09383, 0, 0, 0, -1.867, -0.1296, 0, 0]], "residual_loss_ablation": [0.1585, -1.118, 0.4177, 2.206, -0.1115, -0.321, 0.348, 1.644, 0]}}, {"tokens_str_list": [",", " memory", ",", " and", " other", " cognitive", " capacities", " are", " typically"], "tokens_acts_list": [0, 0, 0, 0, 1.259, 0, 0, 0.4491, 0.3426], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 30195414, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.1334, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "shirt", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "ixel", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "fits", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "shots", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "boys", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " ourselves", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " Marx", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " Descartes", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " negation", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " Aristotle", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.1355, 0, 0, 0], [0, 0, 0, 0, 0, 0.1352, 0, 0, 0], [0, 0, 0, 0, 0, 0.1069, 0, 0, 0], [0, 0, 0, 0, 0, 0.05076, 0, 0, 0], [0, 0, 0, 0, 0, -0.002106, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -1.849, 0, 0, 0], [0, 0, 0, 0, 0, -1.806, 0, 0, 0], [0, 0, 0, 0, 0, -1.761, 0, 0, 0], [0, 0, 0, 0, 0, -1.71, 0, 0, 0], [0, 0, 0, 0, 0, -1.707, 0, 0, 0]], "residual_loss_ablation": [-1.328, 2.179, -0.5455, -0.7513, -1.341, 2.34, 1.463, 1.569, 0]}}, {"tokens_str_list": [" was", " developed", " mainly", " by", " an", " apost", "ate", " div", "inity"], "tokens_acts_list": [0.9422, 1.039, 0, 1.604, 1.257, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 83332593, "ablations": {"loss_ablation": [0, -0.1259, -0.02269, 0, 0.2084, -0.04695, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", " shipped", " Makeup", "<META>", " County", "shirt", "<META>", "<META>", "<META>"], ["<EOT>", " purchased", "resistant", "<EOT>", " Hurricane", "ixel", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", " booked", " Wednesday", "<META_START>", "shirt", " oven", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", " parked", " Tuesday", "<SOS>", " Kit", "starter", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", " filed", " Friday", "<META_END>", "sville", " Office", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " inferences", " interpretation", "<META>", " inferences", "ifies", "<META>", "<META>", "<META>"], ["<EOT>", " ontology", " inferences", "<EOT>", " meanings", " certain", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", " hypothesis", "matical", "<META_START>", "ibility", " priori", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "mitives", " meanings", "<SOS>", " representations", "leness", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", " notions", " interpretations", "<META_END>", " sake", "inely", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, -0.07797, 0.3117, 0, -0.1397, 0.01716, 0, 0, 0], [0, -0.08292, 0.2971, 0, -0.1768, -0.1354, 0, 0, 0], [0, -0.09554, 0.2879, 0, -0.1835, -0.1871, 0, 0, 0], [0, -0.1103, 0.2826, 0, -0.1914, -0.2114, 0, 0, 0], [0, -0.1109, 0.2701, 0, -0.2067, -0.2237, 0, 0, 0]], "bottom_logit_ablation_values": [[0, -1.546, -0.9674, 0, -2.087, -2.013, 0, 0, 0], [0, -1.515, -0.9436, 0, -2.067, -1.912, 0, 0, 0], [0, -1.489, -0.9318, 0, -2.064, -1.865, 0, 0, 0], [0, -1.478, -0.9248, 0, -2.051, -1.857, 0, 0, 0], [0, -1.466, -0.9191, 0, -2.044, -1.852, 0, 0, 0]], "residual_loss_ablation": [-0.3831, 1.62, 0.04447, 1.698, 0.2864, -0.2468, 5.308, -2.055, 0]}}, {"tokens_str_list": [" advantage", " that", ",", " if", " used", " as", " a", " \"", "behaviour"], "tokens_acts_list": [0, 1.413, 0, 1.598, 1.247, 2.222, 1.683, 1.455, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 19224658, "ablations": {"loss_ablation": [0, 0, 0.08807, 0, -0.2769, 0.05534, -0.05063, 0.2765, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", " Makeup", "<META>", "hire", " ashore", " TX", "chedule", "<META>"], ["<EOT>", "<EOT>", " TX", "<EOT>", " TX", "chedule", "rolled", "ixel", "<EOT>"], ["<META_START>", "<META_START>", " County", "<META_START>", "shirt", " ago", "amide", " midfielder", "<META_START>"], ["<SOS>", "<SOS>", "shirt", "<SOS>", " booked", "resistant", " volleyball", " Makeup", "<SOS>"], ["<META_END>", "<META_END>", " mall", "<META_END>", "ungal", "icillin", "chedule", " mall", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "licity", "<META>", "itions", " ontology", "uality", " ourselves", "<META>"], ["<EOT>", "<EOT>", "ential", "<EOT>", " ontology", " sake", "ighteous", " oneself", "<EOT>"], ["<META_START>", "<META_START>", " meanings", "<META_START>", " negation", " negation", " ontology", "ions", "<META_START>"], ["<SOS>", "<SOS>", " negation", "<SOS>", " meanings", " metaph", " meanings", " truths", "<SOS>"], ["<META_END>", "<META_END>", "trical", "<META_END>", " philosophical", "sofar", "arrative", " meanings", "<META_END>"]], "top_logit_ablation_values": [[0, 0, -0.3614, 0, 0.02301, -0.2758, -0.05462, -0.08352, 0], [0, 0, -0.3871, 0, 0.01796, -0.3152, -0.06242, -0.09656, 0], [0, 0, -0.4079, 0, -0.06251, -0.3343, -0.1196, -0.1116, 0], [0, 0, -0.4905, 0, -0.06582, -0.3365, -0.1718, -0.1472, 0], [0, 0, -0.5688, 0, -0.07069, -0.3574, -0.1747, -0.1543, 0]], "bottom_logit_ablation_values": [[0, 0, -2.461, 0, -2.344, -2.137, -2.902, -2.413, 0], [0, 0, -2.423, 0, -2.317, -2.115, -2.895, -2.348, 0], [0, 0, -2.411, 0, -2.282, -2.075, -2.886, -2.26, 0], [0, 0, -2.402, 0, -2.278, -2.044, -2.87, -2.257, 0], [0, 0, -2.392, 0, -2.255, -2.036, -2.834, -2.221, 0]], "residual_loss_ablation": [0.2377, 0.8474, 0.1899, 0.7732, 2.034, -0.1017, -0.1311, 0.4223, 0]}}, {"tokens_str_list": [",", " but", " such", " bounds", " also", " hold", " immediately", " for", " non"], "tokens_acts_list": [0, 0.3983, 0, 0, 1.243, 0, 0.1603, 0.3363, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 96643509, "ablations": {"loss_ablation": [0, 0, 0.07924, 0, 0, -0.08978, 0, -0.01955, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", " TX", "<META>", "<META>", "ixel", "<META>", " booked", "<META>"], ["<EOT>", "<EOT>", "lop", "<EOT>", "<EOT>", " fees", "<EOT>", "County", "<EOT>"], ["<META_START>", "<META_START>", " County", "<META_START>", "<META_START>", "lop", "<META_START>", "out", "<META_START>"], ["<SOS>", "<SOS>", "hire", "<SOS>", "<SOS>", "gage", "<SOS>", " County", "<SOS>"], ["<META_END>", "<META_END>", "shirt", "<META_END>", "<META_END>", "chedule", "<META_END>", " TX", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", " philosophical", "<META>", "<META>", " philosophers", "<META>", " philosophers", "<META>"], ["<EOT>", "<EOT>", " metaph", "<EOT>", "<EOT>", " philosophical", "<EOT>", " philosophical", "<EOT>"], ["<META_START>", "<META_START>", " philosophers", "<META_START>", "<META_START>", " metaph", "<META_START>", " metaph", "<META_START>"], ["<SOS>", "<SOS>", " truths", "<SOS>", "<SOS>", " Aristotle", "<SOS>", " morality", "<SOS>"], ["<META_END>", "<META_END>", " Philosophy", "<META_END>", "<META_END>", " Descartes", "<META_END>", " Aristotle", "<META_END>"]], "top_logit_ablation_values": [[0, 0, -0.07261, 0, 0, -0.2172, 0, -0.0374, 0], [0, 0, -0.07388, 0, 0, -0.2452, 0, -0.03741, 0], [0, 0, -0.09599, 0, 0, -0.2459, 0, -0.03783, 0], [0, 0, -0.1027, 0, 0, -0.2467, 0, -0.0388, 0], [0, 0, -0.1065, 0, 0, -0.2468, 0, -0.03896, 0]], "bottom_logit_ablation_values": [[0, 0, -0.6927, 0, 0, -2.134, 0, -0.2775, 0], [0, 0, -0.6889, 0, 0, -2.131, 0, -0.2757, 0], [0, 0, -0.6751, 0, 0, -2.097, 0, -0.2672, 0], [0, 0, -0.6556, 0, 0, -2.063, 0, -0.2638, 0], [0, 0, -0.6522, 0, 0, -2.057, 0, -0.2633, 0]], "residual_loss_ablation": [-0.08406, 0.2856, 0.3509, 6.891, 2.308, 0.6721, -0.248, -0.6581, 0]}}]}, {"quantile_name": "Subsample Interval 8", "quantile_max_act": 0.8423, "examples": [{"tokens_str_list": [" attention", " is", " shifted", " away", " from", " the", " emotional", " stimulus", " to"], "tokens_acts_list": [0, 0, 0, 0, 0.8423, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 32422075, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.03143, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "ixel", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "shirt", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " mall", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " FC", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "chedule", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " philosophers", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " Aristotle", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " philosophical", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " metaph", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " Marx", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.1938, 0, 0, 0], [0, 0, 0, 0, 0, 0.1133, 0, 0, 0], [0, 0, 0, 0, 0, 0.03097, 0, 0, 0], [0, 0, 0, 0, 0, -0.01375, 0, 0, 0], [0, 0, 0, 0, 0, -0.0434, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -1.19, 0, 0, 0], [0, 0, 0, 0, 0, -1.152, 0, 0, 0], [0, 0, 0, 0, 0, -1.119, 0, 0, 0], [0, 0, 0, 0, 0, -1.118, 0, 0, 0], [0, 0, 0, 0, 0, -1.104, 0, 0, 0]], "residual_loss_ablation": [0.2608, 1.403, 1.692, 2.776, 0.378, -0.3863, 1.371, 2.897, 0]}}, {"tokens_str_list": [" him", " to", " suppose", " that", " species", " themselves", " undergo", " change", "."], "tokens_acts_list": [0.4469, 0, 0.8787, 1.173, 0.8389, 1.241, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 1507931, "ablations": {"loss_ablation": [0.0002608, 0.03508, 0, -0.02775, -0.2339, 0.1062, -0.1901, 0, 0], "top_logit_ablation_tokens_str": [["shirt", " ashore", "<META>", "cin", "shirt", " ashore", "hire", "<META>", "<META>"], ["cin", "burg", "<EOT>", "sville", " trout", "chedule", " ashore", "<EOT>", "<EOT>"], [" trout", "hire", "<META_START>", "out", " larvae", "hire", "borough", "<META_START>", "<META_START>"], ["days", "wich", "<SOS>", "stown", "shire", "peed", " booked", "<SOS>", "<SOS>"], ["hire", " booked", "<META_END>", " ashore", "shirts", "boat", " evacuated", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" reasoning", " ontology", "<META>", " philosophical", " philosophical", " philosophical", "ential", "<META>", "<META>"], [" philosophical", " philosophical", "<EOT>", " ontology", " conceptual", " reasoning", " philosophical", "<EOT>", "<EOT>"], ["ential", " philosophers", "<META_START>", " reasoning", " truths", " philosophers", " ontology", "<META_START>", "<META_START>"], [" ontology", " empirical", "<SOS>", " conceptual", " ontology", " empirical", " reasoning", "<SOS>", "<SOS>"], [" truths", " metaph", "<META_END>", " philosophers", " empirical", " Freud", " Philosophy", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[-0.04559, 0.06202, 0, 0.1009, -0.1206, 0.008713, -0.116, 0, 0], [-0.07749, 0.05104, 0, 0.09931, -0.2423, -0.03998, -0.1506, 0, 0], [-0.0867, 0.04653, 0, 0.09696, -0.2677, -0.04187, -0.1563, 0, 0], [-0.09205, 0.03738, 0, 0.07784, -0.3108, -0.04786, -0.1769, 0, 0], [-0.09278, 0.0345, 0, 0.04988, -0.3271, -0.05072, -0.1948, 0, 0]], "bottom_logit_ablation_values": [[-0.8723, -0.5256, 0, -1.326, -2.117, -1.176, -1.806, 0, 0], [-0.8554, -0.5084, 0, -1.302, -2.082, -1.122, -1.798, 0, 0], [-0.8527, -0.5019, 0, -1.282, -2.039, -1.114, -1.789, 0, 0], [-0.8514, -0.4971, 0, -1.279, -2.037, -1.108, -1.769, 0, 0], [-0.8221, -0.4858, 0, -1.278, -2.018, -1.105, -1.756, 0, 0]], "residual_loss_ablation": [2.931, -0.7957, 2.391, 0.9572, 5.529, 1.77, 3.57, 1.539, 0]}}, {"tokens_str_list": [" where", " you", " can", " input", " any", " conclusion", " that", " you", " want"], "tokens_acts_list": [0.0359, 0, 0, 0, 0.8355, 0, 0.4511, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 37764580, "ablations": {"loss_ablation": [0, -0.007581, 0, 0, 0, 0.158, 0, -0.1195, 0], "top_logit_ablation_tokens_str": [["<META>", " Hurricane", "<META>", "<META>", "<META>", "chedule", "<META>", " TX", "<META>"], ["<EOT>", " County", "<EOT>", "<EOT>", "<EOT>", "amide", "<EOT>", " Makeup", "<EOT>"], ["<META_START>", " Rangers", "<META_START>", "<META_START>", "<META_START>", " weekend", "<META_START>", " County", "<META_START>"], ["<SOS>", " Diocese", "<SOS>", "<SOS>", "<SOS>", " TX", "<SOS>", " weekend", "<SOS>"], ["<META_END>", " TX", "<META_END>", "<META_END>", "<META_END>", " fees", "<META_END>", "amide", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " hypothesis", "<META>", "<META>", "<META>", " existence", "<META>", "trical", "<META>"], ["<EOT>", "ifies", "<EOT>", "<EOT>", "<EOT>", " Aristotle", "<EOT>", " ontology", "<EOT>"], ["<META_START>", "gment", "<META_START>", "<META_START>", "<META_START>", " Descartes", "<META_START>", " existence", "<META_START>"], ["<SOS>", "eness", "<SOS>", "<SOS>", "<SOS>", " philosophers", "<SOS>", " metaph", "<SOS>"], ["<META_END>", " ontology", "<META_END>", "<META_END>", "<META_END>", " Marx", "<META_END>", " philosophical", "<META_END>"]], "top_logit_ablation_values": [[0, 3.982e-05, 0, 0, 0, 0.02759, 0, 0.009916, 0], [0, -0.0006863, 0, 0, 0, 0.01293, 0, -0.000114, 0], [0, -0.001281, 0, 0, 0, -0.03175, 0, -0.009004, 0], [0, -0.001583, 0, 0, 0, -0.0495, 0, -0.02799, 0], [0, -0.001669, 0, 0, 0, -0.05484, 0, -0.03639, 0]], "bottom_logit_ablation_values": [[0, -0.05164, 0, 0, 0, -1.406, 0, -0.7157, 0], [0, -0.05116, 0, 0, 0, -1.398, 0, -0.7092, 0], [0, -0.05109, 0, 0, 0, -1.386, 0, -0.7091, 0], [0, -0.0508, 0, 0, 0, -1.352, 0, -0.7008, 0], [0, -0.05066, 0, 0, 0, -1.302, 0, -0.6979, 0]], "residual_loss_ablation": [1.239, -0.05517, -0.2842, 1.134, -0.1608, -0.2503, -1.234, 0.4775, 0]}}, {"tokens_str_list": [" act", " and", " respond", " to", " each", " other", ".", " For", " instance"], "tokens_acts_list": [0, 0, 0.3673, 0.4279, 0.8327, 0, 0, 1.048, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 84511894, "ablations": {"loss_ablation": [0, 0, 0, -0.007204, -0.0344, -0.06995, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "out", "chedule", "chedule", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "ers", "shirt", "shirt", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "arrow", "shirts", "amide", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "outs", " firefighters", " weekend", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "ls", " County", "bike", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", " Philosophy", " Aristotle", " Aristotle", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "utable", " ontology", " philosophers", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", " philosophical", "matical", " Descartes", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", " ontology", " Descartes", " truths", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", " interpretation", " philosophers", " negation", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0.2229, 0.1326, 0.09592, 0, 0, 0], [0, 0, 0, 0.1874, 0.1133, 0.08905, 0, 0, 0], [0, 0, 0, 0.1837, 0.098, 0.08042, 0, 0, 0], [0, 0, 0, 0.1637, 0.09594, 0.07997, 0, 0, 0], [0, 0, 0, 0.1568, 0.09485, 0.07297, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, -0.4247, -0.5263, -1.296, 0, 0, 0], [0, 0, 0, -0.4189, -0.5143, -1.289, 0, 0, 0], [0, 0, 0, -0.403, -0.5127, -1.263, 0, 0, 0], [0, 0, 0, -0.4029, -0.5118, -1.224, 0, 0, 0], [0, 0, 0, -0.4023, -0.5098, -1.202, 0, 0, 0]], "residual_loss_ablation": [0.7026, 1.046, 1.297, 2.809, -0.738, 0.2145, 3.085, 0.2178, 0]}}, {"tokens_str_list": [" if", " the", " sub", "contract", " contains", " words", " of", " definite", " limitation"], "tokens_acts_list": [0, 0, 0, 0, 0.8266, 0, 0, 1.398, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 65244606, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.2982, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " TX", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " fees", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "grant", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " County", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "erry", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " philosophers", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " Descartes", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " metaph", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " philosophical", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " Aristotle", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, -0.316, 0, 0, 0], [0, 0, 0, 0, 0, -0.3486, 0, 0, 0], [0, 0, 0, 0, 0, -0.3621, 0, 0, 0], [0, 0, 0, 0, 0, -0.3833, 0, 0, 0], [0, 0, 0, 0, 0, -0.3873, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -1.562, 0, 0, 0], [0, 0, 0, 0, 0, -1.548, 0, 0, 0], [0, 0, 0, 0, 0, -1.536, 0, 0, 0], [0, 0, 0, 0, 0, -1.535, 0, 0, 0], [0, 0, 0, 0, 0, -1.522, 0, 0, 0]], "residual_loss_ablation": [0.03835, 0.1995, 0.1464, 5.966, 3.38, 1.527, -0.7352, 4.671, 0]}}]}, {"quantile_name": "Subsample Interval 9", "quantile_max_act": 0.4213, "examples": [{"tokens_str_list": [" metabolism", ").", " Perhaps", ",", " for", " all", " we", " know", ","], "tokens_acts_list": [0, 0, 1.065, 0, 0.4213, 0, 0, 0.1414, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 82532643, "ablations": {"loss_ablation": [0, 0, 0, -0.03685, 0, 0.01895, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "shirt", "<META>", "hire", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "cin", "<EOT>", "shirt", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", " TX", "<META_START>", "stown", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "sville", "<SOS>", " County", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", " County", "<META_END>", " TX", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", " philosophical", "<META>", " philosophers", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", " philosophers", "<EOT>", " philosophical", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", " truths", "<META_START>", " truths", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", " meanings", "<SOS>", " notions", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", " metaph", "<META_END>", " Descartes", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, -0.3621, 0, -0.1137, 0, 0, 0], [0, 0, 0, -0.3957, 0, -0.1418, 0, 0, 0], [0, 0, 0, -0.4015, 0, -0.1451, 0, 0, 0], [0, 0, 0, -0.4343, 0, -0.1579, 0, 0, 0], [0, 0, 0, -0.4492, 0, -0.1599, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, -1.934, 0, -0.8873, 0, 0, 0], [0, 0, 0, -1.918, 0, -0.8788, 0, 0, 0], [0, 0, 0, -1.909, 0, -0.8738, 0, 0, 0], [0, 0, 0, -1.894, 0, -0.8552, 0, 0, 0], [0, 0, 0, -1.871, 0, -0.8528, 0, 0, 0]], "residual_loss_ablation": [-0.596, 7.833, 1.677, 1.162, 0.4147, -1.005, 0.9378, 0.5208, 0]}}, {"tokens_str_list": [",", " we", " further", " urge", " that", " neuro", "eth", "ics", " should"], "tokens_acts_list": [0, 0, 0, 0, 0.421, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 17019702, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.06635, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " Makeup", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " TX", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " Veterans", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "grant", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "ctomy", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "trical", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " existence", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " meanings", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " negation", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "licity", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.07502, 0, 0, 0], [0, 0, 0, 0, 0, 0.008121, 0, 0, 0], [0, 0, 0, 0, 0, -0.001597, 0, 0, 0], [0, 0, 0, 0, 0, -0.008532, 0, 0, 0], [0, 0, 0, 0, 0, -0.00963, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.6554, 0, 0, 0], [0, 0, 0, 0, 0, -0.6498, 0, 0, 0], [0, 0, 0, 0, 0, -0.6444, 0, 0, 0], [0, 0, 0, 0, 0, -0.6434, 0, 0, 0], [0, 0, 0, 0, 0, -0.6322, 0, 0, 0]], "residual_loss_ablation": [0.01802, 0.1057, 0.6971, 0.03656, 6.269, 3.051, 1.261, 8.681, 0]}}, {"tokens_str_list": [" the", " proposition", ",", " with", " which", " I", " have", " no", " quarrel"], "tokens_acts_list": [0, 0, 0, 0.4462, 0.4182, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 45118788, "ablations": {"loss_ablation": [0.008945, 0, 0, 0, -0.01745, -0.1592, 0, 0, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "<META>", "<META>", "shirt", "shirt", "<META>", "<META>", "<META>"], [" TX", "<EOT>", "<EOT>", "<EOT>", " County", " TX", "<EOT>", "<EOT>", "<EOT>"], [" booked", "<META_START>", "<META_START>", "<META_START>", " TX", " airport", "<META_START>", "<META_START>", "<META_START>"], [" County", "<SOS>", "<SOS>", "<SOS>", "cin", " County", "<SOS>", "<SOS>", "<SOS>"], ["hire", "<META_END>", "<META_END>", "<META_END>", " booked", "hire", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" ontology", "<META>", "<META>", "<META>", " philosophical", " philosophical", "<META>", "<META>", "<META>"], [" philosophical", "<EOT>", "<EOT>", "<EOT>", " philosophers", " truths", "<EOT>", "<EOT>", "<EOT>"], [" truths", "<META_START>", "<META_START>", "<META_START>", " ontology", " oneself", "<META_START>", "<META_START>", "<META_START>"], [" philosophers", "<SOS>", "<SOS>", "<SOS>", " truths", " existence", "<SOS>", "<SOS>", "<SOS>"], [" hypotheses", "<META_END>", "<META_END>", "<META_END>", " morality", " negation", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[-0.01178, 0, 0, 0, -0.09054, -0.1108, 0, 0, 0], [-0.02025, 0, 0, 0, -0.1056, -0.1531, 0, 0, 0], [-0.02309, 0, 0, 0, -0.1106, -0.1583, 0, 0, 0], [-0.02442, 0, 0, 0, -0.1428, -0.1611, 0, 0, 0], [-0.02622, 0, 0, 0, -0.1492, -0.1643, 0, 0, 0]], "bottom_logit_ablation_values": [[-0.1969, 0, 0, 0, -0.8527, -0.8564, 0, 0, 0], [-0.1961, 0, 0, 0, -0.8485, -0.8259, 0, 0, 0], [-0.1944, 0, 0, 0, -0.8387, -0.8116, 0, 0, 0], [-0.1935, 0, 0, 0, -0.8357, -0.7983, 0, 0, 0], [-0.1934, 0, 0, 0, -0.8307, -0.797, 0, 0, 0]], "residual_loss_ablation": [-0.5756, 1.161, 1.263, -0.1405, 1.605, 0.8605, -1.037, 0.6462, 0]}}, {"tokens_str_list": [" of", " production", " whose", " overriding", " objective", " is", " to", " produce", ","], "tokens_acts_list": [0, 0, 0, 0, 0.4172, 0.4723, 0.1232, 0.4604, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 69664435, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, -0.04461, -0.0513, -0.02061, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " Makeup", "oprop", "hire", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "gie", "amide", " pump", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "boat", " depot", "}}$.", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "led", "orta", "roller", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "roller", "ulf", "chedule", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " Aristotle", " meanings", " philosophical", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " philosophical", "utable", " inferences", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " Philosophy", " philosophers", " Descartes", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "utable", " inferences", " meanings", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " Descartes", " Philosophy", " truths", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.0313, 0.2195, 0.01234, 0], [0, 0, 0, 0, 0, 0.01788, 0.2047, 0.002993, 0], [0, 0, 0, 0, 0, 0.007093, 0.1735, 0.0004405, 0], [0, 0, 0, 0, 0, 0.003464, 0.1729, -5.925e-05, 0], [0, 0, 0, 0, 0, 0.001065, 0.1715, -0.0008447, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.5445, -0.5251, -0.1771, 0], [0, 0, 0, 0, 0, -0.5278, -0.5217, -0.1747, 0], [0, 0, 0, 0, 0, -0.5161, -0.5173, -0.1717, 0], [0, 0, 0, 0, 0, -0.5122, -0.5166, -0.1698, 0], [0, 0, 0, 0, 0, -0.5089, -0.5052, -0.1691, 0]], "residual_loss_ablation": [0.1811, 7.341, 0.4325, -0.3657, 2.959, 3.317, 1.823, 2.686, 0]}}, {"tokens_str_list": [" moment", " when", " we", " believe", " an", " experience", " to", " be", " one"], "tokens_acts_list": [0, 0, 0, 0, 0.4169, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 14023956, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.02026, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "shirt", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "shots", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " Eagles", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " ambulance", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "chedule", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "ifies", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " Descartes", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " metaph", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " hypotheses", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " philosophical", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.08526, 0, 0, 0], [0, 0, 0, 0, 0, -0.02983, 0, 0, 0], [0, 0, 0, 0, 0, -0.03256, 0, 0, 0], [0, 0, 0, 0, 0, -0.04249, 0, 0, 0], [0, 0, 0, 0, 0, -0.04346, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.6451, 0, 0, 0], [0, 0, 0, 0, 0, -0.6259, 0, 0, 0], [0, 0, 0, 0, 0, -0.6226, 0, 0, 0], [0, 0, 0, 0, 0, -0.6217, 0, 0, 0], [0, 0, 0, 0, 0, -0.6137, 0, 0, 0]], "residual_loss_ablation": [0.2605, 0.0953, 1.12, 0.004852, 0.06019, -0.6411, -1.401, -0.519, 0]}}]}, {"quantile_name": "Subsample Interval 10", "quantile_max_act": 0.006313, "examples": [{"tokens_str_list": ["'t", " rejecting", " pushes", ",", " that", " it", " can", "'t", " handle"], "tokens_acts_list": [0, 0.7448, 0, 0, 0.006313, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 52946365, "ablations": {"loss_ablation": [0, 0, -0.004528, 0, 0, 0.0001037, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "shirt", "<META>", "<META>", "out", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", " County", "<EOT>", "<EOT>", "rangle", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", " TX", "<META_START>", "<META_START>", "riday", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "amide", "<SOS>", "<SOS>", "idx", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "Aid", "<META_END>", "<META_END>", " board", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", " philosophers", "<META>", "<META>", "ifiable", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", " philosophical", "<EOT>", "<EOT>", " MERCHANTABILITY", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", " metaph", "<META_START>", "<META_START>", " truths", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", " truths", "<SOS>", "<SOS>", "ieur", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", " Descartes", "<META_END>", "<META_END>", " ontology", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, -0.04472, 0, 0, 0.004808, 0, 0, 0], [0, 0, -0.07646, 0, 0, 0.004785, 0, 0, 0], [0, 0, -0.08432, 0, 0, 0.004287, 0, 0, 0], [0, 0, -0.1256, 0, 0, 0.004185, 0, 0, 0], [0, 0, -0.1325, 0, 0, 0.003998, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, -1.002, 0, 0, -0.0125, 0, 0, 0], [0, 0, -0.9938, 0, 0, -0.01222, 0, 0, 0], [0, 0, -0.9863, 0, 0, -0.0121, 0, 0, 0], [0, 0, -0.9829, 0, 0, -0.01199, 0, 0, 0], [0, 0, -0.9748, 0, 0, -0.01197, 0, 0, 0]], "residual_loss_ablation": [0.02134, 2.761, 2.003, 1.504, 0.2219, -0.5449, -0.6239, 3.531, 0]}}, {"tokens_str_list": [".", " You", " can", "'t", " reject", " the", " self", "-", "restraint"], "tokens_acts_list": [0, 0, 0, 0, 0.006189, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 98032422, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.0002108, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "!\u201d", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "\u201d", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "\u201d)", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", ",\u201d", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "?\u201d", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "iotic", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "missive", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "ulate", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "ponents", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "umably", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.005879, 0, 0, 0], [0, 0, 0, 0, 0, 0.005732, 0, 0, 0], [0, 0, 0, 0, 0, 0.005579, 0, 0, 0], [0, 0, 0, 0, 0, 0.005503, 0, 0, 0], [0, 0, 0, 0, 0, 0.005377, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.005056, 0, 0, 0], [0, 0, 0, 0, 0, -0.005035, 0, 0, 0], [0, 0, 0, 0, 0, -0.004845, 0, 0, 0], [0, 0, 0, 0, 0, -0.004679, 0, 0, 0], [0, 0, 0, 0, 0, -0.004434, 0, 0, 0]], "residual_loss_ablation": [4.783, 2.393, 0.04064, 0.3639, 1.65, -0.7899, -1.125, -0.02737, 0]}}, {"tokens_str_list": [" strong", " to", " reject", " it", ";", " if", " the", " evidence", " is"], "tokens_acts_list": [0.8778, 0.4522, 1.923, 0.7489, 0.005816, 1.022, 0, 0, 0.3314], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 25793974, "ablations": {"loss_ablation": [0, -0.04401, 0.09514, -0.02635, 0.1345, 0.0002656, 0.1205, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "led", "chedule", "shirt", "burg", "out", "shirt", "<META>", "<META>"], ["<EOT>", "<EOT>", "AP", "out", "ella", "   ", "shire", "<EOT>", "<EOT>"], ["<META_START>", "out", "ixel", "adium", "out", "    ", "out", "<META_START>", "<META_START>"], ["<SOS>", "pons", "shirt", " TX", "cheon", "\r", " TX", "<SOS>", "<SOS>"], ["<META_END>", " (@", "grant", " County", "in", "els", "oxide", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " Descartes", " philosophical", " ontology", " ontology", " Descartes", " ontology", "<META>", "<META>"], ["<EOT>", "uality", " Descartes", " metaph", " philosophers", "fillment", " philosophical", "<EOT>", "<EOT>"], ["<META_START>", " metaph", " Aristotle", " Descartes", " meanings", "IBILITY", " metaph", "<META_START>", "<META_START>"], ["<SOS>", " ontology", " ontology", " philosophical", " philosophical", "\u03ba", " truths", "<SOS>", "<SOS>"], ["<META_END>", " Freud", " metaph", " philosophers", " metaph", "inciple", " notions", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, -0.1315, -0.1851, 0.3712, -0.09111, 0.003056, -0.09748, 0, 0], [0, -0.265, -0.1975, 0.3454, -0.0951, 0.003026, -0.1439, 0, 0], [0, -0.2738, -0.2127, 0.2186, -0.1216, 0.002706, -0.1454, 0, 0], [0, -0.2816, -0.2156, 0.1951, -0.1274, 0.002606, -0.1544, 0, 0], [0, -0.2881, -0.2199, 0.1434, -0.129, 0.002281, -0.1696, 0, 0]], "bottom_logit_ablation_values": [[0, -1.656, -0.9739, -2.264, -1.499, -0.009616, -1.725, 0, 0], [0, -1.621, -0.9737, -2.224, -1.415, -0.009507, -1.671, 0, 0], [0, -1.617, -0.9657, -2.209, -1.366, -0.009094, -1.651, 0, 0], [0, -1.609, -0.9521, -2.152, -1.362, -0.009042, -1.646, 0, 0], [0, -1.606, -0.9363, -2.143, -1.356, -0.008824, -1.63, 0, 0]], "residual_loss_ablation": [2.438, 1.459, 5.566, 0.4943, 1.335, 0.6928, 0.04192, 4.144, 0]}}, {"tokens_str_list": [" on", " shortest", " paths", ",", " but", " rather", " also", " takes", " into"], "tokens_acts_list": [0, 0, 0, 0, 0.004612, 0.2091, 1.181, 0.1819, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 61244674, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, -0.001565, -0.02336, -0.04685, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "tlement", "ixel", "ixel", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "ftime", "hire", " booked", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "hire", "around", "hire", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "HEAfg", "dry", "ipe", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " namespacedef", "rification", "County", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", ".", " Descartes", " philosophers", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "?", " Philosophy", " Descartes", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", ":", " morality", " morality", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " '", " philosophers", " metaph", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "-", "acy", " Philosophy", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.003176, 0.02753, -0.2504, 0], [0, 0, 0, 0, 0, 0.002985, 0.00652, -0.4095, 0], [0, 0, 0, 0, 0, 0.002984, -0.001681, -0.4366, 0], [0, 0, 0, 0, 0, 0.002755, -0.006364, -0.4438, 0], [0, 0, 0, 0, 0, 0.002748, -0.008815, -0.4538, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.01983, -0.3263, -2.432, 0], [0, 0, 0, 0, 0, -0.0197, -0.3185, -2.426, 0], [0, 0, 0, 0, 0, -0.0195, -0.3095, -2.413, 0], [0, 0, 0, 0, 0, -0.01909, -0.3077, -2.382, 0], [0, 0, 0, 0, 0, -0.01904, -0.3027, -2.359, 0]], "residual_loss_ablation": [1.458, 2.873, 3.358, -0.6035, 1.426, -0.2599, 2.398, 0.3783, 0]}}, {"tokens_str_list": [",", " colours", " (", "etc", ")", " in", " the", " C", "NS"], "tokens_acts_list": [0, 0, 0.6455, 0, 0.004359, 1.152, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 87899630, "ablations": {"loss_ablation": [0, 0, 0, 0.004681, 0, -0.001119, 0.07962, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "shirt", "<META>", "out", "ixel", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "out", "<EOT>", "A", "lace", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", " County", "<META_START>", "I", " vitro", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "AP", "<SOS>", " (@", "soles", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", " TX", "<META_END>", "N", "esville", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", " philosophical", "<META>", "ributed", "leness", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", " philosophers", "<EOT>", "uring", "ifies", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", " Aristotle", "<META_START>", "urbed", " philosophers", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", " metaph", "<SOS>", "eenth", " Aristotle", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", " Descartes", "<META_END>", " \"-//", "ism", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0.03641, 0, 0.003589, 0.1392, 0, 0], [0, 0, 0, -0.01414, 0, 0.003354, -0.0235, 0, 0], [0, 0, 0, -0.03792, 0, 0.002706, -0.03499, 0, 0], [0, 0, 0, -0.04076, 0, 0.002639, -0.03658, 0, 0], [0, 0, 0, -0.04589, 0, 0.002396, -0.03758, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, -0.7838, 0, -0.00709, -1.477, 0, 0], [0, 0, 0, -0.7761, 0, -0.006893, -1.428, 0, 0], [0, 0, 0, -0.7748, 0, -0.0068, -1.359, 0, 0], [0, 0, 0, -0.7704, 0, -0.006759, -1.341, 0, 0], [0, 0, 0, -0.7617, 0, -0.006694, -1.337, 0, 0]], "residual_loss_ablation": [-0.6982, 2.48, 0.7482, 0.113, 4.523, -0.8791, -1.271, -0.489, 0]}}]}, {"quantile_name": "Bottom Activations", "quantile_max_act": 4.435e-05, "examples": [{"tokens_str_list": [",", " or", " even", " consciously", ",", " referred", " to", " their", " real"], "tokens_acts_list": [0, 0, 0.3073, 1.997, 4.435e-05, 0.9681, 0.4797, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 72585043, "ablations": {"loss_ablation": [0.3377, 0, 0, 0.111, 0.0411, 0.0008345, -0.01372, 0.04594, 0], "top_logit_ablation_tokens_str": [[" booked", "<META>", "<META>", " County", "noon", "\n", " County", " County", "<META>"], [" evacuated", "<EOT>", "<EOT>", "chedule", " evacuated", "}}$", "shire", "shirt", "<EOT>"], [" purchased", "<META_START>", "<META_START>", "oxide", " booked", "\r", " Friday", "chedule", "<META_START>"], [" notified", "<SOS>", "<SOS>", " sleeves", " wore", "ellants", "shirt", "iscopal", "<SOS>"], [" County", "<META_END>", "<META_END>", "zzles", " notified", " tire", "District", "inafter", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" Descartes", "<META>", "<META>", " ontology", " theory", "lected", " ontology", " causal", "<META>"], [" intuition", "<EOT>", "<EOT>", " conceptual", " ontology", "backer", "istem", " inferences", "<EOT>"], [" consciousness", "<META_START>", "<META_START>", "\u03c0", " hypothesis", "ritic", " intuit", " assumptions", "<META_START>"], [" truths", "<SOS>", "<SOS>", " causal", " Philosophy", "venth", "matical", " intuit", "<SOS>"], [" oneself", "<META_END>", "<META_END>", " interpretation", "ology", "\u00ec", "icism", " rational", "<META_END>"]], "top_logit_ablation_values": [[0.108, 0, 0, 0.03319, 0.254, 0.001226, -0.1827, 0.1331, 0], [0.06107, 0, 0, 0.02375, 0.2307, 0.001051, -0.2672, 0.1079, 0], [0.01787, 0, 0, 0.0231, 0.206, 0.0009508, -0.2793, 0.102, 0], [0.01303, 0, 0, 0.01546, 0.1794, 0.0006574, -0.2851, 0.08307, 0], [0.009461, 0, 0, 0.01536, 0.1761, 0.0006161, -0.2902, 0.06933, 0]], "bottom_logit_ablation_values": [[-2.445, 0, 0, -0.3793, -2.228, -0.007372, -1.355, -0.6359, 0], [-2.425, 0, 0, -0.3693, -2.224, -0.006998, -1.349, -0.6265, 0], [-2.417, 0, 0, -0.366, -2.208, -0.006874, -1.337, -0.6257, 0], [-2.39, 0, 0, -0.3638, -2.2, -0.006814, -1.335, -0.6168, 0], [-2.385, 0, 0, -0.3626, -2.175, -0.006749, -1.326, -0.6162, 0]], "residual_loss_ablation": [-2.919, 0.08227, -0.721, 1.486, -0.8516, 2.002, 1.67, -0.211, 0]}}, {"tokens_str_list": [" [", "this", " album", "]", " lacks", " the", " cum", "bers", "ome"], "tokens_acts_list": [0, 0, 0, 0, 4.387e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 52022395, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.001045, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "hetically", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "eeper", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "rals", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "rises", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "robe", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " Commons", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " behalf", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " APPE", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " Ethics", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " Judgment", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.009687, 0, 0, 0], [0, 0, 0, 0, 0, 0.009172, 0, 0, 0], [0, 0, 0, 0, 0, 0.009133, 0, 0, 0], [0, 0, 0, 0, 0, 0.009089, 0, 0, 0], [0, 0, 0, 0, 0, 0.00897, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.001029, 0, 0, 0], [0, 0, 0, 0, 0, -0.0008035, 0, 0, 0], [0, 0, 0, 0, 0, -0.0007995, 0, 0, 0], [0, 0, 0, 0, 0, -0.0007457, 0, 0, 0], [0, 0, 0, 0, 0, -0.0006897, 0, 0, 0]], "residual_loss_ablation": [0.32, 0.1265, 1.443, 2.667, 5.06, -0.5955, 1.527, 8.865, 0]}}, {"tokens_str_list": [" consistency", " of", " the", " existence", " of", " Wood", "in", " card", "inals"], "tokens_acts_list": [0, 0, 0, 0, 4.387e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 39290411, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "residual_loss_ablation": [0.6006, -0.4638, -0.7698, 0.4617, -0.06294, 1.026, 2.167, 1.617, 0]}}, {"tokens_str_list": [" skills", " necessary", " to", " attempt", " interpreting", ",", " let", " alone", " musical"], "tokens_acts_list": [0, 0, 0, 0, 4.339e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 48285115, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, -0.0008459, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " inbox", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "oz", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "ls", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " trout", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "((-", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", " versa", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " conception", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "ensor", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", ")?", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " THEORY", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, -0.001619, 0, 0, 0], [0, 0, 0, 0, 0, -0.00175, 0, 0, 0], [0, 0, 0, 0, 0, -0.001882, 0, 0, 0], [0, 0, 0, 0, 0, -0.002158, 0, 0, 0], [0, 0, 0, 0, 0, -0.002187, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.0163, 0, 0, 0], [0, 0, 0, 0, 0, -0.01548, 0, 0, 0], [0, 0, 0, 0, 0, -0.01519, 0, 0, 0], [0, 0, 0, 0, 0, -0.01504, 0, 0, 0], [0, 0, 0, 0, 0, -0.01495, 0, 0, 0]], "residual_loss_ablation": [2.347, 4.181, 0.8349, -0.1731, 2.038, 0.856, 0.1315, 1.715, 0]}}, {"tokens_str_list": [" possible", " trans", "ients", " that", " may", " be", " important", " for", " the"], "tokens_acts_list": [0, 0, 0, 0, 3.815e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 77070936, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.0002029, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "meter", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "rosine", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "uration", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "ynthesis", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "ulating", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "\u00f6s", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "ATIONS", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "vez", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "======================", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " bounds", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.001686, 0, 0, 0], [0, 0, 0, 0, 0, 0.001466, 0, 0, 0], [0, 0, 0, 0, 0, 0.00145, 0, 0, 0], [0, 0, 0, 0, 0, 0.001363, 0, 0, 0], [0, 0, 0, 0, 0, 0.001294, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.002091, 0, 0, 0], [0, 0, 0, 0, 0, -0.002001, 0, 0, 0], [0, 0, 0, 0, 0, -0.001976, 0, 0, 0], [0, 0, 0, 0, 0, -0.001955, 0, 0, 0], [0, 0, 0, 0, 0, -0.001889, 0, 0, 0]], "residual_loss_ablation": [0.9594, 0.9, 0.2433, 0.6511, 2.331, -0.9444, 1.431, 0.05908, 0]}}, {"tokens_str_list": [" by", " that", " single", " but", " various", "ly", " stated", " defense", " is"], "tokens_acts_list": [0, 0, 0, 0, 3.481e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 51894337, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "residual_loss_ablation": [1.976, 0.2486, 0.2965, -0.1655, 0.3569, 2.73, 0.184, 2.485, 0]}}, {"tokens_str_list": [" any", " software", ",", " but", " rather", " using", " an", " approach", " of"], "tokens_acts_list": [0, 0, 0, 0, 3.338e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 84501481, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "residual_loss_ablation": [2.057, 3.338, -1.076, 0.3221, -0.573, 3.066, -0.2311, 2.711, 0]}}, {"tokens_str_list": ["et", " on", " the", " topic", " of", " inner", " speech", ";", " Pi"], "tokens_acts_list": [0, 0, 0, 0, 3.29e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 94995397, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "residual_loss_ablation": [3.4, -0.8011, -0.5113, -0.72, 0.08043, -1.08, 3.938, 0.6278, 0]}}, {"tokens_str_list": [" simple", " as", " possible", " but", " no", " simpler", ".'", "\"\u2014", " Richard"], "tokens_acts_list": [0.2058, 0.3559, 0.8226, 0.2568, 3.242e-05, 0.1639, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 56865391, "ablations": {"loss_ablation": [0, -0.00914, -0.004912, -0.03573, 0.01192, -4.864e-05, -0.009998, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "!\u201d", "shirt", "!\u201d", "shirt", " Redistributions", "!\u201d", "<META>", "<META>"], ["<EOT>", " Makeup", "led", " Makeup", "hire", "glut", " buddy", "<EOT>", "<EOT>"], ["<META_START>", "days", " Makeup", ",\u201d", "sville", "ysis", " sweater", "<META_START>", "<META_START>"], ["<SOS>", ",\u201d", " TX", " cooker", "lop", " manslaughter", "shirt", "<SOS>", "<SOS>"], ["<META_END>", "day", "esville", " daddy", " booked", " monocytogenes", " kit", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " causal", "gment", "leness", " hypothesis", "naissance", " causal", "<META>", "<META>"], ["<EOT>", " ontology", "utable", " ontology", " inferences", "iors", " contradictory", "<EOT>", "<EOT>"], ["<META_START>", " Aristotle", " interpretation", "ucid", " hypotheses", "MENT", "ucid", "<META_START>", "<META_START>"], ["<SOS>", " negation", " causal", " causal", " causal", "ables", " Descartes", "<SOS>", "<SOS>"], ["<META_END>", " implied", " meanings", " negation", " existence", "rolled", " analogous", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0.124, 0.1351, 0.3414, 0.05497, 0.005832, 0.06314, 0, 0], [0, 0.1157, 0.1339, 0.2985, 0.008936, 0.004582, 0.05923, 0, 0], [0, 0.1117, 0.1225, 0.2782, 0.008735, 0.004377, 0.05078, 0, 0], [0, 0.1115, 0.1212, 0.278, 0.008703, 0.004156, 0.05022, 0, 0], [0, 0.1098, 0.1202, 0.276, 0.007214, 0.004139, 0.0492, 0, 0]], "bottom_logit_ablation_values": [[0, -0.1329, -0.3926, -0.7225, -0.3146, -0.003509, -0.1341, 0, 0], [0, -0.1283, -0.3628, -0.7003, -0.3097, -0.003033, -0.134, 0, 0], [0, -0.1245, -0.3479, -0.6838, -0.3094, -0.002869, -0.1329, 0, 0], [0, -0.1243, -0.3467, -0.6825, -0.3067, -0.002772, -0.1317, 0, 0], [0, -0.118, -0.3412, -0.6527, -0.3011, -0.002712, -0.1296, 0, 0]], "residual_loss_ablation": [4.254, 2.424, 1.785, -1.35, -0.06437, 1.849, 2.195, 2.175, 0]}}, {"tokens_str_list": ["\n", "\n", "Now", "adays", " reference", " to", " the", " _", "Sh"], "tokens_acts_list": [0, 0, 0, 0, 2.813e-05, 0.7382, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 50762417, "ablations": {"loss_ablation": [-0.07608, 0, 0, 0, 0, -0.002661, 0.03613, 0, 0], "top_logit_ablation_tokens_str": [["hire", "<META>", "<META>", "<META>", "<META>", "\n", "shirt", "<META>", "<META>"], ["shirt", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "h", "shirts", "<EOT>", "<EOT>"], ["chedule", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "-", " County", "<META_START>", "<META_START>"], [" County", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " (", "chedule", "<SOS>", "<SOS>"], ["out", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "0", " jail", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" metaph", "<META>", "<META>", "<META>", "<META>", "mptotic", " philosophical", "<META>", "<META>"], [" philosophical", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "ichlet", "eness", "<EOT>", "<EOT>"], [" ontology", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "indrome", " metaph", "<META_START>", "<META_START>"], [" reasoning", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "CMeta", " hypothesis", "<SOS>", "<SOS>"], [" philosophers", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " utila", "eced", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0.0198, 0, 0, 0, 0, 0.004333, -0.09589, 0, 0], [0.007119, 0, 0, 0, 0, 0.002517, -0.1515, 0, 0], [-0.02417, 0, 0, 0, 0, 0.002155, -0.1521, 0, 0], [-0.03115, 0, 0, 0, 0, 0.002142, -0.1775, 0, 0], [-0.03407, 0, 0, 0, 0, 0.001813, -0.1805, 0, 0]], "bottom_logit_ablation_values": [[-1.259, 0, 0, 0, 0, -0.0156, -1.086, 0, 0], [-1.258, 0, 0, 0, 0, -0.01524, -1.085, 0, 0], [-1.231, 0, 0, 0, 0, -0.01459, -1.075, 0, 0], [-1.216, 0, 0, 0, 0, -0.01445, -1.054, 0, 0], [-1.207, 0, 0, 0, 0, -0.01436, -1.052, 0, 0]], "residual_loss_ablation": [1.813, -0.0002731, 0.03371, 4.777, -0.03059, 1.606, -0.8279, 2.65, 0]}}, {"tokens_str_list": [" of", " being", " wide", ",", " broad", ",", " huge", ",", " spac"], "tokens_acts_list": [2.268, 1.788, 0, 0, 2.289e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 88562656, "ablations": {"loss_ablation": [0, 0.1021, -0.04817, 0, 0, -0.0005239, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "ixel", " worn", "<META>", "<META>", "el", "<META>", "<META>", "<META>"], ["<EOT>", "shirt", " evacuated", "<EOT>", "<EOT>", "ro", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "wear", " booked", "<META_START>", "<META_START>", "els", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "ixels", "fits", "<SOS>", "<SOS>", "ened", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "robe", " shipped", "<META_END>", "<META_END>", "en", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " causal", " hypothesis", "<META>", "<META>", "ynaptic", "<META>", "<META>", "<META>"], ["<EOT>", " hypotheses", " inferences", "<EOT>", "<EOT>", "prio", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", " hypothesis", " hypotheses", "<META_START>", "<META_START>", "odo", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", " assumptions", " inference", "<SOS>", "<SOS>", "izations", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", " rational", " assumptions", "<META_END>", "<META_END>", " Chomsky", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 1.221, 0.2825, 0, 0, 0.003157, 0, 0, 0], [0, 0.8947, 0.2156, 0, 0, 0.002816, 0, 0, 0], [0, 0.7998, 0.2043, 0, 0, 0.001755, 0, 0, 0], [0, 0.7892, 0.2042, 0, 0, 0.001752, 0, 0, 0], [0, 0.7561, 0.197, 0, 0, 0.001721, 0, 0, 0]], "bottom_logit_ablation_values": [[0, -1.771, -2.282, 0, 0, -0.01597, 0, 0, 0], [0, -1.765, -2.142, 0, 0, -0.01557, 0, 0, 0], [0, -1.764, -2.116, 0, 0, -0.01535, 0, 0, 0], [0, -1.76, -2.094, 0, 0, -0.01531, 0, 0, 0], [0, -1.757, -2.091, 0, 0, -0.01511, 0, 0, 0]], "residual_loss_ablation": [0.4361, 1.321, 1.011, -0.6754, 2.535, 1.128, 2.685, -0.1399, 0]}}, {"tokens_str_list": [" and", " Cass", "ius", " are", " alone", ";", " they", " have", " quarrel"], "tokens_acts_list": [0, 0, 0, 0, 2.146e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 348584, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0.001146, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "\u2019", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", ".\u201d", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", ".", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", ",\u201d", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "!\u201d", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "phosphate", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "ocene", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "CreateView", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "liad", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "rosine", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.005239, 0, 0, 0], [0, 0, 0, 0, 0, 0.00402, 0, 0, 0], [0, 0, 0, 0, 0, 0.003561, 0, 0, 0], [0, 0, 0, 0, 0, 0.003319, 0, 0, 0], [0, 0, 0, 0, 0, 0.003172, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.007108, 0, 0, 0], [0, 0, 0, 0, 0, -0.006815, 0, 0, 0], [0, 0, 0, 0, 0, -0.006774, 0, 0, 0], [0, 0, 0, 0, 0, -0.006645, 0, 0, 0], [0, 0, 0, 0, 0, -0.006546, 0, 0, 0]], "residual_loss_ablation": [-0.7747, 1.391, 2.593, 1.532, 0.1979, 0.8743, -0.6092, -0.5428, 0]}}, {"tokens_str_list": [".", " (", "I", " repeat", " that", " I", " am", " tele", "sc"], "tokens_acts_list": [0, 0, 0, 0.5212, 1.907e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 15261694, "ablations": {"loss_ablation": [0, 0, 0, 0, 0.05755, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "shirt", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "out", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "led", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "days", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "shots", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", " ontology", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", " causal", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", " truths", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", " empirical", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", " hypotheses", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0.1898, 0, 0, 0, 0], [0, 0, 0, 0, 0.09252, 0, 0, 0, 0], [0, 0, 0, 0, 0.08045, 0, 0, 0, 0], [0, 0, 0, 0, 0.08012, 0, 0, 0, 0], [0, 0, 0, 0, 0.07447, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, -0.5624, 0, 0, 0, 0], [0, 0, 0, 0, -0.5621, 0, 0, 0, 0], [0, 0, 0, 0, -0.5534, 0, 0, 0, 0], [0, 0, 0, 0, -0.5533, 0, 0, 0, 0], [0, 0, 0, 0, -0.5383, 0, 0, 0, 0]], "residual_loss_ablation": [0.9533, 0.3312, 1.528, -0.198, -0.6413, 0.7405, 0.1528, 0.108, 0]}}, {"tokens_str_list": [" the", " other", " hand", ",", " the", " latent", " meaning", " is", " attributed"], "tokens_acts_list": [0.1423, 1.312, 0, 0, 1.764e-05, 0, 1.168, 1.494, 2.045], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 35334335, "ablations": {"loss_ablation": [0.1336, -0.02552, -0.4453, 0, 0, 0, 0, 0.2745, 0], "top_logit_ablation_tokens_str": [["days", " morning", " morning", "<META>", "<META>", "<META>", "<META>", "fully", "<META>"], ["hire", " afternoon", " afternoon", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "out", "<EOT>"], ["month", " evening", " evening", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "room", "<META_START>"], ["shirt", " night", "month", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "boat", "<SOS>"], ["noon", "arrow", " evenings", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "rooms", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["mmas", " ourselves", " ourselves", "<META>", "<META>", "<META>", "<META>", " philosophical", "<META>"], [" philosophers", " MERCHANTABILITY", "ational", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " rational", "<EOT>"], [" theore", "notations", " morality", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " empirical", "<META_START>"], ["nostic", "mitives", " correctness", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " conceptual", "<SOS>"], [" notions", " philosophers", "arity", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " moral", "<META_END>"]], "top_logit_ablation_values": [[1.063, -0.01341, 0.2784, 0, 0, 0, 0, 0.5035, 0], [0.8079, -0.01353, 0.2734, 0, 0, 0, 0, 0.4042, 0], [0.806, -0.01653, 0.2299, 0, 0, 0, 0, 0.3887, 0], [0.6936, -0.02074, 0.2234, 0, 0, 0, 0, 0.3825, 0], [0.6649, -0.02301, 0.2084, 0, 0, 0, 0, 0.3706, 0]], "bottom_logit_ablation_values": [[-3.239, -0.2157, -1.834, 0, 0, 0, 0, -1.277, 0], [-3.215, -0.2143, -1.689, 0, 0, 0, 0, -1.249, 0], [-3.199, -0.2075, -1.68, 0, 0, 0, 0, -1.244, 0], [-3.176, -0.2071, -1.672, 0, 0, 0, 0, -1.178, 0], [-3.165, -0.2022, -1.662, 0, 0, 0, 0, -1.158, 0]], "residual_loss_ablation": [-0.3315, -0.07573, 3.687, 0.9004, -0.2974, 3.591, 0.5401, 2.342, 0]}}, {"tokens_str_list": [" majority", " of", " major", " flaws", " with", " this", " game", ",", " its"], "tokens_acts_list": [0, 0, 0, 0, 1.478e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 67304494, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, -0.0002956, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "STITUTE", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "oprop", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "poon", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "ABILITY", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "\u0438\u0438", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "ness", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", " (_", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", " \\[[@", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", " (", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " \"", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.007646, 0, 0, 0], [0, 0, 0, 0, 0, 0.007067, 0, 0, 0], [0, 0, 0, 0, 0, 0.006862, 0, 0, 0], [0, 0, 0, 0, 0, 0.006626, 0, 0, 0], [0, 0, 0, 0, 0, 0.006596, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.00332, 0, 0, 0], [0, 0, 0, 0, 0, -0.003273, 0, 0, 0], [0, 0, 0, 0, 0, -0.002898, 0, 0, 0], [0, 0, 0, 0, 0, -0.002894, 0, 0, 0], [0, 0, 0, 0, 0, -0.00284, 0, 0, 0]], "residual_loss_ablation": [-0.08798, -0.1298, 0.6488, 5.236, -1.693, -0.1071, 1.741, -0.3107, 0]}}, {"tokens_str_list": [" positive", " energy", " matter", " to", " corresponding", " states", " of", " negative", " energy"], "tokens_acts_list": [0, 0, 0, 0, 1.287e-05, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 68342944, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, -0.000104, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "ened", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "\u0443\u0436", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "ens", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "rile", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "ted", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "using", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "chaft", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "whether", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "weighted", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", " Gecko", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0.0009489, 0, 0, 0], [0, 0, 0, 0, 0, 0.0007973, 0, 0, 0], [0, 0, 0, 0, 0, 0.0007439, 0, 0, 0], [0, 0, 0, 0, 0, 0.0006924, 0, 0, 0], [0, 0, 0, 0, 0, 0.000639, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, -0.006028, 0, 0, 0], [0, 0, 0, 0, 0, -0.005664, 0, 0, 0], [0, 0, 0, 0, 0, -0.00549, 0, 0, 0], [0, 0, 0, 0, 0, -0.00533, 0, 0, 0], [0, 0, 0, 0, 0, -0.005307, 0, 0, 0]], "residual_loss_ablation": [0.8361, 1.371, 4.031, 1.187, 0.2783, 1.748, -1.412, 3.237, 0]}}, {"tokens_str_list": [" elements", " can", " be", " found", " combined", " with", " different", " B", "s"], "tokens_acts_list": [0, 0.143, 0.2654, 0.2914, 1.049e-05, 1.352, 0.7727, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 73665569, "ablations": {"loss_ablation": [0.1687, 0, -0.009593, -0.055, 0.01449, -0.0008081, -0.02029, -0.03748, 0], "top_logit_ablation_tokens_str": [["shirt", "<META>", "shirt", " booked", "out", "light", "shirt", "shirt", "<META>"], ["AP", "<EOT>", "isters", " purchased", " Makeup", "\n", "shirts", "ixel", "<EOT>"], [" County", "<META_START>", "ette", "shirt", "outs", "days", "ixel", " pads", "<META_START>"], ["irt", "<SOS>", "hire", " shipped", "irt", " day", " TX", "shots", "<SOS>"], ["out", "<META_END>", "cels", " topped", "shirt", " for", " pads", " sleeves", "<META_END>"]], "bottom_logit_ablation_tokens_str": [[" philosophers", "<META>", " philosophical", " philosophers", " Descartes", "htags", " philosophical", " Aristotle", "<META>"], [" Aristotle", "<EOT>", " philosophers", " truths", " philosophical", "laboration", " philosophers", " Descartes", "<EOT>"], [" philosophical", "<META_START>", " Aristotle", " Aristotle", " Aristotle", "ority", " truths", " philosophical", "<META_START>"], [" Descartes", "<SOS>", " truths", " Philosophy", " Philosophy", " CAUSED", " ontology", " morally", "<SOS>"], [" oneself", "<META_END>", " ontology", " existence", " philosophers", "ithmetic", " Descartes", " oneself", "<META_END>"]], "top_logit_ablation_values": [[-0.007454, 0, -0.02929, -0.04323, -0.01327, 0.001471, -0.1579, -0.2021, 0], [-0.04059, 0, -0.03179, -0.05538, -0.02021, 0.001351, -0.3513, -0.2728, 0], [-0.04911, 0, -0.03681, -0.05692, -0.02719, 0.001096, -0.3582, -0.2741, 0], [-0.04966, 0, -0.03723, -0.06547, -0.0289, 0.0006871, -0.4031, -0.2844, 0], [-0.05064, 0, -0.04192, -0.06567, -0.04067, 0.0006571, -0.4844, -0.2923, 0]], "bottom_logit_ablation_values": [[-0.572, 0, -0.2822, -0.537, -0.4543, -0.01054, -2.393, -1.386, 0], [-0.5669, 0, -0.2688, -0.5349, -0.4491, -0.00997, -2.384, -1.36, 0], [-0.5622, 0, -0.2677, -0.5196, -0.4466, -0.009789, -2.331, -1.32, 0], [-0.5561, 0, -0.2672, -0.5136, -0.442, -0.009648, -2.305, -1.315, 0], [-0.5559, 0, -0.2662, -0.5119, -0.4408, -0.009625, -2.295, -1.31, 0]], "residual_loss_ablation": [2.798, 1.134, -0.1996, 0.7452, 1.682, -0.587, 1.021, -0.6304, 0]}}, {"tokens_str_list": [" be", ",", " or", " what", " ought", " to", " be", ".", " Just"], "tokens_acts_list": [0, 0, 0, 0, 5.245e-06, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 27854658, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "residual_loss_ablation": [0.6593, 3.29, 1.093, 1.471, 1.634, 8.508, 1.224, 2.52, 0]}}, {"tokens_str_list": [" about", " the", " state", ".", " Already", " here", " there", " are", " choices"], "tokens_acts_list": [0, 0, 0, 0, 3.815e-06, 0, 0, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 70653487, "ablations": {"loss_ablation": [0, 0, 0, 0, 0, 0, 0, 0, 0], "top_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>", "<META>"], ["<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>", "<EOT>"], ["<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>", "<META_START>"], ["<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>", "<SOS>"], ["<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>", "<META_END>"]], "top_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "bottom_logit_ablation_values": [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], "residual_loss_ablation": [2.458, -0.3889, 2.648, 1.159, 1.268, 0.8421, 2.336, -0.379, 0]}}, {"tokens_str_list": [" possible", " for", " a", " person", " to", " be", " fully", " morally", " responsible"], "tokens_acts_list": [0.1662, 1.364, 0.854, 0, 2.861e-06, 0.3897, 0.9423, 0, 0], "train_token_ind": 4, "is_repeated_datapoint": 0, "datapoint_ind": 25889212, "ablations": {"loss_ablation": [0, -0.02778, 0.08755, 0.03333, 0, 0.001139, 0.06216, 0.441, 0], "top_logit_ablation_tokens_str": [["<META>", "!!!", "ixel", " weekend", "<META>", " sj\u00e4", " booked", " booked", "<META>"], ["<EOT>", " (@", "shirt", "orta", "<EOT>", "\u0432\u0435\u0434\u0438\u0442\u0435", " evacuated", " evacuated", "<EOT>"], ["<META_START>", "!!", " Makeup", " month", "<META_START>", "DictWriter", " notified", " cleaned", "<META_START>"], ["<SOS>", " Friday", "days", "chedule", "<SOS>", "TubeA", " contacted", " notified", "<SOS>"], ["<META_END>", " Wednesday", " County", "ette", "<META_END>", "DictReader", " purchased", "oxide", "<META_END>"]], "bottom_logit_ablation_tokens_str": [["<META>", " ontology", " metaph", " truths", "<META>", "\n", " existence", "ities", "<META>"], ["<EOT>", " philosophers", " philosophical", " ourselves", "<EOT>", " '", " truths", " truths", "<EOT>"], ["<META_START>", " philosophical", " ontology", " oneself", "<META_START>", " is", " metaph", "eness", "<META_START>"], ["<SOS>", " metaph", " truths", " notions", "<SOS>", "s", " ontology", " notions", "<SOS>"], ["<META_END>", "icism", " phenomena", " meanings", "<META_END>", " (", " notions", " phenomena", "<META_END>"]], "top_logit_ablation_values": [[0, 0.01128, 0.09776, 0.01923, 0, 0.008544, 0.08168, 0.2582, 0], [0, 0.008742, 0.007238, -0.03253, 0, 0.008182, 0.05235, 0.1599, 0], [0, 0.007093, -0.01687, -0.05408, 0, 0.008135, 0.02768, 0.0924, 0], [0, 0.004309, -0.04853, -0.06644, 0, 0.008063, 0.02424, 0.06422, 0], [0, 0.004087, -0.05146, -0.07057, 0, 0.008056, 0.0005465, 0.05969, 0]], "bottom_logit_ablation_values": [[0, -0.219, -2.128, -1.462, 0, -0.009959, -0.657, -1.367, 0], [0, -0.2169, -2.078, -1.453, 0, -0.009845, -0.6527, -1.36, 0], [0, -0.2054, -2.055, -1.446, 0, -0.009619, -0.6466, -1.326, 0], [0, -0.2025, -2.052, -1.416, 0, -0.009379, -0.6421, -1.322, 0], [0, -0.2002, -2.002, -1.396, 0, -0.009112, -0.6421, -1.319, 0]], "residual_loss_ablation": [5.536, -1, -0.1714, 1.168, 2.502, -1.017, -0.2684, 0.171, 0]}}]}], "statistics": {"density_frac": 0.004295, "global_max_act": 5.114, "abs_max_neuron_weights": [0.2146, 0.2122, 0.2017], "abs_max_neuron_inds": [281, 214, 353], "l1_max_neuron_weights": [0.01695, 0.01676, 0.01593]}, "logit_effects": {"negatives": [["sville", -0.2776], ["burg", -0.283], ["month", -0.2854], ["ixel", -0.2866], ["hire", -0.2888], ["AP", -0.2913], [" TX", -0.292], ["out", -0.312], [" County", -0.3185], ["shirt", -0.3616]], "positives": [[" philosophical", 0.4429], [" Aristotle", 0.4392], [" philosophers", 0.4346], [" Descartes", 0.4262], [" metaph", 0.4259], [" ontology", 0.4134], [" truths", 0.3984], [" oneself", 0.3934], [" negation", 0.3924], [" empirical", 0.3907]]}}